{"cells":[{"cell_type":"markdown","metadata":{"id":"Oc4QzK2lMdkH"},"source":["# Dilated HarDNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15636,"status":"ok","timestamp":1649227112421,"user":{"displayName":"Maxim Dobrokhvalov","userId":"05960264113093771023"},"user_tz":-180},"id":"IrFsKEKW7429","outputId":"f673c672-bbac-4264-b1e3-78dbff639305"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5\n"]}],"source":["import sys\n","sys.path.insert(0, '.')\n","import os\n","import logging\n","import random\n","import time\n","import math\n","import torch\n","import numpy as np \n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.nn import init\n","from torch.utils import data\n","from datetime import datetime, timedelta\n","from collections import OrderedDict\n","\n","from torchsummary import summary\n","\n","import torch.distributed as dist\n","\n"," \n","!pip install tensorboardX\n","from tensorboardX import SummaryWriter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvH1JPHzSxXK"},"outputs":[],"source":["os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yTqA_v7SK42"},"outputs":[],"source":["batch_size = 8\n","n_workers = 2\n","print_interval=10\n","val_interval=500\n","\n","n_classes = 19\n","\n","model_arch = 'Linear_HarDNblock_dilated'\n","\n","\n","bn_mom = 0.1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdAvR6h_Y43C"},"outputs":[],"source":["# Setup device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"_rlqCnjINmRs"},"source":["---\n","## Architecture"]},{"cell_type":"markdown","metadata":{"id":"ej7GwMiBroOM"},"source":["### Blocks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfQMxl1EysQu"},"outputs":[],"source":["class DilatedConv(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size=3, dilation=1, stride=1, bias=False):\n","        super().__init__()\n","        num_splits = 2\n","        assert(out_planes%num_splits == 0)\n","        conv_in_planes = in_planes // num_splits\n","        conv_out_planes = out_planes // num_splits\n","        groups = 2\n","        conv_1 = nn.Conv2d(conv_in_planes, conv_out_planes, kernel_size, padding=kernel_size//2, dilation=1, stride=stride, groups=groups, bias=bias)\n","        conv_n = nn.Conv2d(conv_in_planes, conv_out_planes, 3, padding=dilation, dilation=dilation, stride=stride, groups=groups, bias=bias)\n","        self.convs=nn.ModuleList([conv_1, conv_n])\n","        self.num_splits=num_splits\n","        self.init_weight()\n","    \n","    def forward(self,x):\n","        x=torch.tensor_split(x,self.num_splits,dim=1)\n","        res = []\n","        for i in range(self.num_splits):\n","            res.append(self.convs[i](x[i]))\n","        return torch.cat(res,dim=1)\n","    \n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VehqKGzJyoV9"},"outputs":[],"source":["class ConvX(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel=3, stride=1, dilation=1):\n","        super(ConvX, self).__init__()\n","        if dilation == 1:\n","            self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel, stride=stride, padding=kernel//2, bias=False)\n","        else:\n","            self.conv = DilatedConv(in_planes, out_planes, kernel_size=kernel, stride=stride, dilation=dilation, bias=False)\n","        self.bn = nn.BatchNorm2d(out_planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.init_weight()\n","\n","    def forward(self, x):\n","        out = self.relu(self.bn(self.conv(x)))\n","        return out\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)"]},{"cell_type":"code","source":["class HarDBlock(nn.Module):\n","    def __init__(self, in_channels, growth_rate, grmul, n_layers, dilation=1, keepBase=False):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.growth_rate = growth_rate\n","        self.grmul = grmul\n","        self.links = []\n","        self.out_channels = 0\n","        self.keepBase = keepBase\n","        self.layers = nn.ModuleList([])\n","        for i in range(n_layers):\n","            out_ch, in_ch, el_links = self.get_links(i+1)\n","            self.links.append(el_links)\n","            self.layers.append(ConvX(in_ch, out_ch, dilation=dilation))\n","            if (i % 2 == 0) or (i == n_layers - 1):\n","                self.out_channels += out_ch\n","\n","    def __out_ch(self, layer_id): \n","        out_ch = self.growth_rate\n","        for i in range(1, int(math.log2(layer_id))+1):\n","            if layer_id % 2**i == 0:\n","                out_ch = out_ch*self.grmul\n","        return (int(out_ch + 3) // 4) * 4\n","\n","    def get_links(self, layer_id):\n","        in_ch = 0\n","        links_ = []\n","        for i in range(int(math.log2(layer_id))):\n","            diff = 2**i\n","            if (layer_id % diff == 0) and layer_id - diff > 0:\n","                in_ch += self.__out_ch(layer_id - diff)\n","                links_.append(layer_id - diff)\n","        if math.log2(layer_id).is_integer():\n","            in_ch += self.in_channels\n","            links_.append(0)\n","        return self.__out_ch(layer_id), in_ch, links_\n","\n","    def forward(self, x):\n","        data = [x]\n","        for layer in range(len(self.layers)):\n","            layer_input = []\n","            for link in self.links[layer]:\n","                layer_input.append(data[link])\n","            in_ = layer_input[0] if len(layer_input) == 1 \\\n","                else torch.cat(layer_input, dim=1)\n","            data.append(self.layers[layer](in_))\n","        t = len(data)\n","        out = []\n","        for i in range(t):\n","            if (i % 2 == 1) or (self.keepBase and i == 0) or (i == t-1):\n","                out.append(data[i])\n","        return torch.cat(out, dim=1)\n","    \n","    def get_out_ch(self):\n","        return self.out_channels"],"metadata":{"id":"nzcGQginKYKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwHZ02vWozj0"},"outputs":[],"source":["class CatBottleneck(nn.Module):\n","    def __init__(self, in_planes, out_planes, block_num=4, stride=1, dilation=1):\n","        super(CatBottleneck, self).__init__()\n","        assert block_num > 1, print(\"block number should be larger than 1.\")\n","        self.conv_list = nn.ModuleList()\n","        self.stride = stride\n","        if stride == 2:\n","            self.avd_layer = nn.Sequential(\n","                nn.Conv2d(out_planes//2, out_planes//2, kernel_size=3, stride=2, padding=1, groups=out_planes//2, bias=False),\n","                nn.BatchNorm2d(out_planes//2),\n","            )\n","            self.skip = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n","            stride = 1\n","\n","        for idx in range(block_num):\n","            blk = None\n","            if idx == 0:\n","                blk = ConvX(in_planes, out_planes//2, kernel=1)\n","            elif idx == 1 and block_num == 2:\n","                blk = ConvX(out_planes//2, out_planes//2, stride=stride, dilation=dilation)\n","            elif idx == 1 and block_num > 2:\n","                blk = ConvX(out_planes//2, out_planes//4, stride=stride, dilation=dilation)\n","            elif idx < block_num - 1:\n","                blk = ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx+1)), dilation=dilation)\n","            else:\n","                blk = ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx)), dilation=dilation)\n","            self.conv_list.append(blk)\n","            \n","    def forward(self, x):\n","        out_list = []\n","        out1 = self.conv_list[0](x)\n","\n","        for idx, conv in enumerate(self.conv_list[1:]):\n","            if idx == 0:\n","                if self.stride == 2:\n","                    out = conv(self.avd_layer(out1))\n","                else:\n","                    out = conv(out1)\n","            else:\n","                out = conv(out)\n","            out_list.append(out)\n","\n","        if self.stride == 2:\n","            out1 = self.skip(out1)\n","        out_list.insert(0, out1)\n","\n","        out = torch.cat(out_list, dim=1)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIqdvmi-yzg4"},"outputs":[],"source":["class BiSeNetOutput(nn.Module):\n","    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n","        super(BiSeNetOutput, self).__init__()\n","        self.conv = ConvX(in_chan, mid_chan, kernel=3, stride=1)\n","        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n","        self.init_weight()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.conv_out(x)\n","        return x\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUHie9tesVSD"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, num_classes, channels):\n","        super().__init__()\n","        channels4, channels8, channels16 = channels\n","        self.head16=ConvX(channels16, 128, 1)\n","        self.head8=ConvX(channels8, 128, 1)\n","        self.head4=ConvX(channels4, 8, 1)\n","        self.conv8=ConvX(128,64,3,1,1)\n","        self.conv4=ConvX(64+8,64,3,1,1)\n","        self.classifier=nn.Conv2d(64, num_classes, 1)\n","\n","    def forward(self, x):\n","        x4, x8, x16 = x\n","        x16=self.head16(x16)\n","        x8=self.head8(x8)\n","        x4=self.head4(x4)\n","        x16 = F.interpolate(x16, size=x8.shape[-2:], mode='bilinear', align_corners=False)\n","        x8= x8 + x16\n","        x8=self.conv8(x8)\n","        x8 = F.interpolate(x8, size=x4.shape[-2:], mode='bilinear', align_corners=False)\n","        x4=torch.cat((x8,x4),dim=1)\n","        x4=self.conv4(x4)\n","        x4=self.classifier(x4)\n","        return x4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88rzHfln70PP"},"outputs":[],"source":["class LinearHarDBlockDilated(nn.Module):\n","    def __init__(self, dilations: list, target_planes=256, n_classes=19):\n","        super(LinearHarDBlockDilated, self).__init__()\n","        self.conv0 = ConvX(3, 32, kernel=3, stride=2)\n","        self.conv1 = ConvX(32, 64, kernel=3, stride=2)\n","        scale = 4\n","        planes = 64\n","        blocks = []\n","        self.outs = []\n","        channels = []\n","        prev_dilation = dilations[0]\n","        for i, dilation in enumerate(dilations):\n","            if prev_dilation != dilation and scale <= 16:\n","                scale *= 2\n","                blocks.append(ConvX(planes, planes, 3, 2))\n","            in_planes = planes\n","            out_planes = planes\n","            blk = HarDBlock(in_planes, growth_rate=40, grmul=1.7, n_layers=4, dilation=dilation)\n","            blocks.append(blk)\n","            blk_out_ch = blk.get_out_ch()\n","            if blk_out_ch < target_planes:\n","                out_planes = min(blk_out_ch, target_planes)\n","                blocks.append(ConvX(blk_out_ch, out_planes, kernel=1))\n","                planes = out_planes\n","            if i == 0 or i == len(dilations) - 1:\n","                channels.append(out_planes)\n","                self.outs.append(i)\n","            if i + 1 < len(dilations) and scale < 16 and dilation != dilations[i+1]:\n","                channels.append(out_planes)\n","                self.outs.append(i)\n","            prev_dilation = dilation\n","        self.blocks = nn.ModuleList(blocks)\n","        self.detail_out = BiSeNetOutput(channels[0], 64, 1)\n","        self.decoder = Decoder(n_classes, channels[1:])\n","\n","    def forward(self, x):\n","        out = self.conv0(x)\n","        out = self.conv1(out)\n","        store_out = []\n","        for i, module in enumerate(self.blocks):\n","            out = module(out)\n","            if i in self.outs:\n","                store_out.append(out)\n","\n","        out_detail = self.detail_out(store_out[0])\n","        out = self.decoder(store_out[1:])\n","        out_detail = F.interpolate(out_detail, size=x.shape[-2:], mode='bilinear', align_corners=False)\n","        out = F.interpolate(out, size=x.shape[-2:], mode='bilinear', align_corners=False)\n","        return out, out_detail"]},{"cell_type":"markdown","metadata":{"id":"MfAtjams6ikd"},"source":["---\n","## Loss & Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1X1GCpDNvej"},"outputs":[],"source":["class OhemCELoss(nn.Module):\n","    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n","        super(OhemCELoss, self).__init__()\n","        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).to(device)\n","        self.n_min = n_min\n","        self.ignore_lb = ignore_lb\n","        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n","\n","    def forward(self, logits, labels):\n","        N, C, H, W = logits.size()\n","        loss = self.criteria(logits, labels).view(-1)\n","        loss, _ = torch.sort(loss, descending=True)\n","        if loss[self.n_min] > self.thresh:\n","            loss = loss[loss>self.thresh]\n","        else:\n","            loss = loss[:self.n_min]\n","        return torch.mean(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AtvMtbc_LfS"},"outputs":[],"source":["float_tensor_type = torch.cuda.FloatTensor if device.type=='cuda' else torch.FloatTensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hxwvvyh6PdMg"},"outputs":[],"source":["def dice_loss_func(input, target):\n","    smooth = 1.\n","    n = input.size(0)\n","    iflat = input.view(n, -1)\n","    tflat = target.view(n, -1)\n","    intersection = (iflat * tflat).sum(1)\n","    loss = 1 - ((2. * intersection + smooth) /\n","                (iflat.sum(1) + tflat.sum(1) + smooth))\n","    return loss.mean()\n","\n","\n","class DetailAggregateLoss(nn.Module):\n","    def __init__(self, *args, **kwargs):\n","        super(DetailAggregateLoss, self).__init__()\n","        \n","        self.laplacian_kernel = torch.tensor(\n","            [-1, -1, -1, -1, 8, -1, -1, -1, -1],\n","            dtype=torch.float32).reshape(1, 1, 3, 3).requires_grad_(False).type(float_tensor_type)        \n","\n","        self.fuse_kernel = torch.nn.Parameter(torch.tensor([[6./10], [3./10], [1./10]],\n","            dtype=torch.float32).reshape(1, 3, 1, 1).type(float_tensor_type))\n","\n","    def forward(self, boundary_logits, gtmasks):\n","\n","        boundary_targets = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, padding=1)\n","        boundary_targets = boundary_targets.clamp(min=0)\n","        boundary_targets[boundary_targets > 0.1] = 1\n","        boundary_targets[boundary_targets <= 0.1] = 0\n","\n","        boundary_targets_x2 = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, stride=2, padding=1)\n","        boundary_targets_x2 = boundary_targets_x2.clamp(min=0)\n","        \n","        boundary_targets_x4 = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, stride=4, padding=1)\n","        boundary_targets_x4 = boundary_targets_x4.clamp(min=0)\n","\n","        boundary_targets_x4_up = F.interpolate(boundary_targets_x4, boundary_targets.shape[2:], mode='nearest')\n","        boundary_targets_x2_up = F.interpolate(boundary_targets_x2, boundary_targets.shape[2:], mode='nearest')\n","        \n","        boundary_targets_x2_up[boundary_targets_x2_up > 0.1] = 1\n","        boundary_targets_x2_up[boundary_targets_x2_up <= 0.1] = 0\n","        \n","        \n","        boundary_targets_x4_up[boundary_targets_x4_up > 0.1] = 1\n","        boundary_targets_x4_up[boundary_targets_x4_up <= 0.1] = 0\n","       \n","        boudary_targets_pyramids = torch.stack((boundary_targets, boundary_targets_x2_up, boundary_targets_x4_up), dim=1)\n","        \n","        boudary_targets_pyramids = boudary_targets_pyramids.squeeze(2)\n","        boudary_targets_pyramid = F.conv2d(boudary_targets_pyramids, self.fuse_kernel)\n","\n","        boudary_targets_pyramid[boudary_targets_pyramid > 0.1] = 1\n","        boudary_targets_pyramid[boudary_targets_pyramid <= 0.1] = 0\n","        \n","        \n","        if boundary_logits.shape[-1] != boundary_targets.shape[-1]:\n","            boundary_logits = F.interpolate(\n","                boundary_logits, boundary_targets.shape[2:], mode='bilinear', align_corners=True)\n","        \n","        bce_loss = F.binary_cross_entropy_with_logits(boundary_logits, boudary_targets_pyramid)\n","        dice_loss = dice_loss_func(torch.sigmoid(boundary_logits), boudary_targets_pyramid)\n","        return bce_loss,  dice_loss\n","\n","    def get_params(self):\n","        wd_params, nowd_params = [], []\n","        for name, module in self.named_modules():\n","                nowd_params += list(module.parameters())\n","        return nowd_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpE2HElyOJB3"},"outputs":[],"source":["class Optimizer(object):\n","    def __init__(self, model, loss, lr0, momentum, wd, warmup_steps, \n","                 warmup_start_lr, max_iter, power, *args, **kwargs):\n","        self.warmup_steps = warmup_steps\n","        self.warmup_start_lr = warmup_start_lr\n","        self.lr0 = lr0\n","        self.lr = self.lr0\n","        self.max_iter = float(max_iter)\n","        self.power = power\n","        self.it = 0\n","        # wd_params, nowd_params = model.get_params() # , lr_mul_wd_params, lr_mul_nowd_params\n","        loss_nowd_params = loss.get_params()\n","        #---------------------------------------------------------------------------\n","        #---------------------------------------------------------------------------\n","        #---------------------------------------------------------------------------\n","        param_list = [\n","                {'params': model.parameters()},\n","                {'params': loss_nowd_params}]\n","        self.optim = torch.optim.SGD(\n","                param_list,\n","                # model.parameters(),\n","                lr = lr0,\n","                momentum = momentum,\n","                weight_decay = wd)\n","        self.warmup_factor = (self.lr0/self.warmup_start_lr)**(1./self.warmup_steps)\n","\n","    def get_lr(self):\n","        if self.it <= self.warmup_steps:\n","            lr = self.warmup_start_lr*(self.warmup_factor**self.it)\n","        else:\n","            factor = (1-(self.it-self.warmup_steps)/(self.max_iter-self.warmup_steps))**self.power\n","            lr = self.lr0 * factor\n","        return lr\n","\n","    def step(self):\n","        self.lr = self.get_lr()\n","        for pg in self.optim.param_groups:\n","            if pg.get('lr_mul', False):\n","                pg['lr'] = self.lr * 10\n","            else:\n","                pg['lr'] = self.lr\n","        if self.optim.defaults.get('lr_mul', False):\n","            self.optim.defaults['lr'] = self.lr * 10\n","        else:\n","            self.optim.defaults['lr'] = self.lr\n","        self.it += 1\n","        self.optim.step()\n","        if self.it == self.warmup_steps+2:\n","            logger.info('==> warmup done, start to implement poly lr strategy')\n","    \n","    def get_state(self):\n","        return {\n","            'warmup_steps': self.warmup_steps,\n","            'warmup_start_lr': self.warmup_start_lr,\n","            'lr0': self.lr0,\n","            'lr': self.lr,\n","            'max_iter': self.max_iter,\n","            'power': self.power, \n","            'it': self.it,\n","            'optim_state': self.optim.state_dict(),\n","            'warmup_factor': self.warmup_factor\n","        }\n","\n","    def load_state(self, state):\n","        self.warmup_steps = state.get('warmup_steps')\n","        self.warmup_start_lr = state.get('warmup_start_lr')\n","        self.lr0 = state.get('lr0')\n","        self.lr = state.get('lr')\n","        self.max_iter = state.get('max_iter')\n","        self.power = state.get('power')\n","        self.it = state.get('it')\n","        self.optim.load_state_dict(state.get('optim_state'))\n","        self.warmup_factor = state.get('warmup_factor')\n","\n","\n","    def zero_grad(self):\n","        self.optim.zero_grad()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-4mDzEJUx7x"},"outputs":[],"source":["score_thres = 0.7\n","n_img_per_gpu = 8\n","cropsize = (512, 1024)\n","n_min = n_img_per_gpu*cropsize[0]*cropsize[1]//32\n","ignore_idx=255"]},{"cell_type":"markdown","metadata":{"id":"o-tF2L-AXDcn"},"source":["---\n","## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1649227113302,"user":{"displayName":"Maxim Dobrokhvalov","userId":"05960264113093771023"},"user_tz":-180},"id":"rUhwVh6gXReD","outputId":"55c1aea1-61ad-4081-a61e-b5088f21f31f"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-06 06:38:32--  https://raw.githubusercontent.com/MichaelFan01/STDC-Seg/master/cityscapes_info.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7412 (7.2K) [text/plain]\n","Saving to: ‘cityscapes_info.json’\n","\n","\rcityscapes_info.jso   0%[                    ]       0  --.-KB/s               \rcityscapes_info.jso 100%[===================>]   7.24K  --.-KB/s    in 0s      \n","\n","2022-04-06 06:38:32 (65.5 MB/s) - ‘cityscapes_info.json’ saved [7412/7412]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/MichaelFan01/STDC-Seg/master/cityscapes_info.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hLh2hCJPJsj"},"outputs":[],"source":["from PIL import Image\n","import PIL.ImageEnhance as ImageEnhance\n","import random\n","import numpy as np\n","\n","\n","class RandomCrop(object):\n","    def __init__(self, size, *args, **kwargs):\n","        self.size = size\n","\n","    def __call__(self, im_lb):\n","        im = im_lb['im']\n","        lb = im_lb['lb']\n","        assert im.size == lb.size\n","        W, H = self.size\n","        w, h = im.size\n","\n","        if (W, H) == (w, h): return dict(im=im, lb=lb)\n","        if w < W or h < H:\n","            scale = float(W) / w if w < h else float(H) / h\n","            w, h = int(scale * w + 1), int(scale * h + 1)\n","            im = im.resize((w, h), Image.BILINEAR)\n","            lb = lb.resize((w, h), Image.NEAREST)\n","        sw, sh = random.random() * (w - W), random.random() * (h - H)\n","        crop = int(sw), int(sh), int(sw) + W, int(sh) + H\n","        return dict(\n","                im = im.crop(crop),\n","                lb = lb.crop(crop)\n","                    )\n","\n","\n","class HorizontalFlip(object):\n","    def __init__(self, p=0.5, *args, **kwargs):\n","        self.p = p\n","\n","    def __call__(self, im_lb):\n","        if random.random() > self.p:\n","            return im_lb\n","        else:\n","            im = im_lb['im']\n","            lb = im_lb['lb']\n","            return dict(im = im.transpose(Image.FLIP_LEFT_RIGHT),\n","                        lb = lb.transpose(Image.FLIP_LEFT_RIGHT),\n","                    )\n","\n","\n","class RandomScale(object):\n","    def __init__(self, scales=(1, ), *args, **kwargs):\n","        self.scales = scales\n","        # print('scales: ', scales)\n","\n","    def __call__(self, im_lb):\n","        im = im_lb['im']\n","        lb = im_lb['lb']\n","        W, H = im.size\n","        scale = random.choice(self.scales)\n","        # scale = np.random.uniform(min(self.scales), max(self.scales))\n","        w, h = int(W * scale), int(H * scale)\n","        return dict(im = im.resize((w, h), Image.BILINEAR),\n","                    lb = lb.resize((w, h), Image.NEAREST),\n","                )\n","\n","\n","class ColorJitter(object):\n","    def __init__(self, brightness=None, contrast=None, saturation=None, *args, **kwargs):\n","        if not brightness is None and brightness>0:\n","            self.brightness = [max(1-brightness, 0), 1+brightness]\n","        if not contrast is None and contrast>0:\n","            self.contrast = [max(1-contrast, 0), 1+contrast]\n","        if not saturation is None and saturation>0:\n","            self.saturation = [max(1-saturation, 0), 1+saturation]\n","\n","    def __call__(self, im_lb):\n","        im = im_lb['im']\n","        lb = im_lb['lb']\n","        r_brightness = random.uniform(self.brightness[0], self.brightness[1])\n","        r_contrast = random.uniform(self.contrast[0], self.contrast[1])\n","        r_saturation = random.uniform(self.saturation[0], self.saturation[1])\n","        im = ImageEnhance.Brightness(im).enhance(r_brightness)\n","        im = ImageEnhance.Contrast(im).enhance(r_contrast)\n","        im = ImageEnhance.Color(im).enhance(r_saturation)\n","        return dict(im = im,\n","                    lb = lb,\n","                )\n","\n","\n","class MultiScale(object):\n","    def __init__(self, scales):\n","        self.scales = scales\n","\n","    def __call__(self, img):\n","        W, H = img.size\n","        sizes = [(int(W*ratio), int(H*ratio)) for ratio in self.scales]\n","        imgs = []\n","        [imgs.append(img.resize(size, Image.BILINEAR)) for size in sizes]\n","        return imgs\n","\n","\n","class Compose(object):\n","    def __init__(self, do_list):\n","        self.do_list = do_list\n","\n","    def __call__(self, im_lb):\n","        for comp in self.do_list:\n","            im_lb = comp(im_lb)\n","        return im_lb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c88kJt45PP93"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","\n","import os.path as osp\n","import os\n","from PIL import Image\n","import numpy as np\n","import json\n","\n","\n","\n","class CityScapes(Dataset):\n","    def __init__(self, rootpth, cropsize=(640, 480), mode='train', \n","    randomscale=(0.125, 0.25, 0.375, 0.5, 0.675, 0.75, 0.875, 1.0, 1.25, 1.5), *args, **kwargs):\n","        super(CityScapes, self).__init__(*args, **kwargs)\n","        assert mode in ('train', 'val', 'test', 'trainval')\n","        self.mode = mode\n","        print('self.mode', self.mode)\n","        self.ignore_lb = 255\n","\n","        with open('./cityscapes_info.json', 'r') as fr:\n","            labels_info = json.load(fr)\n","        self.lb_map = {el['id']: el['trainId'] for el in labels_info}\n","        \n","\n","        ## parse img directory\n","        self.imgs = {}\n","        imgnames = []\n","        impth = osp.join(rootpth, 'leftImg8bit', mode)\n","        folders = os.listdir(impth)\n","        for fd in folders:\n","            fdpth = osp.join(impth, fd)\n","            im_names = os.listdir(fdpth)\n","            names = [el.replace('_leftImg8bit.png', '') for el in im_names]\n","            impths = [osp.join(fdpth, el) for el in im_names]\n","            imgnames.extend(names)\n","            self.imgs.update(dict(zip(names, impths)))\n","\n","        ## parse gt directory\n","        self.labels = {}\n","        gtnames = []\n","        gtpth = osp.join(rootpth, 'gtFine', mode)\n","        folders = os.listdir(gtpth)\n","        for fd in folders:\n","            fdpth = osp.join(gtpth, fd)\n","            lbnames = os.listdir(fdpth)\n","            lbnames = [el for el in lbnames if 'labelIds' in el]\n","            names = [el.replace('_gtFine_labelIds.png', '') for el in lbnames]\n","            lbpths = [osp.join(fdpth, el) for el in lbnames]\n","            gtnames.extend(names)\n","            self.labels.update(dict(zip(names, lbpths)))\n","\n","        self.imnames = imgnames\n","        self.len = len(self.imnames)\n","        print('self.len', self.mode, self.len)\n","        assert set(imgnames) == set(gtnames)\n","        assert set(self.imnames) == set(self.imgs.keys())\n","        assert set(self.imnames) == set(self.labels.keys())\n","\n","        ## pre-processing\n","        self.to_tensor = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","            ])\n","        self.trans_train = Compose([\n","            ColorJitter(\n","                brightness = 0.5,\n","                contrast = 0.5,\n","                saturation = 0.5),\n","            HorizontalFlip(),\n","            RandomScale(randomscale),\n","            RandomCrop(cropsize)\n","            ])\n","\n","\n","    def __getitem__(self, idx):\n","        fn  = self.imnames[idx]\n","        impth = self.imgs[fn]\n","        lbpth = self.labels[fn]\n","        img = Image.open(impth).convert('RGB')\n","        label = Image.open(lbpth)\n","        if self.mode == 'train' or self.mode == 'trainval':\n","            im_lb = dict(im = img, lb = label)\n","            im_lb = self.trans_train(im_lb)\n","            img, label = im_lb['im'], im_lb['lb']\n","        img = self.to_tensor(img)\n","        label = np.array(label).astype(np.int64)[np.newaxis, :]\n","        label = self.convert_labels(label)\n","        return img, label\n","\n","\n","    def __len__(self):\n","        return self.len\n","\n","\n","    def convert_labels(self, label):\n","        for k, v in self.lb_map.items():\n","            label[label == k] = v\n","        return label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21663,"status":"ok","timestamp":1649227135833,"user":{"displayName":"Maxim Dobrokhvalov","userId":"05960264113093771023"},"user_tz":-180},"id":"CdTbxumgrAqo","outputId":"db54c00a-4cf1-4e71-e099-733ecf2c9c31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87f6J6wdXxpA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649227164211,"user_tz":-180,"elapsed":28394,"user":{"displayName":"Maxim Dobrokhvalov","userId":"05960264113093771023"}},"outputId":"0782ca2a-374d-41b9-814a-d572e7cacbbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["self.mode train\n","self.len train 2975\n","self.mode val\n","self.len val 500\n"]}],"source":["dspth = '/content/drive/MyDrive/RnD/datasets/'\n","cfg_data = {\n","    'dataset': 'cityscapes',\n","    'train_split': 'train',\n","    'val_split': 'val',\n","    'img_rows': cropsize[0],\n","    'img_cols': cropsize[1],\n","    'path': dspth\n","}\n","randomscale = (0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0, 1.125, 1.25, 1.375, 1.5)\n","ds = CityScapes(cfg_data['path'], cropsize=cropsize, mode='train', randomscale=randomscale)\n","# sampler = torch.utils.data.distributed.DistributedSampler(ds)\n","dl = DataLoader(ds,\n","                batch_size = batch_size,\n","                shuffle = False,\n","                # sampler = sampler,\n","                num_workers = n_workers,\n","                pin_memory = False,\n","                drop_last = True)\n","# exit(0)\n","dsval = CityScapes(cfg_data['path'], mode='val', randomscale=randomscale)\n","# sampler_val = torch.utils.data.distributed.DistributedSampler(dsval)\n","dlval = DataLoader(dsval,\n","                batch_size = 2,\n","                shuffle = False,\n","                # sampler = sampler_val,\n","                num_workers = n_workers,\n","                drop_last = False)\n","\n","## model\n","ignore_idx = 255"]},{"cell_type":"markdown","metadata":{"id":"csU-R0brNqti"},"source":["---\n","# Preparing for train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_MVKWyjNwFH"},"outputs":[],"source":["best_iou = -100.0\n","flag = True\n","loss_all = 0\n","loss_n = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GqMaMb8PBiV"},"outputs":[],"source":["def get_logger(logdir):\n","    logger = logging.getLogger(\"DR_test\")\n","    ts = str(datetime.now()).split(\".\")[0].replace(\" \", \"_\")\n","    ts = ts.replace(\":\", \"_\").replace(\"-\", \"_\")\n","    file_path = os.path.join(logdir, \"run_{}.log\".format(ts))\n","    hdlr = logging.FileHandler(file_path)\n","    formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n","    hdlr.setFormatter(formatter)\n","    logger.addHandler(hdlr)\n","    logger.setLevel(logging.INFO)\n","    return logger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCjt4GHxQV-S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649227165524,"user_tz":-180,"elapsed":1322,"user":{"displayName":"Maxim Dobrokhvalov","userId":"05960264113093771023"}},"outputId":"fbf00cfa-aa5d-40f3-a0e4-90fa5bb770aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'FCHarDNet'...\n","remote: Enumerating objects: 130, done.\u001b[K\n","remote: Counting objects: 100% (13/13), done.\u001b[K\n","remote: Compressing objects: 100% (12/12), done.\u001b[K\n","remote: Total 130 (delta 2), reused 7 (delta 1), pack-reused 117\u001b[K\n","Receiving objects: 100% (130/130), 9.10 MiB | 23.00 MiB/s, done.\n","Resolving deltas: 100% (50/50), done.\n"]}],"source":["!git clone https://github.com/PingoLH/FCHarDNet.git\n","!cp -r FCHarDNet/ptsemseg ./\n","!rm -rf FCHarDNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-4aOn4dQeLT"},"outputs":[],"source":["from ptsemseg.loader import get_loader\n","from ptsemseg.metrics import runningScore, averageMeter\n","from ptsemseg.augmentations import get_composed_augmentations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJTlBpSmStgU"},"outputs":[],"source":["base_path = \"/content/drive/MyDrive/RnD/runs/HarDNet\"\n","model_modification = 'Dilated_4959'\n","model_modification_path = os.path.join(base_path, model_modification)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAoSxPyeaFio","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649227166326,"user_tz":-180,"elapsed":554,"user":{"displayName":"Maxim Dobrokhvalov","userId":"05960264113093771023"}},"outputId":"6b507f28-1236-4bf0-99bc-6a1994df1455"},"outputs":[{"output_type":"stream","name":"stdout","text":["RUNDIR: /content/drive/MyDrive/RnD/runs/HarDNet/Dilated_4959/2022-04-06 06:39:25\n"]}],"source":["logdir = os.path.join(model_modification_path, str(datetime.fromtimestamp(int(time.time()))))\n","writer = SummaryWriter(log_dir=logdir)\n","\n","print(\"RUNDIR: {}\".format(logdir))\n","\n","logger = get_logger(logdir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDPzLJpLP7au"},"outputs":[],"source":["# Setup seeds\n","torch.manual_seed(1337)\n","torch.cuda.manual_seed(1337)\n","np.random.seed(1337)\n","random.seed(1337)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G95g0arBR1AY"},"outputs":[],"source":["running_metrics_val = runningScore(n_classes)\n","\n","dilations = [1, 1, 1, 2, 2, 2, 4, 4, 4, 8, 8]\n","model = LinearHarDBlockDilated(dilations)\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        nn.init.xavier_normal_(m.weight)\n","\n","model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n","model.apply(weights_init)\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_5wFQ0UVnHt"},"outputs":[],"source":["# optimizer init data\n","momentum = 0.9\n","weight_decay = 5e-4\n","lr_start = 1e-2\n","power = 0.9\n","warmup_steps = 1000\n","warmup_start_lr = 1e-5\n","epoch_iteration = len(ds) // batch_size\n","max_epoch = 484\n","max_iter = max_epoch * epoch_iteration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rg_faMQHOnMZ"},"outputs":[],"source":["start_epoch = 0\n","it = 0\n","local_max_epoch = start_epoch + 6 if start_epoch + 6 < max_epoch else max_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfsksqZNg3TM"},"outputs":[],"source":["criteria_ffm = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1\n","boundary_loss_func = DetailAggregateLoss()                                          # out3\n","criteria_val = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1 \n","val_loss_meter = averageMeter()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFawk_jCVuJg"},"outputs":[],"source":["optim = Optimizer(\n","            model = model.module,\n","            loss = boundary_loss_func,\n","            lr0 = lr_start,\n","            momentum = momentum,\n","            wd = weight_decay,\n","            warmup_steps = warmup_steps,\n","            warmup_start_lr = warmup_start_lr,\n","            max_iter = max_iter,\n","            power = power)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcCTdTNiWhH3"},"outputs":[],"source":["loss_avg = []\n","loss_boundery_bce = []\n","loss_boundery_dice = []"]},{"cell_type":"markdown","metadata":{"id":"2aFTxfMXSqw3"},"source":["---\n","## Restore state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuw_ClrzSqJM"},"outputs":[],"source":["runs = sorted(os.listdir(model_modification_path), reverse=True)\n","best_path = None\n","last_path = None\n","for run in runs:\n","    tmp_base = os.path.join(model_modification_path, run)\n","    model_name = \"{}_{}\".format(model_arch, cfg_data['dataset'])\n","    checkpoint = os.path.join(tmp_base, model_name+'_checkpoint.pkl')\n","    best = os.path.join(tmp_base, model_name+'_best_model.pkl')\n","    if not last_path and os.path.exists(checkpoint):\n","        last_path = checkpoint\n","    if not best_path and os.path.exists(best):\n","        best_path = best\n","    if last_path and best_path:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jc6LeN9TOCq"},"outputs":[],"source":["loaded = torch.load(last_path)\n","best_iou_arrc = torch.load(best_path)\n","model_state = loaded.get('model_state')\n","\n","optimizer_state = loaded.get('optimizer_state')\n","start_epoch = loaded.get('epoch') + 1\n","local_max_epoch = start_epoch + 12\n","\n","best_iou = best_iou_arrc.get('best_iou')\n","i = 0\n","flag = True\n","loss_all = 0\n","loss_n = 0\n","\n","model.load_state_dict(model_state)\n","optim.load_state(optimizer_state)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4HbAmnvVMFA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649227179017,"user_tz":-180,"elapsed":32,"user":{"displayName":"Maxim Dobrokhvalov","userId":"05960264113093771023"}},"outputId":"e05f456e-5cfc-4259-f1ea-bfeb35a76fc9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12, 24, 0.2444844925521009)"]},"metadata":{},"execution_count":37}],"source":["file_ = list(filter(lambda x: x.endswith('.log'), os.listdir(logdir)))[0]\n","file_ = os.path.join(logdir, file_)\n","if not flag and osp.isfile(file_):\n","    with open(file_, \"r\") as f:\n","        str_ = f.readlines()[-24]\n","        st__, end__ = str_.find('Epoch') + 6, str_.find(' Iter')\n","        if st__ > -1 and end__ > -1 and local_max_epoch - 1 == int(str_[st__:end__]):\n","            start_epoch = local_max_epoch\n","            local_max_epoch += 12\n","\n","\n","start_epoch, local_max_epoch, best_iou"]},{"cell_type":"markdown","metadata":{"id":"1ZjxXbpI_MDX"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuAUlVp7XTSc","outputId":"decf569e-ed78-4df6-cf35-2968ec64a902"},"outputs":[{"output_type":"stream","name":"stdout","text":["loss/train_loss 1.891932725906372 10\n","loss/train_loss 1.7107865810394287 20\n","loss/train_loss 2.4063501358032227 30\n","loss/train_loss 1.8365590572357178 40\n","loss/train_loss 1.7762398719787598 50\n","loss/train_loss 2.1504526138305664 60\n","loss/train_loss 1.9234713315963745 70\n","loss/train_loss 1.776898980140686 80\n","loss/train_loss 1.8373000621795654 90\n","loss/train_loss 2.1039223670959473 100\n","loss/train_loss 1.6998591423034668 110\n","loss/train_loss 2.2428207397460938 120\n","loss/train_loss 1.9891433715820312 130\n","loss/train_loss 2.0237367153167725 140\n","loss/train_loss 2.2747228145599365 150\n","loss/train_loss 1.9942841529846191 160\n","loss/train_loss 2.074845314025879 170\n","loss/train_loss 2.5347585678100586 180\n","loss/train_loss 1.9456737041473389 190\n","loss/train_loss 2.2065742015838623 200\n","loss/train_loss 2.0525262355804443 210\n","loss/train_loss 2.0738396644592285 220\n","loss/train_loss 2.134902000427246 230\n","loss/train_loss 2.004265308380127 240\n","loss/train_loss 2.2432117462158203 250\n","loss/train_loss 1.6663174629211426 260\n","loss/train_loss 1.9885066747665405 270\n","loss/train_loss 2.0582027435302734 280\n","loss/train_loss 2.087480306625366 290\n","loss/train_loss 2.200568675994873 300\n","loss/train_loss 2.0952470302581787 310\n","loss/train_loss 2.234443426132202 320\n","loss/train_loss 2.0752511024475098 330\n","loss/train_loss 2.2646265029907227 340\n","loss/train_loss 2.015416145324707 350\n","loss/train_loss 2.1645350456237793 360\n","loss/train_loss 1.6416726112365723 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.1044887354373931 371\n","Overall Acc: \t 0.7713476658157107\n","val_metrics/Overall Acc: \t 0.7713476658157107 371\n","Mean Acc : \t 0.29498187236064793\n","val_metrics/Mean Acc : \t 0.29498187236064793 371\n","FreqW Acc : \t 0.6349412376566053\n","val_metrics/FreqW Acc : \t 0.6349412376566053 371\n","Mean IoU : \t 0.22937323827338685\n","val_metrics/Mean IoU : \t 0.22937323827338685 371\n","val_metrics/cls_0 0.8435916734431725 371\n","val_metrics/cls_1 0.3969168844926515 371\n","val_metrics/cls_2 0.6149863352792496 371\n","val_metrics/cls_3 1.9046808424522407e-05 371\n","val_metrics/cls_4 0.02090179495898783 371\n","val_metrics/cls_5 0.08317307695771617 371\n","val_metrics/cls_6 0.0 371\n","val_metrics/cls_7 0.18400801782312765 371\n","val_metrics/cls_8 0.5433497072392005 371\n","val_metrics/cls_9 0.10306154713194493 371\n","val_metrics/cls_10 0.7642352176658639 371\n","val_metrics/cls_11 0.20141376937980954 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.5327375632660715 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.06969689274812971 371\n","loss/train_loss 2.0659677982330322 10\n","loss/train_loss 1.9059804677963257 20\n","loss/train_loss 1.8835026025772095 30\n","loss/train_loss 2.151297092437744 40\n","loss/train_loss 1.8010975122451782 50\n","loss/train_loss 2.3230905532836914 60\n","loss/train_loss 1.9820573329925537 70\n","loss/train_loss 1.966918706893921 80\n","loss/train_loss 1.7853788137435913 90\n","loss/train_loss 2.2923386096954346 100\n","loss/train_loss 1.7840008735656738 110\n","loss/train_loss 1.9034560918807983 120\n","loss/train_loss 2.026845932006836 130\n","loss/train_loss 2.105833053588867 140\n","loss/train_loss 2.3524465560913086 150\n","loss/train_loss 1.8673030138015747 160\n","loss/train_loss 2.517481803894043 170\n","loss/train_loss 2.184276580810547 180\n","loss/train_loss 1.797508716583252 190\n","loss/train_loss 2.1977391242980957 200\n","loss/train_loss 1.8220229148864746 210\n","loss/train_loss 2.036900520324707 220\n","loss/train_loss 2.2042458057403564 230\n","loss/train_loss 1.9104808568954468 240\n","loss/train_loss 2.0329668521881104 250\n","loss/train_loss 2.070786952972412 260\n","loss/train_loss 2.021993637084961 270\n","loss/train_loss 1.8738185167312622 280\n","loss/train_loss 2.111760377883911 290\n","loss/train_loss 2.5783777236938477 300\n","loss/train_loss 1.9435994625091553 310\n","loss/train_loss 2.452850580215454 320\n","loss/train_loss 2.182551383972168 330\n","loss/train_loss 1.8159910440444946 340\n","loss/train_loss 2.0803985595703125 350\n","loss/train_loss 2.183210849761963 360\n","loss/train_loss 2.0589964389801025 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.4026824064254761 371\n","Overall Acc: \t 0.8170972701074951\n","val_metrics/Overall Acc: \t 0.8170972701074951 371\n","Mean Acc : \t 0.3176718586313461\n","val_metrics/Mean Acc : \t 0.3176718586313461 371\n","FreqW Acc : \t 0.6934484693574638\n","val_metrics/FreqW Acc : \t 0.6934484693574638 371\n","Mean IoU : \t 0.2572720671154146\n","val_metrics/Mean IoU : \t 0.2572720671154146 371\n","val_metrics/cls_0 0.8518040743119716 371\n","val_metrics/cls_1 0.339092456706404 371\n","val_metrics/cls_2 0.7004713988678684 371\n","val_metrics/cls_3 1.681969362853632e-05 371\n","val_metrics/cls_4 0.05625743039256815 371\n","val_metrics/cls_5 0.13903493439006767 371\n","val_metrics/cls_6 0.0 371\n","val_metrics/cls_7 0.1909909745429164 371\n","val_metrics/cls_8 0.7407819215321637 371\n","val_metrics/cls_9 0.08623471643692583 371\n","val_metrics/cls_10 0.7581094089293107 371\n","val_metrics/cls_11 0.2699754597044185 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.5810800664610624 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.17431961322357245 371\n","loss/train_loss 2.242062568664551 10\n","loss/train_loss 1.8340765237808228 20\n","loss/train_loss 2.5291330814361572 30\n","loss/train_loss 2.063307285308838 40\n","loss/train_loss 1.8681044578552246 50\n","loss/train_loss 2.3148210048675537 60\n","loss/train_loss 2.0793116092681885 70\n","loss/train_loss 1.686232089996338 80\n","loss/train_loss 1.9345545768737793 90\n","loss/train_loss 2.0508062839508057 100\n","loss/train_loss 1.9049437046051025 110\n","loss/train_loss 2.1659839153289795 120\n","loss/train_loss 1.75527822971344 130\n","loss/train_loss 1.9111700057983398 140\n","loss/train_loss 2.376188039779663 150\n","loss/train_loss 1.9611525535583496 160\n","loss/train_loss 2.1015048027038574 170\n","loss/train_loss 2.405874729156494 180\n","loss/train_loss 2.054539680480957 190\n","loss/train_loss 1.9661126136779785 200\n","loss/train_loss 1.885629415512085 210\n","loss/train_loss 2.142770767211914 220\n","loss/train_loss 2.1867475509643555 230\n","loss/train_loss 1.84483802318573 240\n","loss/train_loss 2.201082229614258 250\n","loss/train_loss 1.8722046613693237 260\n","loss/train_loss 1.9531402587890625 270\n","loss/train_loss 1.9954372644424438 280\n","loss/train_loss 2.425794839859009 290\n","loss/train_loss 2.5679287910461426 300\n","loss/train_loss 2.0052409172058105 310\n","loss/train_loss 1.9851272106170654 320\n","loss/train_loss 2.5096611976623535 330\n","loss/train_loss 2.1297569274902344 340\n","loss/train_loss 1.9355093240737915 350\n","loss/train_loss 1.9939665794372559 360\n","loss/train_loss 2.0077853202819824 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.4423496203422546 371\n","Overall Acc: \t 0.8047761215859193\n","val_metrics/Overall Acc: \t 0.8047761215859193 371\n","Mean Acc : \t 0.3168713173740986\n","val_metrics/Mean Acc : \t 0.3168713173740986 371\n","FreqW Acc : \t 0.6918722064907443\n","val_metrics/FreqW Acc : \t 0.6918722064907443 371\n","Mean IoU : \t 0.2527391605127181\n","val_metrics/Mean IoU : \t 0.2527391605127181 371\n","val_metrics/cls_0 0.851057959264216 371\n","val_metrics/cls_1 0.3144481647269732 371\n","val_metrics/cls_2 0.7131785829153963 371\n","val_metrics/cls_3 0.0 371\n","val_metrics/cls_4 0.03082401136018808 371\n","val_metrics/cls_5 0.19576924162858703 371\n","val_metrics/cls_6 0.001068901324111873 371\n","val_metrics/cls_7 0.228083315440964 371\n","val_metrics/cls_8 0.7714226558152015 371\n","val_metrics/cls_9 0.09714237746010732 371\n","val_metrics/cls_10 0.8067934099129157 371\n","val_metrics/cls_11 0.25937120506504013 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.42698560806381763 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.10589861676412592 371\n","loss/train_loss 2.429248094558716 10\n","loss/train_loss 1.9160161018371582 20\n","loss/train_loss 2.091708183288574 30\n","loss/train_loss 2.104762077331543 40\n","loss/train_loss 1.7995617389678955 50\n","loss/train_loss 2.6178956031799316 60\n","loss/train_loss 1.6845488548278809 70\n","loss/train_loss 2.162148952484131 80\n","loss/train_loss 1.7777048349380493 90\n","loss/train_loss 1.900565266609192 100\n","loss/train_loss 1.886153221130371 110\n","loss/train_loss 2.1930418014526367 120\n","loss/train_loss 1.7770609855651855 130\n","loss/train_loss 1.9878735542297363 140\n","loss/train_loss 2.184328556060791 150\n","loss/train_loss 2.358167886734009 160\n","loss/train_loss 2.215263843536377 170\n","loss/train_loss 2.590486526489258 180\n","loss/train_loss 1.86085045337677 190\n","loss/train_loss 1.9575951099395752 200\n","loss/train_loss 1.870802402496338 210\n","loss/train_loss 1.7549588680267334 220\n","loss/train_loss 2.061558485031128 230\n","loss/train_loss 1.6633247137069702 240\n","loss/train_loss 1.6622196435928345 250\n","loss/train_loss 1.8803801536560059 260\n","loss/train_loss 1.7820820808410645 270\n","loss/train_loss 2.268883466720581 280\n","loss/train_loss 2.1157822608947754 290\n","loss/train_loss 2.045414447784424 300\n","loss/train_loss 2.2660088539123535 310\n","loss/train_loss 2.276092529296875 320\n","loss/train_loss 2.2772812843322754 330\n","loss/train_loss 1.855393409729004 340\n","loss/train_loss 2.005192756652832 350\n","loss/train_loss 1.8233418464660645 360\n","loss/train_loss 1.8343980312347412 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.2140994446277618 371\n","Overall Acc: \t 0.835299264069691\n","val_metrics/Overall Acc: \t 0.835299264069691 371\n","Mean Acc : \t 0.3498000790355799\n","val_metrics/Mean Acc : \t 0.3498000790355799 371\n","FreqW Acc : \t 0.7284343965917027\n","val_metrics/FreqW Acc : \t 0.7284343965917027 371\n","Mean IoU : \t 0.2879373469564461\n","val_metrics/Mean IoU : \t 0.2879373469564461 371\n","val_metrics/cls_0 0.8768707109077174 371\n","val_metrics/cls_1 0.34291378664481703 371\n","val_metrics/cls_2 0.7217192385037232 371\n","val_metrics/cls_3 0.0007061411210752988 371\n","val_metrics/cls_4 0.10428485471173528 371\n","val_metrics/cls_5 0.2096959560741161 371\n","val_metrics/cls_6 5.529065189890215e-05 371\n","val_metrics/cls_7 0.19798491496558768 371\n","val_metrics/cls_8 0.8072593572056591 371\n","val_metrics/cls_9 0.1514814913019118 371\n","val_metrics/cls_10 0.809882542794833 371\n","val_metrics/cls_11 0.3507812867071494 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.6386502335988219 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.2585237869834306 371\n","loss/train_loss 2.2788610458374023 10\n","loss/train_loss 1.907734751701355 20\n","loss/train_loss 2.1638755798339844 30\n","loss/train_loss 2.0792689323425293 40\n","loss/train_loss 2.0597922801971436 50\n","loss/train_loss 2.162428140640259 60\n","loss/train_loss 1.912506103515625 70\n","loss/train_loss 2.1724228858947754 80\n","loss/train_loss 1.7095187902450562 90\n","loss/train_loss 1.981939435005188 100\n","loss/train_loss 1.7728605270385742 110\n","loss/train_loss 2.291614055633545 120\n","loss/train_loss 1.919853925704956 130\n","loss/train_loss 2.093754529953003 140\n","loss/train_loss 2.171588659286499 150\n","loss/train_loss 2.2725555896759033 160\n","loss/train_loss 2.2090678215026855 170\n","loss/train_loss 2.0309739112854004 180\n","loss/train_loss 1.74308443069458 190\n","loss/train_loss 2.2739548683166504 200\n","loss/train_loss 1.7349317073822021 210\n","loss/train_loss 1.9436691999435425 220\n","loss/train_loss 1.8525390625 230\n","loss/train_loss 2.023695468902588 240\n","loss/train_loss 1.7262964248657227 250\n","loss/train_loss 1.8748726844787598 260\n","loss/train_loss 1.9802647829055786 270\n","loss/train_loss 2.110156774520874 280\n","loss/train_loss 2.4042506217956543 290\n","loss/train_loss 2.3620715141296387 300\n","loss/train_loss 2.198173999786377 310\n","loss/train_loss 2.0087881088256836 320\n","loss/train_loss 2.0083680152893066 330\n","loss/train_loss 1.876128911972046 340\n","loss/train_loss 1.9787330627441406 350\n","loss/train_loss 1.8632380962371826 360\n","loss/train_loss 1.8492751121520996 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.234808676481247 371\n","Overall Acc: \t 0.8452060239758153\n","val_metrics/Overall Acc: \t 0.8452060239758153 371\n","Mean Acc : \t 0.36481489589073585\n","val_metrics/Mean Acc : \t 0.36481489589073585 371\n","FreqW Acc : \t 0.7401508871445331\n","val_metrics/FreqW Acc : \t 0.7401508871445331 371\n","Mean IoU : \t 0.2970861794676732\n","val_metrics/Mean IoU : \t 0.2970861794676732 371\n","val_metrics/cls_0 0.8895075234905812 371\n","val_metrics/cls_1 0.42368514868615903 371\n","val_metrics/cls_2 0.7388380929804954 371\n","val_metrics/cls_3 0.0009560404420731004 371\n","val_metrics/cls_4 0.10069541829997584 371\n","val_metrics/cls_5 0.25800175832929273 371\n","val_metrics/cls_6 0.0014603761048861796 371\n","val_metrics/cls_7 0.24494644512674696 371\n","val_metrics/cls_8 0.8094594773881967 371\n","val_metrics/cls_9 0.21067120209188273 371\n","val_metrics/cls_10 0.7338081253424379 371\n","val_metrics/cls_11 0.3288431797761166 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.6344356205187045 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.2693290013082421 371\n","loss/train_loss 2.1278109550476074 10\n","loss/train_loss 1.768134355545044 20\n","loss/train_loss 2.698660373687744 30\n","loss/train_loss 1.8562848567962646 40\n","loss/train_loss 1.8406739234924316 50\n","loss/train_loss 2.420006513595581 60\n","loss/train_loss 1.8625625371932983 70\n","loss/train_loss 2.267673969268799 80\n","loss/train_loss 1.810575008392334 90\n","loss/train_loss 2.4463095664978027 100\n","loss/train_loss 1.7904759645462036 110\n","loss/train_loss 2.2076921463012695 120\n","loss/train_loss 1.8474395275115967 130\n","loss/train_loss 1.9065048694610596 140\n","loss/train_loss 2.4581503868103027 150\n","loss/train_loss 1.7907631397247314 160\n","loss/train_loss 2.119748830795288 170\n","loss/train_loss 2.0977680683135986 180\n","loss/train_loss 2.0485172271728516 190\n","loss/train_loss 1.999859094619751 200\n","loss/train_loss 1.8646303415298462 210\n","loss/train_loss 2.1714348793029785 220\n","loss/train_loss 2.537986993789673 230\n","loss/train_loss 1.6759073734283447 240\n","loss/train_loss 2.100599527359009 250\n","loss/train_loss 1.6874761581420898 260\n","loss/train_loss 2.0716798305511475 270\n","loss/train_loss 1.9247987270355225 280\n","loss/train_loss 2.2429542541503906 290\n","loss/train_loss 2.251500129699707 300\n","loss/train_loss 2.3723485469818115 310\n","loss/train_loss 1.7762691974639893 320\n","loss/train_loss 2.098146915435791 330\n","loss/train_loss 2.087392807006836 340\n","loss/train_loss 2.3229856491088867 350\n","loss/train_loss 1.802152395248413 360\n","loss/train_loss 1.8594317436218262 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.346312938451767 371\n","Overall Acc: \t 0.8256971152519478\n","val_metrics/Overall Acc: \t 0.8256971152519478 371\n","Mean Acc : \t 0.34708127874512346\n","val_metrics/Mean Acc : \t 0.34708127874512346 371\n","FreqW Acc : \t 0.715503246421316\n","val_metrics/FreqW Acc : \t 0.715503246421316 371\n","Mean IoU : \t 0.27519993420108974\n","val_metrics/Mean IoU : \t 0.27519993420108974 371\n","val_metrics/cls_0 0.8840208947158329 371\n","val_metrics/cls_1 0.4064356132222273 371\n","val_metrics/cls_2 0.7138152389772682 371\n","val_metrics/cls_3 0.0017477214945211793 371\n","val_metrics/cls_4 0.08449928262141138 371\n","val_metrics/cls_5 0.24165704934220006 371\n","val_metrics/cls_6 0.0021268897106194455 371\n","val_metrics/cls_7 0.19545122931321465 371\n","val_metrics/cls_8 0.7402215368054466 371\n","val_metrics/cls_9 0.13966519831717925 371\n","val_metrics/cls_10 0.7866646472492108 371\n","val_metrics/cls_11 0.2732561165536861 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.5843273653537274 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.1749099661441594 371\n","loss/train_loss 2.2362163066864014 10\n","loss/train_loss 1.8458980321884155 20\n","loss/train_loss 2.7241368293762207 30\n","loss/train_loss 1.959708333015442 40\n","loss/train_loss 1.8341881036758423 50\n","loss/train_loss 2.29056978225708 60\n","loss/train_loss 1.8693146705627441 70\n","loss/train_loss 1.916613221168518 80\n","loss/train_loss 1.6754637956619263 90\n","loss/train_loss 1.8770568370819092 100\n","loss/train_loss 1.8963466882705688 110\n","loss/train_loss 2.076676845550537 120\n","loss/train_loss 1.7873784303665161 130\n","loss/train_loss 1.9389164447784424 140\n","loss/train_loss 2.487457513809204 150\n","loss/train_loss 1.6746799945831299 160\n","loss/train_loss 2.45760440826416 170\n","loss/train_loss 2.266205310821533 180\n","loss/train_loss 2.0879018306732178 190\n","loss/train_loss 1.9735356569290161 200\n","loss/train_loss 1.803026795387268 210\n","loss/train_loss 2.159850835800171 220\n","loss/train_loss 2.1990928649902344 230\n","loss/train_loss 1.8145349025726318 240\n","loss/train_loss 1.9456820487976074 250\n","loss/train_loss 1.7672476768493652 260\n","loss/train_loss 1.7798889875411987 270\n","loss/train_loss 1.9783387184143066 280\n","loss/train_loss 2.2519149780273438 290\n","loss/train_loss 1.991604208946228 300\n","loss/train_loss 1.9329451322555542 310\n","loss/train_loss 2.2978134155273438 320\n","loss/train_loss 2.164788246154785 330\n","loss/train_loss 2.2139039039611816 340\n","loss/train_loss 2.0202136039733887 350\n","loss/train_loss 1.891217589378357 360\n","loss/train_loss 1.9890131950378418 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.353094880104065 371\n","Overall Acc: \t 0.8297618086520391\n","val_metrics/Overall Acc: \t 0.8297618086520391 371\n","Mean Acc : \t 0.348873917752925\n","val_metrics/Mean Acc : \t 0.348873917752925 371\n","FreqW Acc : \t 0.7206251503692468\n","val_metrics/FreqW Acc : \t 0.7206251503692468 371\n","Mean IoU : \t 0.2834101357678955\n","val_metrics/Mean IoU : \t 0.2834101357678955 371\n","val_metrics/cls_0 0.8715962979707852 371\n","val_metrics/cls_1 0.46697963297528844 371\n","val_metrics/cls_2 0.7147702172798395 371\n","val_metrics/cls_3 0.0016145409163939224 371\n","val_metrics/cls_4 0.062082958220521754 371\n","val_metrics/cls_5 0.16019864027762554 371\n","val_metrics/cls_6 0.006821477205691823 371\n","val_metrics/cls_7 0.26652950655905855 371\n","val_metrics/cls_8 0.7899109382498782 371\n","val_metrics/cls_9 0.20641724068789527 371\n","val_metrics/cls_10 0.7808368639624366 371\n","val_metrics/cls_11 0.32617454382967226 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.5459367767447009 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.18492294471022555 371\n","loss/train_loss 2.2825136184692383 10\n","loss/train_loss 1.7671808004379272 20\n","loss/train_loss 2.3275156021118164 30\n","loss/train_loss 1.900483250617981 40\n","loss/train_loss 1.7448915243148804 50\n","loss/train_loss 2.227651357650757 60\n","loss/train_loss 1.8334605693817139 70\n","loss/train_loss 1.8541805744171143 80\n","loss/train_loss 1.8637323379516602 90\n","loss/train_loss 1.7797878980636597 100\n","loss/train_loss 1.702183485031128 110\n","loss/train_loss 2.161566734313965 120\n","loss/train_loss 2.055471897125244 130\n","loss/train_loss 2.042154312133789 140\n","loss/train_loss 1.8155779838562012 150\n","loss/train_loss 1.676182508468628 160\n","loss/train_loss 2.1782193183898926 170\n","loss/train_loss 1.9978224039077759 180\n","loss/train_loss 1.8544871807098389 190\n","loss/train_loss 1.8975284099578857 200\n","loss/train_loss 1.927661657333374 210\n","loss/train_loss 2.320889472961426 220\n","loss/train_loss 2.404572010040283 230\n","loss/train_loss 1.7338290214538574 240\n","loss/train_loss 2.013082981109619 250\n","loss/train_loss 1.8995417356491089 260\n","loss/train_loss 2.1011266708374023 270\n","loss/train_loss 1.799142599105835 280\n","loss/train_loss 2.350850820541382 290\n","loss/train_loss 1.8124079704284668 300\n","loss/train_loss 2.4461755752563477 310\n","loss/train_loss 1.8990086317062378 320\n","loss/train_loss 2.633385181427002 330\n","loss/train_loss 1.8737704753875732 340\n","loss/train_loss 1.9648823738098145 350\n","loss/train_loss 2.17100191116333 360\n","loss/train_loss 1.8040629625320435 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.2817266163825989 371\n","Overall Acc: \t 0.8340347192280002\n","val_metrics/Overall Acc: \t 0.8340347192280002 371\n","Mean Acc : \t 0.3844625707473001\n","val_metrics/Mean Acc : \t 0.3844625707473001 371\n","FreqW Acc : \t 0.735464152207679\n","val_metrics/FreqW Acc : \t 0.735464152207679 371\n","Mean IoU : \t 0.2979244531538586\n","val_metrics/Mean IoU : \t 0.2979244531538586 371\n","val_metrics/cls_0 0.8856405101944046 371\n","val_metrics/cls_1 0.46481270762994187 371\n","val_metrics/cls_2 0.7481727266012569 371\n","val_metrics/cls_3 0.00014213987337495428 371\n","val_metrics/cls_4 0.07670809791151023 371\n","val_metrics/cls_5 0.2656533298058494 371\n","val_metrics/cls_6 0.01665405329055228 371\n","val_metrics/cls_7 0.30090047893658656 371\n","val_metrics/cls_8 0.7848960493549293 371\n","val_metrics/cls_9 0.2446047501444092 371\n","val_metrics/cls_10 0.8172729575251758 371\n","val_metrics/cls_11 0.2969707196699867 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.5449783794405384 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.21315770954479796 371\n","loss/train_loss 2.321143627166748 10\n","loss/train_loss 1.7763222455978394 20\n","loss/train_loss 1.9042657613754272 30\n","loss/train_loss 1.9414680004119873 40\n","loss/train_loss 1.7934751510620117 50\n","loss/train_loss 2.372490882873535 60\n","loss/train_loss 2.0024166107177734 70\n","loss/train_loss 1.6792635917663574 80\n","loss/train_loss 1.9500677585601807 90\n","loss/train_loss 1.9899654388427734 100\n","loss/train_loss 1.8653831481933594 110\n","loss/train_loss 2.24644136428833 120\n","loss/train_loss 1.8193002939224243 130\n","loss/train_loss 1.8987568616867065 140\n","loss/train_loss 1.9184520244598389 150\n","loss/train_loss 2.4544076919555664 160\n","loss/train_loss 2.078322172164917 170\n","loss/train_loss 2.064542531967163 180\n","loss/train_loss 2.1279520988464355 190\n","loss/train_loss 2.164522409439087 200\n","loss/train_loss 1.914378046989441 210\n","loss/train_loss 2.112952470779419 220\n","loss/train_loss 2.0077571868896484 230\n","loss/train_loss 1.6945066452026367 240\n","loss/train_loss 2.167778491973877 250\n","loss/train_loss 1.6145813465118408 260\n","loss/train_loss 2.29545521736145 270\n","loss/train_loss 1.9402751922607422 280\n","loss/train_loss 2.3795602321624756 290\n","loss/train_loss 2.32197904586792 300\n","loss/train_loss 2.0460853576660156 310\n","loss/train_loss 1.9119997024536133 320\n","loss/train_loss 2.605318546295166 330\n","loss/train_loss 2.008692502975464 340\n","loss/train_loss 2.1509037017822266 350\n","loss/train_loss 2.156419277191162 360\n","loss/train_loss 2.0437755584716797 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.2101569290161134 371\n","Overall Acc: \t 0.8451577425065417\n","val_metrics/Overall Acc: \t 0.8451577425065417 371\n","Mean Acc : \t 0.38464936150142975\n","val_metrics/Mean Acc : \t 0.38464936150142975 371\n","FreqW Acc : \t 0.7452076727318059\n","val_metrics/FreqW Acc : \t 0.7452076727318059 371\n","Mean IoU : \t 0.3116661017299957\n","val_metrics/Mean IoU : \t 0.3116661017299957 371\n","val_metrics/cls_0 0.8939517295769516 371\n","val_metrics/cls_1 0.48478624268959225 371\n","val_metrics/cls_2 0.736716816423319 371\n","val_metrics/cls_3 0.0034002939123755896 371\n","val_metrics/cls_4 0.12430987251555843 371\n","val_metrics/cls_5 0.23235150126989101 371\n","val_metrics/cls_6 0.013827148000860141 371\n","val_metrics/cls_7 0.2761666056598992 371\n","val_metrics/cls_8 0.7819852876427047 371\n","val_metrics/cls_9 0.21877211940734678 371\n","val_metrics/cls_10 0.8241890610755914 371\n","val_metrics/cls_11 0.36911364733008895 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.6551936982652325 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 0.0 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.3068919091005069 371\n","loss/train_loss 1.8815624713897705 10\n","loss/train_loss 1.7086232900619507 20\n","loss/train_loss 2.184419631958008 30\n","loss/train_loss 2.0300252437591553 40\n","loss/train_loss 1.9816265106201172 50\n","loss/train_loss 2.3566272258758545 60\n","loss/train_loss 2.1401357650756836 70\n","loss/train_loss 1.8679548501968384 80\n","loss/train_loss 1.6178545951843262 90\n","loss/train_loss 1.7723627090454102 100\n","loss/train_loss 1.9594333171844482 110\n","loss/train_loss 2.2211928367614746 120\n","loss/train_loss 1.8077195882797241 130\n","loss/train_loss 1.717651605606079 140\n","loss/train_loss 2.4535973072052 150\n","loss/train_loss 1.7035926580429077 160\n","loss/train_loss 2.0376944541931152 170\n","loss/train_loss 2.210916042327881 180\n","loss/train_loss 1.8824143409729004 190\n","loss/train_loss 2.2890989780426025 200\n","loss/train_loss 1.818385124206543 210\n","loss/train_loss 2.058690309524536 220\n","loss/train_loss 1.7437994480133057 230\n","loss/train_loss 1.7142468690872192 240\n","loss/train_loss 2.1696090698242188 250\n","loss/train_loss 1.8847042322158813 260\n","loss/train_loss 1.7640585899353027 270\n","loss/train_loss 2.1607816219329834 280\n","loss/train_loss 2.3192572593688965 290\n","loss/train_loss 2.3404555320739746 300\n","loss/train_loss 1.9120784997940063 310\n","loss/train_loss 2.038064956665039 320\n","loss/train_loss 1.990518569946289 330\n","loss/train_loss 1.8575918674468994 340\n","loss/train_loss 2.064323902130127 350\n","loss/train_loss 1.8017951250076294 360\n","loss/train_loss 1.8527966737747192 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.2475646562576295 371\n","Overall Acc: \t 0.852878005603658\n","val_metrics/Overall Acc: \t 0.852878005603658 371\n","Mean Acc : \t 0.3625609630006646\n","val_metrics/Mean Acc : \t 0.3625609630006646 371\n","FreqW Acc : \t 0.7507595728310277\n","val_metrics/FreqW Acc : \t 0.7507595728310277 371\n","Mean IoU : \t 0.3085541634277009\n","val_metrics/Mean IoU : \t 0.3085541634277009 371\n","val_metrics/cls_0 0.8954286664129115 371\n","val_metrics/cls_1 0.5297775674918501 371\n","val_metrics/cls_2 0.7236519142752755 371\n","val_metrics/cls_3 0.016676263009973415 371\n","val_metrics/cls_4 0.10130131689167185 371\n","val_metrics/cls_5 0.23923100258572855 371\n","val_metrics/cls_6 0.005386441700372206 371\n","val_metrics/cls_7 0.23367699111541676 371\n","val_metrics/cls_8 0.8056061404463413 371\n","val_metrics/cls_9 0.17201975846717782 371\n","val_metrics/cls_10 0.8055305618429002 371\n","val_metrics/cls_11 0.31879538149735065 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.7054045315244301 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 3.816813174291967e-05 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.31000439973317473 371\n","loss/train_loss 2.044884443283081 10\n","loss/train_loss 1.7359552383422852 20\n","loss/train_loss 2.1375908851623535 30\n","loss/train_loss 1.936788558959961 40\n","loss/train_loss 1.9021469354629517 50\n","loss/train_loss 2.270233392715454 60\n","loss/train_loss 1.827183485031128 70\n","loss/train_loss 2.215059280395508 80\n","loss/train_loss 1.7488174438476562 90\n","loss/train_loss 1.6342120170593262 100\n","loss/train_loss 1.737119197845459 110\n","loss/train_loss 2.157400131225586 120\n","loss/train_loss 1.9906954765319824 130\n","loss/train_loss 2.036435127258301 140\n","loss/train_loss 2.054814100265503 150\n","loss/train_loss 2.120821952819824 160\n","loss/train_loss 2.2268905639648438 170\n","loss/train_loss 2.015577793121338 180\n","loss/train_loss 1.8323559761047363 190\n","loss/train_loss 2.06145977973938 200\n","loss/train_loss 1.8365226984024048 210\n","loss/train_loss 2.3661580085754395 220\n","loss/train_loss 2.0697391033172607 230\n","loss/train_loss 1.7608747482299805 240\n","loss/train_loss 2.3467602729797363 250\n","loss/train_loss 1.7407803535461426 260\n","loss/train_loss 2.2648749351501465 270\n","loss/train_loss 2.0529944896698 280\n","loss/train_loss 2.298783779144287 290\n","loss/train_loss 2.3018617630004883 300\n","loss/train_loss 1.8572362661361694 310\n","loss/train_loss 2.157912254333496 320\n","loss/train_loss 2.203469753265381 330\n","loss/train_loss 1.9777060747146606 340\n","loss/train_loss 2.1824076175689697 350\n","loss/train_loss 1.811316967010498 360\n","loss/train_loss 1.6936824321746826 370\n","validation\n","50\n","100\n","150\n","200\n","250\n","loss/val_loss 1.3608403112888336 371\n","Overall Acc: \t 0.8211288954720302\n","val_metrics/Overall Acc: \t 0.8211288954720302 371\n","Mean Acc : \t 0.3778088353094355\n","val_metrics/Mean Acc : \t 0.3778088353094355 371\n","FreqW Acc : \t 0.7106990101590669\n","val_metrics/FreqW Acc : \t 0.7106990101590669 371\n","Mean IoU : \t 0.28511215094182435\n","val_metrics/Mean IoU : \t 0.28511215094182435 371\n","val_metrics/cls_0 0.8570689125683586 371\n","val_metrics/cls_1 0.4345801210666643 371\n","val_metrics/cls_2 0.7343291759397524 371\n","val_metrics/cls_3 0.004677653322544804 371\n","val_metrics/cls_4 0.10415550689265973 371\n","val_metrics/cls_5 0.23442907931834453 371\n","val_metrics/cls_6 0.015224282931501423 371\n","val_metrics/cls_7 0.314481598011522 371\n","val_metrics/cls_8 0.7468319211691892 371\n","val_metrics/cls_9 0.21693426980934902 371\n","val_metrics/cls_10 0.5778465323977395 371\n","val_metrics/cls_11 0.3132025752070349 371\n","val_metrics/cls_12 0.0 371\n","val_metrics/cls_13 0.6255513086684562 371\n","val_metrics/cls_14 0.0 371\n","val_metrics/cls_15 8.419587327959765e-07 371\n","val_metrics/cls_16 0.0 371\n","val_metrics/cls_17 0.0 371\n","val_metrics/cls_18 0.23781708863281223 371\n","loss/train_loss 2.1289191246032715 10\n","loss/train_loss 1.8298497200012207 20\n","loss/train_loss 1.948218822479248 30\n","loss/train_loss 2.0567662715911865 40\n","loss/train_loss 1.755557656288147 50\n","loss/train_loss 2.111056327819824 60\n","loss/train_loss 1.9717055559158325 70\n","loss/train_loss 1.987771987915039 80\n","loss/train_loss 1.8882687091827393 90\n","loss/train_loss 1.8928008079528809 100\n","loss/train_loss 1.778527021408081 110\n","loss/train_loss 2.1698882579803467 120\n","loss/train_loss 1.9691272974014282 130\n","loss/train_loss 1.7623894214630127 140\n","loss/train_loss 2.181368350982666 150\n","loss/train_loss 1.7192902565002441 160\n","loss/train_loss 2.256154775619507 170\n","loss/train_loss 2.108325719833374 180\n","loss/train_loss 1.631763219833374 190\n","loss/train_loss 2.0080366134643555 200\n","loss/train_loss 1.6695764064788818 210\n","loss/train_loss 1.987508773803711 220\n","loss/train_loss 1.9616153240203857 230\n","loss/train_loss 2.0339274406433105 240\n","loss/train_loss 2.187073230743408 250\n","loss/train_loss 1.8183040618896484 260\n","loss/train_loss 1.885839581489563 270\n","loss/train_loss 2.0916671752929688 280\n","loss/train_loss 2.129514694213867 290\n","loss/train_loss 2.5576658248901367 300\n","loss/train_loss 2.011439800262451 310\n","loss/train_loss 1.9286733865737915 320\n","loss/train_loss 2.5883524417877197 330\n","loss/train_loss 1.7037742137908936 340\n","loss/train_loss 1.953454852104187 350\n","loss/train_loss 1.8942335844039917 360\n","loss/train_loss 1.6636632680892944 370\n","validation\n","50\n","100\n","150\n"]}],"source":["st = glob_st = time.time()\n","flag = False\n","for epoch_id in range(start_epoch, local_max_epoch):\n","    for images, labels in dl:\n","        it += 1\n","        start_ts = time.time()\n","        \n","        model.train()\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        labels = torch.squeeze(labels, 1)\n"," \n","        optim.zero_grad()\n","\n","        out_main, out_detail = model(images)\n"," \n","        loss_ffm = criteria_ffm(out_main, labels)\n","        \n","        boundery_bce, boundery_dice = boundary_loss_func(out_detail, labels)\n","\n","        boundery_bce_loss = boundery_bce\n","        boundery_dice_loss = boundery_dice\n","\n","        loss = loss_ffm + boundery_bce_loss + boundery_dice_loss\n","\n","        loss.backward()\n","        optim.step()\n","\n","        loss_avg.append(loss.item())\n","\n","        loss_boundery_bce.append(boundery_bce_loss.item())\n","        loss_boundery_dice.append(boundery_dice_loss.item())\n","\n","        if (it + 1) % print_interval == 0:\n","            loss_avg = sum(loss_avg) / len(loss_avg)\n","            lr = optim.lr\n","            ed = time.time()\n","            t_intv, glob_t_intv = ed - st, ed - glob_st\n","            eta = int((max_iter - it) * (glob_t_intv / it))\n","            eta = str(timedelta(seconds=eta))\n","\n","            loss_boundery_bce_avg = sum(loss_boundery_bce) / len(loss_boundery_bce)\n","            loss_boundery_dice_avg = sum(loss_boundery_dice) / len(loss_boundery_dice)\n","            msg = ', '.join([\n","                'epoch: {epoch}/{max_epoch}'\n","                'it: {it}/{max_it}',\n","                'lr: {lr:4f}',\n","                'loss: {loss:.4f}',\n","                'boundery_bce_loss: {boundery_bce_loss:.4f}',\n","                'boundery_dice_loss: {boundery_dice_loss:.4f}',\n","                'eta: {eta}',\n","                'time: {time:.4f}',\n","            ]).format(\n","                epoch = epoch_id,\n","                max_epoch = max_epoch,\n","                it = it+1,\n","                max_it = epoch_iteration,\n","                lr = lr,\n","                loss = loss_avg,\n","                boundery_bce_loss = loss_boundery_bce_avg,\n","                boundery_dice_loss = loss_boundery_dice_avg,\n","                time = t_intv,\n","                eta = eta\n","            )\n","            \n","            logger.info(msg)\n","            print(\"loss/train_loss\", loss.item(), it + 1)\n","            loss_avg = []\n","            loss_boundery_bce = []\n","            loss_boundery_dice = []\n","            st = ed\n","\n","        if ((it + 1) % val_interval == 0 and it + 10 < epoch_iteration) or (it + 1) % epoch_iteration == 0:\n","            print('validation')\n","            torch.cuda.empty_cache()\n","            model.eval()\n","            loss_all = 0\n","            loss_n = 0\n","            with torch.no_grad():\n","                for i_val, (images_val, labels_val) in enumerate(dlval):\n","                    if (i_val + 1) % 50 == 0:\n","                        print(i_val + 1)\n","\n","                    images_val = images_val.to(device)\n","                    labels_val = labels_val.to(device)\n","                    labels_val = torch.squeeze(labels_val, 1)\n","\n","                    outputs = model(images_val)[0]\n","                    val_loss = criteria_val(outputs, labels_val)\n","\n","                    pred = outputs.data.max(1)[1].cpu().numpy()\n","                    gt = labels_val.data.cpu().numpy()\n","\n","                    running_metrics_val.update(gt, pred)\n","                    val_loss_meter.update(val_loss.item())\n","\n","            print(\"loss/val_loss\", val_loss_meter.avg, it + 1)\n","            logger.info(\"Epoch %3d Iter %d Val Loss: %.4f\" % (epoch_id, it + 1, val_loss_meter.avg))\n","\n","            score, class_iou = running_metrics_val.get_scores()\n","            for k, v in score.items():\n","                print(k, v)\n","                logger.info(\"{}: {}\".format(k, v))\n","                print(\"val_metrics/{}\".format(k), v, it+ 1)\n","\n","            for k, v in class_iou.items():\n","                logger.info(\"{}: {}\".format(k, v))\n","                print(\"val_metrics/cls_{}\".format(k), v, it+ 1)\n","\n","            val_loss_meter.reset()\n","            running_metrics_val.reset()\n","\n","            state = {\n","                    \"epoch\": epoch_id,\n","                    \"iteration\": it+ 1,\n","                    \"model_state\": model.state_dict(),\n","                    \"optimizer_state\": optim.get_state(),\n","            }\n","            save_path = os.path.join(\n","                writer.file_writer.get_logdir(),\n","                \"{}_{}_checkpoint.pkl\".format(model_arch, cfg_data['dataset']),\n","            )\n","            torch.save(state, save_path)\n","\n","            if score[\"Mean IoU : \\t\"] >= best_iou:\n","                best_iou = score[\"Mean IoU : \\t\"]\n","                state = {\n","                    \"epoch\": epoch_id,\n","                    \"iteration\":it+ 1,\n","                    \"model_state\": model.state_dict(),\n","                    \"best_iou\": best_iou,\n","                }\n","                save_path = os.path.join(\n","                    writer.file_writer.get_logdir(),\n","                    \"{}_{}_best_model.pkl\".format(model_arch, cfg_data['dataset']),\n","                )\n","                torch.save(state, save_path)\n","            torch.cuda.empty_cache()\n","    it = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH8UeQwnQlgY"},"outputs":[],"source":["# with torch.no_grad():\n","#     for (images_val, labels_val, _) in valloader:\n","#         images_val = images_val.to(device)\n","#         labels_val = labels_val.to(device)\n","\n","#         outputs = model(images_val)\n","#         outputs = output_val_upsample(outputs)\n","#         val_loss = loss_fn(input=outputs, target=labels_val)\n","\n","#         pred = outputs.data.max(1)[1].cpu().numpy()\n","#         gt = labels_val.data.cpu().numpy()\n","\n","#         running_metrics_val.update(gt, pred)\n","#         val_loss_meter.update(val_loss.item())\n","\n","# writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n","# logger.info(\"Iter %d Val Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n","\n","# score, class_iou = running_metrics_val.get_scores()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"HarDNet_dilated.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}