{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DDRNet_HBD_DGT.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc4QzK2lMdkH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DDRNet+hardnet+DetailGT\n",
    "\n",
    "Output:\n",
    "1. `seghead(FFM(DAPPPM, layer5_)` - main\n",
    "2. `seghead(layer3_ + upsample(layer3))`\n",
    "3. `seghead(layer4_ + upsample(layer4))`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrFsKEKW7429",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637933951036,
     "user_tz": -180,
     "elapsed": 10588,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "b6161635-e593-42fe-f837-f0cab930f6ea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.utils import data\n",
    "from datetime import datetime, timedelta\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    " \n",
    "!pip install tensorboardX\n",
    "from tensorboardX import SummaryWriter"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
      "\u001B[?25l\r\u001B[K     |██▋                             | 10 kB 23.3 MB/s eta 0:00:01\r\u001B[K     |█████▎                          | 20 kB 27.7 MB/s eta 0:00:01\r\u001B[K     |███████▉                        | 30 kB 26.6 MB/s eta 0:00:01\r\u001B[K     |██████████▌                     | 40 kB 20.1 MB/s eta 0:00:01\r\u001B[K     |█████████████▏                  | 51 kB 15.0 MB/s eta 0:00:01\r\u001B[K     |███████████████▊                | 61 kB 11.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████▍             | 71 kB 12.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████           | 81 kB 13.5 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▋        | 92 kB 11.9 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▎     | 102 kB 12.5 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████   | 112 kB 12.5 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▌| 122 kB 12.5 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 124 kB 12.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.4.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LV35FwJ47ffs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "BatchNorm2d = nn.BatchNorm2d\n",
    "ReLU = nn.ReLU\n",
    "AvgPool2d = nn.AvgPool2d\n",
    "AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZvH1JPHzSxXK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_yTqA_v7SK42",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "batch_size = 8\n",
    "n_workers = 2\n",
    "print_interval=10\n",
    "val_interval=500\n",
    "\n",
    "n_classes = 19\n",
    "\n",
    "model_arch = 'ddrnet_slim_23'\n",
    "\n",
    "bn_mom = 0.1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BdAvR6h_Y43C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "float_tensor_type = torch.cuda.FloatTensor if device.type=='cuda' else torch.FloatTensor"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rlqCnjINmRs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IfQMxl1EysQu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, need_relu=True, *args, **kwargs):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_chan,\n",
    "                out_chan,\n",
    "                kernel_size = ks,\n",
    "                stride = stride,\n",
    "                padding = padding,\n",
    "                bias = False)\n",
    "        # self.bn = BatchNorm2d(out_chan)\n",
    "        self.bn = BatchNorm2d(out_chan)\n",
    "        self.need_relu = need_relu\n",
    "        if need_relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.need_relu:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6zZkeLnrBchL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class StepDAPPM(nn.Module):\n",
    "    def __init__(self, pool_ks, pool_stride, pool_padding, bn_inplanes, conv_out_planes, conv_ks=1):\n",
    "        super(StepDAPPM, self).__init__()\n",
    "        self.pool = None\n",
    "        if pool_ks > 1:\n",
    "            self.pool = nn.AvgPool2d(kernel_size=pool_ks, stride=pool_stride, padding=pool_padding)\n",
    "        elif pool_ks == 1:\n",
    "            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.bn = BatchNorm2d(bn_inplanes, momentum=bn_mom)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(bn_inplanes, conv_out_planes, kernel_size=conv_ks, padding=conv_ks//2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pool is not None:\n",
    "            x = self.pool(x)\n",
    "        x = self.conv(self.relu(self.bn(x)))\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OIqdvmi-yzg4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class DAPPM(nn.Module):\n",
    "    def __init__(self, inplanes, branch_planes, outplanes):\n",
    "        super(DAPPM, self).__init__()\n",
    "        self.scale1 = StepDAPPM( 5, 2, 2, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale2 = StepDAPPM( 9, 4, 4, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale3 = StepDAPPM(17, 8, 8, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale4 = StepDAPPM( 1, 0, 0, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale0 = StepDAPPM( 0, 0, 0, inplanes, branch_planes, conv_ks=1)\n",
    "\n",
    "        self.process1 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        self.process2 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        self.process3 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        self.process4 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        \n",
    "        self.compression = StepDAPPM(0, 0, 0, branch_planes * 5, outplanes, conv_ks=1)\n",
    "        self.shortcut = StepDAPPM(0, 0, 0, inplanes, outplanes, conv_ks=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = self.downsample(x)\n",
    "        width = x.shape[-1]\n",
    "        height = x.shape[-2]\n",
    "        x_list = []\n",
    "\n",
    "        x_list.append(self.scale0(x))\n",
    "        x_list.append(self.process1((F.interpolate(self.scale1(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[0])))\n",
    "        x_list.append((self.process2((F.interpolate(self.scale2(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[1]))))\n",
    "        x_list.append(self.process3((F.interpolate(self.scale3(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[2])))\n",
    "        x_list.append(self.process4((F.interpolate(self.scale4(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[3])))\n",
    "       \n",
    "        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n",
    "        return out    \n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vfsb_YNT38GW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class segmenthead(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n",
    "        super(segmenthead, self).__init__()\n",
    "        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n",
    "        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(self.relu(self.bn1(x)))\n",
    "        out = self.conv2(self.relu(self.bn2(x)))\n",
    "\n",
    "        if self.scale_factor is not None:\n",
    "            height = x.shape[-2] * self.scale_factor\n",
    "            width = x.shape[-1] * self.scale_factor\n",
    "            out = F.interpolate(out,\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vIwOtVnL4uPW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(out_chan,\n",
    "                out_chan//4,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                padding = 0,\n",
    "                bias = False)\n",
    "        self.conv2 = nn.Conv2d(out_chan//4,\n",
    "                out_chan,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                padding = 0,\n",
    "                bias = False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, fsp, fcp):\n",
    "        fcat = torch.cat([fsp, fcp], dim=1)\n",
    "        feat = self.convblk(fcat)\n",
    "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
    "        atten = self.conv1(atten)\n",
    "        atten = self.relu(atten)\n",
    "        atten = self.conv2(atten)\n",
    "        atten = self.sigmoid(atten)\n",
    "        feat_atten = torch.mul(feat, atten)\n",
    "        feat_out = feat_atten + feat\n",
    "        return feat_out\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jwa6yh5u8sWV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class HarDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False):\n",
    "        super(HarDBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.grmul = grmul\n",
    "        self.links = []\n",
    "        self.out_channels = 0\n",
    "        self.keepBase = keepBase\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(n_layers):\n",
    "            out_ch, in_ch, el_links = self.get_links(i+1)\n",
    "            self.links.append(el_links)\n",
    "            self.layers.append(ConvBNReLU(in_ch, out_ch, need_relu=(n_layers - 1 != i)))\n",
    "            if (i % 2 == 0) or (i == n_layers - 1):\n",
    "                self.out_channels += out_ch\n",
    "\n",
    "    def __out_ch(self, layer_id): \n",
    "        out_ch = self.growth_rate\n",
    "        for i in range(1, int(math.log2(layer_id))+1):\n",
    "            if layer_id % 2**i == 0:\n",
    "                out_ch = out_ch*self.grmul\n",
    "        return int(int(out_ch + 1) / 2) * 2\n",
    "\n",
    "    def get_links(self, layer_id):\n",
    "        in_ch = 0\n",
    "        links_ = []\n",
    "        for i in range(int(math.log2(layer_id))):\n",
    "            diff = 2**i\n",
    "            if (layer_id % diff == 0) and layer_id - diff > 0:\n",
    "                in_ch += self.__out_ch(layer_id - diff)\n",
    "                links_.append(layer_id - diff)\n",
    "        if math.log2(layer_id).is_integer():\n",
    "            in_ch += self.in_channels\n",
    "            links_.append(0)\n",
    "        return self.__out_ch(layer_id), in_ch, links_\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = [x]\n",
    "        for layer in range(len(self.layers)):\n",
    "            layer_input = []\n",
    "            for link in self.links[layer]:\n",
    "                layer_input.append(data[link])\n",
    "            in_ = layer_input[0] if len(layer_input) == 1 \\\n",
    "                else torch.cat(layer_input, dim=1)\n",
    "            data.append(self.layers[layer](in_))\n",
    "        t = len(data)\n",
    "        out = []\n",
    "        for i in range(t):\n",
    "            if (i % 2 == 1) or (self.keepBase and i == 0) or (i == t-1):\n",
    "                out.append(data[i])\n",
    "        return torch.cat(out, dim=1)\n",
    "    \n",
    "    def get_out_ch(self):\n",
    "        return self.out_channels\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.layers.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nlyhqLwN-Qn3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class ConvHarDBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, growth_rate=10, grmul=1.7, n_layers=4):\n",
    "        super(ConvHarDBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        if stride != 1:\n",
    "            self.layers.append(ConvBNReLU(inplanes, planes, ks=1, stride=stride,\\\n",
    "                                          padding=0, need_relu=True))\n",
    "        hardblock_inplanes = planes if stride != 1 else inplanes\n",
    "        hardblock = HarDBlock(hardblock_inplanes, growth_rate, grmul, n_layers)\n",
    "        self.layers.append(hardblock)\n",
    "\n",
    "        self.layers.append(ConvBNReLU(hardblock.get_out_ch(), planes, ks=1, \\\n",
    "                                      stride=1, padding=0, need_relu=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x        \n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.layers.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y39UPhD6LNis",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.blk1 = ConvBNReLU(inplanes, planes, ks=1, stride=1, padding=0)\n",
    "        self.blk2 = ConvBNReLU(planes, planes, ks=3, stride=stride, padding=1)\n",
    "        self.blk3 = ConvBNReLU(planes, planes * self.expansion, ks=1, stride=1, padding=0, need_relu=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.no_relu = no_relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('input', x.shape)\n",
    "        residual = x\n",
    "\n",
    "        out = self.blk1(x)\n",
    "        out = self.blk2(out)\n",
    "        out = self.blk3(out)\n",
    "        # print('out', out.shape)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            # print('down', self.downsample)\n",
    "            residual = self.downsample(x)\n",
    "        # print('res', residual.shape)\n",
    "        out += residual\n",
    "        if self.no_relu:\n",
    "            return out\n",
    "        else:\n",
    "            return self.relu(out)\n",
    "    \n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "88rzHfln70PP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class DualResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=19, planes=64, spp_planes=128, head_planes=128, augment=False):\n",
    "        super(DualResNet, self).__init__()\n",
    "\n",
    "        growth_rate = [  10,16,18,24,32]\n",
    "        highres_planes = planes * 2\n",
    "        self.augment = augment\n",
    "\n",
    "\n",
    "        self.conv0 = ConvBNReLU(     3, planes, ks=3, stride=2, padding=1)\n",
    "        self.conv1 = ConvBNReLU(planes, planes, ks=3, stride=2, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.layer1 = ConvHarDBlock(planes,     planes,     stride=1, growth_rate=growth_rate[0])\n",
    "        self.layer2 = ConvHarDBlock(planes,     planes * 2, stride=2, growth_rate=growth_rate[1])\n",
    "        self.layer3 = ConvHarDBlock(planes * 2, planes * 4, stride=2, growth_rate=growth_rate[2])\n",
    "        self.layer4 = ConvHarDBlock(planes * 4, planes * 8, stride=2, growth_rate=growth_rate[3])\n",
    "\n",
    "\n",
    "        self.compression3 = ConvBNReLU(planes * 4, highres_planes, ks=1, stride=1, padding=0, need_relu=False)\n",
    "        self.compression4 = ConvBNReLU(planes * 8, highres_planes, ks=1, stride=1, padding=0, need_relu=False)\n",
    "\n",
    "        self.down3 = ConvBNReLU(highres_planes, planes * 4, ks=3, stride=2, padding=1, need_relu=False)\n",
    "        self.down4_0 = ConvBNReLU(highres_planes, planes * 4, ks=3, stride=2, padding=1, need_relu=True)\n",
    "        self.down4_1 = ConvBNReLU(planes * 4, planes * 8, ks=3, stride=2, padding=1, need_relu=False)\n",
    "\n",
    "        self.layer3_ = ConvHarDBlock(planes * 2, highres_planes, stride=1, growth_rate=growth_rate[2])\n",
    "        \n",
    "        self.layer4_ = ConvHarDBlock(highres_planes, highres_planes, stride=1, growth_rate=growth_rate[2])\n",
    "\n",
    "        self.layer5_ = self._make_bottleneck(highres_planes, highres_planes)\n",
    "\n",
    "        self.layer5 =  self._make_bottleneck(planes * 8, planes * 8, stride=2)\n",
    "\n",
    "        self.spp = DAPPM(planes * 8 * Bottleneck.expansion, spp_planes, planes * 4)\n",
    "\n",
    "        self.ffm = FeatureFusionModule(planes * 4 + highres_planes * 2, planes * 4)\n",
    "\n",
    "        self.seghead_out1 = segmenthead(planes * 4, head_planes, num_classes)\n",
    "        self.seghead_out2 = segmenthead(highres_planes, head_planes, 1)\n",
    "        self.seghead_out3 = segmenthead(highres_planes, head_planes, num_classes)            \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_bottleneck(self, inplanes, planes, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            downsample = ConvBNReLU(inplanes, planes * Bottleneck.expansion, ks=1, stride=stride, padding=0, need_relu=False)\n",
    "            # print(stride, inplanes, planes * Bottleneck.expansion, downsample)\n",
    "        return Bottleneck(inplanes, planes, stride, downsample)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        width, height = x.shape[-1], x.shape[-2]\n",
    "        width_output, height_output = x.shape[-1] // 8, x.shape[-2] // 8\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        layers.append(x)\n",
    "\n",
    "        x = self.layer2(self.relu(x))\n",
    "        layers.append(x)\n",
    "  \n",
    "        x = self.layer3(self.relu(x))\n",
    "        layers.append(x)\n",
    "        x_ = self.layer3_(self.relu(layers[1]))\n",
    "\n",
    "        x = x + self.down3(self.relu(x_))\n",
    "        step_res = self.compression3(self.relu(layers[2]))\n",
    "        x_ = x_ + F.interpolate(\n",
    "                        step_res,\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "        out_2 = x_\n",
    "\n",
    "        x = self.layer4(self.relu(x))\n",
    "        layers.append(x)\n",
    "        \n",
    "        x_ = self.layer4_(self.relu(x_))\n",
    "\n",
    "        x = x + self.down4_1(self.down4_0(self.relu(x_)))\n",
    "        x_ = x_ + F.interpolate(\n",
    "                        self.compression4(self.relu(layers[3])),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out_3 = x_\n",
    "\n",
    "        x_ = self.layer5_(self.relu(x_))\n",
    "        x = F.interpolate(\n",
    "                        self.spp(self.layer5(self.relu(x))),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out_1 = self.ffm(x, x_)\n",
    "        \n",
    "        out_1 = self.seghead_out1(out_1)\n",
    "        out_2 = self.seghead_out2(out_2)\n",
    "        out_3 = self.seghead_out3(out_3)\n",
    "\n",
    "        out_1 = F.interpolate(out_1, (height, width), mode='bilinear', align_corners=True)\n",
    "        out_2 = F.interpolate(out_2, (height, width), mode='bilinear', align_corners=True)\n",
    "        out_3 = F.interpolate(out_3, (height, width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return out_1, out_2, out_3\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9S-uctZ36iGX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# net = DualResNet(num_classes=19, planes=64, spp_planes=128, head_planes=128, augment=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lpOJFmAEP-CX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# net.get_params()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WZkTtHAkFN-S",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# torch.save(net.state_dict(), 'model_maxmIOU103.pth')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfAtjams6ikd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mw8T7rfn57RL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def dice_loss_func(input, target):\n",
    "    smooth = 1.\n",
    "    n = input.size(0)\n",
    "    iflat = input.view(n, -1)\n",
    "    tflat = target.view(n, -1)\n",
    "    intersection = (iflat * tflat).sum(1)\n",
    "    loss = 1 - ((2. * intersection + smooth) /\n",
    "                (iflat.sum(1) + tflat.sum(1) + smooth))\n",
    "    return loss.mean()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x1X1GCpDNvej",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class OhemCELoss(nn.Module):\n",
    "    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n",
    "        super(OhemCELoss, self).__init__()\n",
    "        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).to(device)\n",
    "        self.n_min = n_min\n",
    "        self.ignore_lb = ignore_lb\n",
    "        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        N, C, H, W = logits.size()\n",
    "        loss = self.criteria(logits, labels).view(-1)\n",
    "        loss, _ = torch.sort(loss, descending=True)\n",
    "        if loss[self.n_min] > self.thresh:\n",
    "            loss = loss[loss>self.thresh]\n",
    "        else:\n",
    "            loss = loss[:self.n_min]\n",
    "        return torch.mean(loss)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hxwvvyh6PdMg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class DetailAggregateLoss(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DetailAggregateLoss, self).__init__()\n",
    "        \n",
    "        self.laplacian_kernel = torch.tensor(\n",
    "            [-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "            dtype=torch.float32).reshape(1, 1, 3, 3).requires_grad_(False).type(float_tensor_type)        \n",
    "\n",
    "        self.fuse_kernel = torch.nn.Parameter(torch.tensor([[6./10], [3./10], [1./10]],\n",
    "            dtype=torch.float32).reshape(1, 3, 1, 1).type(float_tensor_type))\n",
    "\n",
    "    def forward(self, boundary_logits, gtmasks):\n",
    "\n",
    "        boundary_targets = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, padding=1)\n",
    "        boundary_targets = boundary_targets.clamp(min=0)\n",
    "        boundary_targets[boundary_targets > 0.1] = 1\n",
    "        boundary_targets[boundary_targets <= 0.1] = 0\n",
    "\n",
    "        boundary_targets_x2 = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, stride=2, padding=1)\n",
    "        boundary_targets_x2 = boundary_targets_x2.clamp(min=0)\n",
    "        \n",
    "        boundary_targets_x4 = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, stride=4, padding=1)\n",
    "        boundary_targets_x4 = boundary_targets_x4.clamp(min=0)\n",
    "\n",
    "        boundary_targets_x4_up = F.interpolate(boundary_targets_x4, boundary_targets.shape[2:], mode='nearest')\n",
    "        boundary_targets_x2_up = F.interpolate(boundary_targets_x2, boundary_targets.shape[2:], mode='nearest')\n",
    "        \n",
    "        boundary_targets_x2_up[boundary_targets_x2_up > 0.1] = 1\n",
    "        boundary_targets_x2_up[boundary_targets_x2_up <= 0.1] = 0\n",
    "        \n",
    "        \n",
    "        boundary_targets_x4_up[boundary_targets_x4_up > 0.1] = 1\n",
    "        boundary_targets_x4_up[boundary_targets_x4_up <= 0.1] = 0\n",
    "       \n",
    "        boudary_targets_pyramids = torch.stack((boundary_targets, boundary_targets_x2_up, boundary_targets_x4_up), dim=1)\n",
    "        \n",
    "        boudary_targets_pyramids = boudary_targets_pyramids.squeeze(2)\n",
    "        boudary_targets_pyramid = F.conv2d(boudary_targets_pyramids, self.fuse_kernel)\n",
    "\n",
    "        boudary_targets_pyramid[boudary_targets_pyramid > 0.1] = 1\n",
    "        boudary_targets_pyramid[boudary_targets_pyramid <= 0.1] = 0\n",
    "        \n",
    "        \n",
    "        if boundary_logits.shape[-1] != boundary_targets.shape[-1]:\n",
    "            boundary_logits = F.interpolate(\n",
    "                boundary_logits, boundary_targets.shape[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        bce_loss = F.binary_cross_entropy_with_logits(boundary_logits, boudary_targets_pyramid)\n",
    "        dice_loss = dice_loss_func(torch.sigmoid(boundary_logits), boudary_targets_pyramid)\n",
    "        return bce_loss,  dice_loss\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "                nowd_params += list(module.parameters())\n",
    "        return nowd_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpE2HElyOJB3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, model, loss, lr0, momentum, wd, warmup_steps, \n",
    "                 warmup_start_lr, max_iter, power, *args, **kwargs):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.lr0 = lr0\n",
    "        self.lr = self.lr0\n",
    "        self.max_iter = float(max_iter)\n",
    "        self.power = power\n",
    "        self.it = 0\n",
    "        wd_params, nowd_params = model.get_params()\n",
    "        loss_nowd_params = loss.get_params()\n",
    "        param_list = [\n",
    "                {'params': wd_params},\n",
    "                {'params': nowd_params, 'weight_decay': 0},\n",
    "                {'params': loss_nowd_params}]\n",
    "        self.optim = torch.optim.SGD(\n",
    "                param_list,\n",
    "                # model.parameters(),\n",
    "                lr = lr0,\n",
    "                momentum = momentum,\n",
    "                weight_decay = wd)\n",
    "        self.warmup_factor = (self.lr0/self.warmup_start_lr)**(1./self.warmup_steps)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.it <= self.warmup_steps:\n",
    "            lr = self.warmup_start_lr*(self.warmup_factor**self.it)\n",
    "        else:\n",
    "            factor = (1-(self.it-self.warmup_steps)/(self.max_iter-self.warmup_steps))**self.power\n",
    "            lr = self.lr0 * factor\n",
    "        return lr\n",
    "\n",
    "    def step(self):\n",
    "        self.lr = self.get_lr()\n",
    "        for pg in self.optim.param_groups:\n",
    "            if pg.get('lr_mul', False):\n",
    "                pg['lr'] = self.lr * 10\n",
    "            else:\n",
    "                pg['lr'] = self.lr\n",
    "        if self.optim.defaults.get('lr_mul', False):\n",
    "            self.optim.defaults['lr'] = self.lr * 10\n",
    "        else:\n",
    "            self.optim.defaults['lr'] = self.lr\n",
    "        self.it += 1\n",
    "        self.optim.step()\n",
    "        if self.it == self.warmup_steps+2:\n",
    "            logger.info('==> warmup done, start to implement poly lr strategy')\n",
    "    \n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'warmup_start_lr': self.warmup_start_lr,\n",
    "            'lr0': self.lr0,\n",
    "            'lr': self.lr,\n",
    "            'max_iter': self.max_iter,\n",
    "            'power': self.power, \n",
    "            'it': self.it,\n",
    "            'optim_state': self.optim.state_dict(),\n",
    "            'warmup_factor': self.warmup_factor\n",
    "        }\n",
    "\n",
    "    def load_state(self, state):\n",
    "        self.warmup_steps = state.get('warmup_steps')\n",
    "        self.warmup_start_lr = state.get('warmup_start_lr')\n",
    "        self.lr0 = state.get('lr0')\n",
    "        self.lr = state.get('lr')\n",
    "        self.max_iter = state.get('max_iter')\n",
    "        self.power = state.get('power')\n",
    "        self.it = state.get('it')\n",
    "        self.optim.load_state_dict(state.get('optim_state'))\n",
    "        self.warmup_factor = state.get('warmup_factor')\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optim.zero_grad()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L-4mDzEJUx7x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "score_thres = 0.7\n",
    "n_img_per_gpu = 8\n",
    "cropsize = (512, 1024)\n",
    "n_min = n_img_per_gpu*cropsize[0]*cropsize[1]//32\n",
    "ignore_idx=255"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-tF2L-AXDcn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUhwVh6gXReD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637933952098,
     "user_tz": -180,
     "elapsed": 18,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "c6b6e475-4e65-49d0-e680-ec73bcf96ce9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!wget https://raw.githubusercontent.com/MichaelFan01/STDC-Seg/master/cityscapes_info.json"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-11-26 13:39:11--  https://raw.githubusercontent.com/MichaelFan01/STDC-Seg/master/cityscapes_info.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7412 (7.2K) [text/plain]\n",
      "Saving to: ‘cityscapes_info.json’\n",
      "\n",
      "\rcityscapes_info.jso   0%[                    ]       0  --.-KB/s               \rcityscapes_info.jso 100%[===================>]   7.24K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-11-26 13:39:11 (55.9 MB/s) - ‘cityscapes_info.json’ saved [7412/7412]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_hLh2hCJPJsj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from PIL import Image\n",
    "import PIL.ImageEnhance as ImageEnhance\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size, *args, **kwargs):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        im = im_lb['im']\n",
    "        lb = im_lb['lb']\n",
    "        assert im.size == lb.size\n",
    "        W, H = self.size\n",
    "        w, h = im.size\n",
    "\n",
    "        if (W, H) == (w, h): return dict(im=im, lb=lb)\n",
    "        if w < W or h < H:\n",
    "            scale = float(W) / w if w < h else float(H) / h\n",
    "            w, h = int(scale * w + 1), int(scale * h + 1)\n",
    "            im = im.resize((w, h), Image.BILINEAR)\n",
    "            lb = lb.resize((w, h), Image.NEAREST)\n",
    "        sw, sh = random.random() * (w - W), random.random() * (h - H)\n",
    "        crop = int(sw), int(sh), int(sw) + W, int(sh) + H\n",
    "        return dict(\n",
    "                im = im.crop(crop),\n",
    "                lb = lb.crop(crop)\n",
    "                    )\n",
    "\n",
    "\n",
    "class HorizontalFlip(object):\n",
    "    def __init__(self, p=0.5, *args, **kwargs):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        if random.random() > self.p:\n",
    "            return im_lb\n",
    "        else:\n",
    "            im = im_lb['im']\n",
    "            lb = im_lb['lb']\n",
    "            return dict(im = im.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "                        lb = lb.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "                    )\n",
    "\n",
    "\n",
    "class RandomScale(object):\n",
    "    def __init__(self, scales=(1, ), *args, **kwargs):\n",
    "        self.scales = scales\n",
    "        # print('scales: ', scales)\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        im = im_lb['im']\n",
    "        lb = im_lb['lb']\n",
    "        W, H = im.size\n",
    "        scale = random.choice(self.scales)\n",
    "        # scale = np.random.uniform(min(self.scales), max(self.scales))\n",
    "        w, h = int(W * scale), int(H * scale)\n",
    "        return dict(im = im.resize((w, h), Image.BILINEAR),\n",
    "                    lb = lb.resize((w, h), Image.NEAREST),\n",
    "                )\n",
    "\n",
    "\n",
    "class ColorJitter(object):\n",
    "    def __init__(self, brightness=None, contrast=None, saturation=None, *args, **kwargs):\n",
    "        if not brightness is None and brightness>0:\n",
    "            self.brightness = [max(1-brightness, 0), 1+brightness]\n",
    "        if not contrast is None and contrast>0:\n",
    "            self.contrast = [max(1-contrast, 0), 1+contrast]\n",
    "        if not saturation is None and saturation>0:\n",
    "            self.saturation = [max(1-saturation, 0), 1+saturation]\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        im = im_lb['im']\n",
    "        lb = im_lb['lb']\n",
    "        r_brightness = random.uniform(self.brightness[0], self.brightness[1])\n",
    "        r_contrast = random.uniform(self.contrast[0], self.contrast[1])\n",
    "        r_saturation = random.uniform(self.saturation[0], self.saturation[1])\n",
    "        im = ImageEnhance.Brightness(im).enhance(r_brightness)\n",
    "        im = ImageEnhance.Contrast(im).enhance(r_contrast)\n",
    "        im = ImageEnhance.Color(im).enhance(r_saturation)\n",
    "        return dict(im = im,\n",
    "                    lb = lb,\n",
    "                )\n",
    "\n",
    "\n",
    "class MultiScale(object):\n",
    "    def __init__(self, scales):\n",
    "        self.scales = scales\n",
    "\n",
    "    def __call__(self, img):\n",
    "        W, H = img.size\n",
    "        sizes = [(int(W*ratio), int(H*ratio)) for ratio in self.scales]\n",
    "        imgs = []\n",
    "        [imgs.append(img.resize(size, Image.BILINEAR)) for size in sizes]\n",
    "        return imgs\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, do_list):\n",
    "        self.do_list = do_list\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        for comp in self.do_list:\n",
    "            im_lb = comp(im_lb)\n",
    "        return im_lb"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c88kJt45PP93",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os.path as osp\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class CityScapes(Dataset):\n",
    "    def __init__(self, rootpth, cropsize=(640, 480), mode='train', \n",
    "    randomscale=(0.125, 0.25, 0.375, 0.5, 0.675, 0.75, 0.875, 1.0, 1.25, 1.5), *args, **kwargs):\n",
    "        super(CityScapes, self).__init__(*args, **kwargs)\n",
    "        assert mode in ('train', 'val', 'test', 'trainval')\n",
    "        self.mode = mode\n",
    "        print('self.mode', self.mode)\n",
    "        self.ignore_lb = 255\n",
    "\n",
    "        with open('./cityscapes_info.json', 'r') as fr:\n",
    "            labels_info = json.load(fr)\n",
    "        self.lb_map = {el['id']: el['trainId'] for el in labels_info}\n",
    "        \n",
    "\n",
    "        ## parse img directory\n",
    "        self.imgs = {}\n",
    "        imgnames = []\n",
    "        impth = osp.join(rootpth, 'leftImg8bit', mode)\n",
    "        folders = os.listdir(impth)\n",
    "        for fd in folders:\n",
    "            fdpth = osp.join(impth, fd)\n",
    "            im_names = os.listdir(fdpth)\n",
    "            names = [el.replace('_leftImg8bit.png', '') for el in im_names]\n",
    "            impths = [osp.join(fdpth, el) for el in im_names]\n",
    "            imgnames.extend(names)\n",
    "            self.imgs.update(dict(zip(names, impths)))\n",
    "\n",
    "        ## parse gt directory\n",
    "        self.labels = {}\n",
    "        gtnames = []\n",
    "        gtpth = osp.join(rootpth, 'gtFine', mode)\n",
    "        folders = os.listdir(gtpth)\n",
    "        for fd in folders:\n",
    "            fdpth = osp.join(gtpth, fd)\n",
    "            lbnames = os.listdir(fdpth)\n",
    "            lbnames = [el for el in lbnames if 'labelIds' in el]\n",
    "            names = [el.replace('_gtFine_labelIds.png', '') for el in lbnames]\n",
    "            lbpths = [osp.join(fdpth, el) for el in lbnames]\n",
    "            gtnames.extend(names)\n",
    "            self.labels.update(dict(zip(names, lbpths)))\n",
    "\n",
    "        self.imnames = imgnames\n",
    "        self.len = len(self.imnames)\n",
    "        print('self.len', self.mode, self.len)\n",
    "        assert set(imgnames) == set(gtnames)\n",
    "        assert set(self.imnames) == set(self.imgs.keys())\n",
    "        assert set(self.imnames) == set(self.labels.keys())\n",
    "\n",
    "        ## pre-processing\n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        self.trans_train = Compose([\n",
    "            ColorJitter(\n",
    "                brightness = 0.5,\n",
    "                contrast = 0.5,\n",
    "                saturation = 0.5),\n",
    "            HorizontalFlip(),\n",
    "            RandomScale(randomscale),\n",
    "            RandomCrop(cropsize)\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn  = self.imnames[idx]\n",
    "        impth = self.imgs[fn]\n",
    "        lbpth = self.labels[fn]\n",
    "        img = Image.open(impth).convert('RGB')\n",
    "        label = Image.open(lbpth)\n",
    "        if self.mode == 'train' or self.mode == 'trainval':\n",
    "            im_lb = dict(im = img, lb = label)\n",
    "            im_lb = self.trans_train(im_lb)\n",
    "            img, label = im_lb['im'], im_lb['lb']\n",
    "        img = self.to_tensor(img)\n",
    "        label = np.array(label).astype(np.int64)[np.newaxis, :]\n",
    "        label = self.convert_labels(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    def convert_labels(self, label):\n",
    "        for k, v in self.lb_map.items():\n",
    "            label[label == k] = v\n",
    "        return label"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdTbxumgrAqo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637934074861,
     "user_tz": -180,
     "elapsed": 122303,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "7b4f3792-11e9-4bf3-cba0-b5765ba5ad25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87f6J6wdXxpA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637934102568,
     "user_tz": -180,
     "elapsed": 27722,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "211edc40-f1f5-4c3b-97e8-4ca8f9c46d3c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dspth = '/content/drive/MyDrive/RnD/datasets/'\n",
    "cfg_data = {\n",
    "    'dataset': 'cityscapes',\n",
    "    'train_split': 'train',\n",
    "    'val_split': 'val',\n",
    "    'img_rows': cropsize[0],\n",
    "    'img_cols': cropsize[1],\n",
    "    'path': dspth\n",
    "}\n",
    "randomscale = (0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0, 1.125, 1.25, 1.375, 1.5)\n",
    "ds = CityScapes(cfg_data['path'], cropsize=cropsize, mode='train', randomscale=randomscale)\n",
    "# sampler = torch.utils.data.distributed.DistributedSampler(ds)\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = batch_size,\n",
    "                shuffle = False,\n",
    "                # sampler = sampler,\n",
    "                num_workers = n_workers,\n",
    "                pin_memory = False,\n",
    "                drop_last = True)\n",
    "# exit(0)\n",
    "dsval = CityScapes(cfg_data['path'], mode='val', randomscale=randomscale)\n",
    "# sampler_val = torch.utils.data.distributed.DistributedSampler(dsval)\n",
    "dlval = DataLoader(dsval,\n",
    "                batch_size = 2,\n",
    "                shuffle = False,\n",
    "                # sampler = sampler_val,\n",
    "                num_workers = n_workers,\n",
    "                drop_last = False)\n",
    "\n",
    "## model\n",
    "ignore_idx = 255"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "self.mode train\n",
      "self.len train 2975\n",
      "self.mode val\n",
      "self.len val 500\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csU-R0brNqti",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "# Preparing for train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q_MVKWyjNwFH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "best_iou = -100.0\n",
    "loss_all = 0\n",
    "loss_n = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2GqMaMb8PBiV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def get_logger(logdir):\n",
    "    logger = logging.getLogger(\"DDRNet_HBD_B2N_DGT\")\n",
    "    ts = str(datetime.now()).split(\".\")[0].replace(\" \", \"_\")\n",
    "    ts = ts.replace(\":\", \"_\").replace(\"-\", \"_\")\n",
    "    file_path = os.path.join(logdir, \"run_{}.log\".format(ts))\n",
    "    hdlr = logging.FileHandler(file_path)\n",
    "    formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCjt4GHxQV-S",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637934106213,
     "user_tz": -180,
     "elapsed": 3680,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "839f4b9d-4938-4c4a-b39d-8658770cacac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!git clone https://github.com/PingoLH/FCHarDNet.git\n",
    "!cp -r FCHarDNet/ptsemseg ./\n",
    "!rm -rf FCHarDNet"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'FCHarDNet'...\n",
      "remote: Enumerating objects: 130, done.\u001B[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001B[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001B[K\n",
      "remote: Total 130 (delta 2), reused 7 (delta 1), pack-reused 117\u001B[K\n",
      "Receiving objects: 100% (130/130), 9.10 MiB | 3.45 MiB/s, done.\n",
      "Resolving deltas: 100% (50/50), done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2-4aOn4dQeLT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from ptsemseg.metrics import runningScore, averageMeter"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nJTlBpSmStgU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "base_path = \"/content/drive/MyDrive/RnD/runs/DDRNet\"\n",
    "model_modification = 'HDB_DGT'\n",
    "model_modification_path = os.path.join(base_path, model_modification)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAoSxPyeaFio",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637934107092,
     "user_tz": -180,
     "elapsed": 883,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "5e2a1bb2-75a8-4c92-ba98-785760f2a5a0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "logdir = os.path.join(model_modification_path, str(datetime.fromtimestamp(int(time.time()))))\n",
    "writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "print(\"RUNDIR: {}\".format(logdir))\n",
    "\n",
    "logger = get_logger(logdir)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RUNDIR: /content/drive/MyDrive/RnD/runs/DDRNet/HDB_DGT/2021-11-26 13:41:45\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nDPzLJpLP7au",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G95g0arBR1AY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "running_metrics_val = runningScore(n_classes)\n",
    "model = DualResNet(num_classes=19, planes=64, \n",
    "                         spp_planes=128, head_planes=128, augment=False)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print( 'Parameters:',total_params )\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model.apply(weights_init)\n",
    "pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P_5wFQ0UVnHt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# optimizer init data\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "lr_start = 1e-2\n",
    "power = 0.9\n",
    "warmup_steps = 1000\n",
    "warmup_start_lr = 1e-5\n",
    "epoch_iteration = len(ds) // batch_size\n",
    "max_epoch = 484\n",
    "# max_epoch = 1\n",
    "max_iter = max_epoch * epoch_iteration"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rg_faMQHOnMZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "start_epoch = 0\n",
    "it = 0\n",
    "local_max_epoch = start_epoch + 1 if start_epoch + 1 < max_epoch else max_epoch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nfsksqZNg3TM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "criteria_ffm = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1\n",
    "criteria_layer4 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx) # out2\n",
    "boundary_loss_func = DetailAggregateLoss()                                          # out3\n",
    "criteria_val = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1 \n",
    "val_loss_meter = averageMeter()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SFawk_jCVuJg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "optim = Optimizer(\n",
    "            model = model.module,\n",
    "            loss = boundary_loss_func,\n",
    "            lr0 = lr_start,\n",
    "            momentum = momentum,\n",
    "            wd = weight_decay,\n",
    "            warmup_steps = warmup_steps,\n",
    "            warmup_start_lr = warmup_start_lr,\n",
    "            max_iter = max_iter,\n",
    "            power = power)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gcCTdTNiWhH3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "loss_avg = []\n",
    "loss_boundery_bce = []\n",
    "loss_boundery_dice = []"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFTxfMXSqw3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Restore state"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kuw_ClrzSqJM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "runs = sorted(os.listdir(model_modification_path), reverse=True)\n",
    "best_path = None\n",
    "last_path = None\n",
    "for run in runs:\n",
    "    tmp_base = os.path.join(model_modification_path, run)\n",
    "    model_name = \"{}_{}\".format(model_arch, cfg_data['dataset'])\n",
    "    checkpoint = os.path.join(tmp_base, model_name+'_checkpoint.pkl')\n",
    "    best = os.path.join(tmp_base, model_name+'_best_model.pkl')\n",
    "    if not last_path and os.path.exists(checkpoint):\n",
    "        last_path = checkpoint\n",
    "    if not best_path and os.path.exists(best):\n",
    "        best_path = best\n",
    "    if last_path and best_path:\n",
    "        break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7MjZMI5bkDt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637934116250,
     "user_tz": -180,
     "elapsed": 16,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "04ca0026-3390-4631-da7c-b892cea4fc76",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "best_path, last_path"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/RnD/runs/DDRNet/HDB_DGT/2021-11-24 09:22:22/ddrnet_slim_23_cityscapes_best_model.pkl',\n",
       " '/content/drive/MyDrive/RnD/runs/DDRNet/HDB_DGT/2021-11-24 09:22:22/ddrnet_slim_23_cityscapes_checkpoint.pkl')"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1jc6LeN9TOCq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if best_path and last_path:\n",
    "    loaded = torch.load(last_path)\n",
    "    best_iou_arrc = torch.load(best_path)\n",
    "    model_state = loaded.get('model_state')\n",
    "\n",
    "    optimizer_state = loaded.get('optimizer_state')\n",
    "    start_epoch = loaded.get('epoch') + 1\n",
    "\n",
    "    best_iou = best_iou_arrc.get('best_iou')\n",
    "    local_max_epoch = start_epoch + 2\n",
    "    flag = True\n",
    "    loss_all = 0\n",
    "    loss_n = 0\n",
    "\n",
    "    model.load_state_dict(model_state)\n",
    "    optim.load_state(optimizer_state)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XftdK_iMCpQB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637934120426,
     "user_tz": -180,
     "elapsed": 372,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "a0de2183-cd45-4286-cffb-fd4b00924337",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!tail -n25 '/content/drive/MyDrive/RnD/runs/DDRNet/HDB_DGT/2021-11-23 15:38:33/run_2021_11_23_15_38_33.log'\n",
    "# file_"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-11-23 16:27:42,896 INFO epoch: 22/484it: 130/371, lr: 0.009632, loss: 3.0833, boundery_bce_loss: 0.1284, boundery_dice_loss: 0.6320, eta: 45 days, 23:37:51, time: 43.7429\n",
      "2021-11-23 16:28:26,006 INFO epoch: 22/484it: 140/371, lr: 0.009631, loss: 3.0869, boundery_bce_loss: 0.1235, boundery_dice_loss: 0.6475, eta: 43 days, 7:38:01, time: 43.1102\n",
      "2021-11-23 16:29:08,198 INFO epoch: 22/484it: 150/371, lr: 0.009631, loss: 3.0488, boundery_bce_loss: 0.1065, boundery_dice_loss: 0.6178, eta: 40 days, 23:55:04, time: 42.1920\n",
      "2021-11-23 16:29:49,525 INFO epoch: 22/484it: 160/371, lr: 0.009630, loss: 2.7134, boundery_bce_loss: 0.1113, boundery_dice_loss: 0.6117, eta: 38 days, 22:56:16, time: 41.3272\n",
      "2021-11-23 16:30:31,823 INFO epoch: 22/484it: 170/371, lr: 0.009630, loss: 3.0231, boundery_bce_loss: 0.1112, boundery_dice_loss: 0.6096, eta: 37 days, 4:02:21, time: 42.2975\n",
      "2021-11-23 16:31:13,475 INFO epoch: 22/484it: 180/371, lr: 0.009629, loss: 3.4781, boundery_bce_loss: 0.1214, boundery_dice_loss: 0.6286, eta: 35 days, 13:45:09, time: 41.6520\n",
      "2021-11-23 16:31:55,053 INFO epoch: 22/484it: 190/371, lr: 0.009629, loss: 2.9283, boundery_bce_loss: 0.1089, boundery_dice_loss: 0.6086, eta: 34 days, 3:29:48, time: 41.5788\n",
      "2021-11-23 16:32:37,103 INFO epoch: 22/484it: 200/371, lr: 0.009628, loss: 3.1484, boundery_bce_loss: 0.1045, boundery_dice_loss: 0.6155, eta: 32 days, 20:48:02, time: 42.0496\n",
      "2021-11-23 16:33:17,756 INFO epoch: 22/484it: 210/371, lr: 0.009628, loss: 2.9812, boundery_bce_loss: 0.1083, boundery_dice_loss: 0.6137, eta: 31 days, 16:42:28, time: 40.6531\n",
      "2021-11-23 16:34:00,187 INFO epoch: 22/484it: 220/371, lr: 0.009627, loss: 2.9482, boundery_bce_loss: 0.1192, boundery_dice_loss: 0.6313, eta: 30 days, 15:35:02, time: 42.4304\n",
      "2021-11-23 16:34:42,628 INFO epoch: 22/484it: 230/371, lr: 0.009627, loss: 3.2068, boundery_bce_loss: 0.1113, boundery_dice_loss: 0.6403, eta: 29 days, 16:39:20, time: 42.4410\n",
      "2021-11-23 16:35:26,432 INFO epoch: 22/484it: 240/371, lr: 0.009626, loss: 3.1039, boundery_bce_loss: 0.1030, boundery_dice_loss: 0.6182, eta: 28 days, 19:55:44, time: 43.8048\n",
      "2021-11-23 16:36:09,029 INFO epoch: 22/484it: 250/371, lr: 0.009626, loss: 3.0850, boundery_bce_loss: 0.1047, boundery_dice_loss: 0.6130, eta: 28 days, 0:37:29, time: 42.5968\n",
      "2021-11-23 16:36:52,194 INFO epoch: 22/484it: 260/371, lr: 0.009625, loss: 2.9247, boundery_bce_loss: 0.1005, boundery_dice_loss: 0.5913, eta: 27 days, 6:55:10, time: 43.1652\n",
      "2021-11-23 16:37:35,514 INFO epoch: 22/484it: 270/371, lr: 0.009625, loss: 2.9941, boundery_bce_loss: 0.1003, boundery_dice_loss: 0.5752, eta: 26 days, 14:33:29, time: 43.3194\n",
      "2021-11-23 16:38:16,735 INFO epoch: 22/484it: 280/371, lr: 0.009624, loss: 3.2426, boundery_bce_loss: 0.1161, boundery_dice_loss: 0.6230, eta: 25 days, 22:59:40, time: 41.2216\n",
      "2021-11-23 16:39:00,093 INFO epoch: 22/484it: 290/371, lr: 0.009624, loss: 3.3021, boundery_bce_loss: 0.1192, boundery_dice_loss: 0.6425, eta: 25 days, 8:52:30, time: 43.3575\n",
      "2021-11-23 16:39:42,705 INFO epoch: 22/484it: 300/371, lr: 0.009623, loss: 3.1822, boundery_bce_loss: 0.1219, boundery_dice_loss: 0.6422, eta: 24 days, 19:34:31, time: 42.6123\n",
      "2021-11-23 16:40:25,524 INFO epoch: 22/484it: 310/371, lr: 0.009623, loss: 3.3022, boundery_bce_loss: 0.1170, boundery_dice_loss: 0.6441, eta: 24 days, 7:10:08, time: 42.8191\n",
      "2021-11-23 16:41:08,480 INFO epoch: 22/484it: 320/371, lr: 0.009622, loss: 3.1973, boundery_bce_loss: 0.1183, boundery_dice_loss: 0.6496, eta: 23 days, 19:33:38, time: 42.9554\n",
      "2021-11-23 16:41:51,602 INFO epoch: 22/484it: 330/371, lr: 0.009622, loss: 3.2567, boundery_bce_loss: 0.1204, boundery_dice_loss: 0.6493, eta: 23 days, 8:40:58, time: 43.1224\n",
      "2021-11-23 16:42:35,689 INFO epoch: 22/484it: 340/371, lr: 0.009621, loss: 3.0866, boundery_bce_loss: 0.1109, boundery_dice_loss: 0.6184, eta: 22 days, 22:35:15, time: 44.0869\n",
      "2021-11-23 16:43:19,347 INFO epoch: 22/484it: 350/371, lr: 0.009621, loss: 3.0554, boundery_bce_loss: 0.1135, boundery_dice_loss: 0.6098, eta: 22 days, 13:00:32, time: 43.6576\n",
      "2021-11-23 16:44:03,562 INFO epoch: 22/484it: 360/371, lr: 0.009620, loss: 2.9216, boundery_bce_loss: 0.1111, boundery_dice_loss: 0.6131, eta: 22 days, 4:02:27, time: 44.2155\n",
      "2021-11-23 16:44:46,858 INFO epoch: 22/484it: 370/371, lr: 0.009620, loss: 3.1484, boundery_bce_loss: 0.1061, boundery_dice_loss: 0.5908, eta: 21 days, 19:26:02, time: 43.2962\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcaafUp4eal5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637934120427,
     "user_tz": -180,
     "elapsed": 14,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "a924d7c3-58bc-4db3-9bd6-503d26d23995",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "file_ = list(filter(lambda x: x.endswith('.log'), os.listdir(logdir)))[0]\n",
    "file_ = os.path.join(logdir, file_)\n",
    "if not flag and osp.isfile(file_):\n",
    "    with open(file_, \"r\") as f:\n",
    "        str_ = f.readlines()[-24]\n",
    "        st__, end__ = str_.find('Epoch') + 6, str_.find(' Iter')\n",
    "        if st__ > -1 and end__ > -1 and local_max_epoch - 1 == int(str_[st__:end__]):\n",
    "            start_epoch = local_max_epoch\n",
    "            local_max_epoch += 2\n",
    "\n",
    "start_epoch, local_max_epoch, best_iou"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(41, 43, 0.4278781883209911)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZjxXbpI_MDX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HuAUlVp7XTSc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637938230494,
     "user_tz": -180,
     "elapsed": 417579,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GivhSqWinIWPCnla4i9mS0iTldWMUfolXP7x1EL=s64",
      "userId": "05960264113093771023"
     }
    },
    "outputId": "54bf3de2-66b2-4da1-db91-6f38c649a7a6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "st = glob_st = time.time()\n",
    "flag = False\n",
    "for epoch_id in range(start_epoch, local_max_epoch):\n",
    "    for images, labels in dl:\n",
    "        it += 1\n",
    "        start_ts = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.squeeze(labels, 1)\n",
    " \n",
    "        optim.zero_grad()\n",
    "\n",
    "        out_ffm, out_l3, out_l4 = model(images)\n",
    " \n",
    "        loss_ffm = criteria_ffm(out_ffm, labels)\n",
    "        loss_l4  = criteria_layer4(out_l4, labels)\n",
    "        boundery_bce, boundery_dice = boundary_loss_func(out_l3, labels)\n",
    "\n",
    "        boundery_bce_loss = boundery_bce\n",
    "        boundery_dice_loss = boundery_dice\n",
    "\n",
    "        loss = loss_ffm + loss_l4 + boundery_bce_loss + boundery_dice_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_avg.append(loss.item())\n",
    "\n",
    "        loss_boundery_bce.append(boundery_bce_loss.item())\n",
    "        loss_boundery_dice.append(boundery_dice_loss.item())\n",
    "\n",
    "        if (it + 1) % print_interval == 0:\n",
    "            loss_avg = sum(loss_avg) / len(loss_avg)\n",
    "            lr = optim.lr\n",
    "            ed = time.time()\n",
    "            t_intv, glob_t_intv = ed - st, ed - glob_st\n",
    "            eta = int((max_iter - it) * (glob_t_intv / it))\n",
    "            eta = str(timedelta(seconds=eta))\n",
    "\n",
    "            loss_boundery_bce_avg = sum(loss_boundery_bce) / len(loss_boundery_bce)\n",
    "            loss_boundery_dice_avg = sum(loss_boundery_dice) / len(loss_boundery_dice)\n",
    "            msg = ', '.join([\n",
    "                'epoch: {epoch}/{max_epoch}'\n",
    "                'it: {it}/{max_it}',\n",
    "                'lr: {lr:4f}',\n",
    "                'loss: {loss:.4f}',\n",
    "                'boundery_bce_loss: {boundery_bce_loss:.4f}',\n",
    "                'boundery_dice_loss: {boundery_dice_loss:.4f}',\n",
    "                'eta: {eta}',\n",
    "                'time: {time:.4f}',\n",
    "            ]).format(\n",
    "                epoch = epoch_id,\n",
    "                max_epoch = max_epoch,\n",
    "                it = it+1,\n",
    "                max_it = epoch_iteration,\n",
    "                lr = lr,\n",
    "                loss = loss_avg,\n",
    "                boundery_bce_loss = loss_boundery_bce_avg,\n",
    "                boundery_dice_loss = loss_boundery_dice_avg,\n",
    "                time = t_intv,\n",
    "                eta = eta\n",
    "            )\n",
    "            \n",
    "            logger.info(msg)\n",
    "            print(\"loss/train_loss\", loss.item(), it + 1)\n",
    "            loss_avg = []\n",
    "            loss_boundery_bce = []\n",
    "            loss_boundery_dice = []\n",
    "            st = ed\n",
    "\n",
    "        if ((it + 1) % val_interval == 0 and it + 10 < epoch_iteration) or (it + 1) % epoch_iteration == 0:\n",
    "            print('validation')\n",
    "            torch.cuda.empty_cache()\n",
    "            model.eval()\n",
    "            loss_all = 0\n",
    "            loss_n = 0\n",
    "            with torch.no_grad():\n",
    "                for i_val, (images_val, labels_val) in enumerate(dlval):\n",
    "                    if (i_val + 1) % 50 == 0:\n",
    "                        print(i_val + 1)\n",
    "\n",
    "                    images_val = images_val.to(device)\n",
    "                    labels_val = labels_val.to(device)\n",
    "                    labels_val = torch.squeeze(labels_val, 1)\n",
    "\n",
    "                    outputs = model(images_val)[0]\n",
    "                    val_loss = criteria_val(outputs, labels_val)\n",
    "\n",
    "                    pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "                    gt = labels_val.data.cpu().numpy()\n",
    "\n",
    "                    running_metrics_val.update(gt, pred)\n",
    "                    val_loss_meter.update(val_loss.item())\n",
    "\n",
    "            print(\"loss/val_loss\", val_loss_meter.avg)\n",
    "            logger.info(\"Epoch %3d Iter %d Val Loss: %.4f\" % (epoch_id, it + 1, val_loss_meter.avg))\n",
    "\n",
    "            score, class_iou = running_metrics_val.get_scores()\n",
    "            for k, v in score.items():\n",
    "                print(k, v)\n",
    "                logger.info(\"{}: {}\".format(k, v))\n",
    "                print(\"val_metrics/{}\".format(k), v)\n",
    "\n",
    "            for k, v in class_iou.items():\n",
    "                logger.info(\"{}: {}\".format(k, v))\n",
    "                print(\"val_metrics/cls_{}\".format(k), v)\n",
    "\n",
    "            val_loss_meter.reset()\n",
    "            running_metrics_val.reset()\n",
    "\n",
    "            state = {\n",
    "                    \"epoch\": epoch_id,\n",
    "                    \"iteration\": it+ 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optim.get_state(),\n",
    "            }\n",
    "            save_path = os.path.join(\n",
    "                writer.file_writer.get_logdir(),\n",
    "                \"{}_{}_checkpoint.pkl\".format(model_arch, cfg_data['dataset']),\n",
    "            )\n",
    "            torch.save(state, save_path)\n",
    "\n",
    "            if score[\"Mean IoU : \\t\"] >= best_iou:\n",
    "                best_iou = score[\"Mean IoU : \\t\"]\n",
    "                state = {\n",
    "                    \"epoch\": epoch_id,\n",
    "                    \"iteration\":it+ 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"best_iou\": best_iou,\n",
    "                }\n",
    "                save_path = os.path.join(\n",
    "                    writer.file_writer.get_logdir(),\n",
    "                    \"{}_{}_best_model.pkl\".format(model_arch, cfg_data['dataset']),\n",
    "                )\n",
    "                torch.save(state, save_path)\n",
    "            torch.cuda.empty_cache()\n",
    "    it = 0"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss/train_loss 3.4885222911834717 10\n",
      "loss/train_loss 3.0776875019073486 20\n",
      "loss/train_loss 3.1200971603393555 30\n",
      "loss/train_loss 2.9808926582336426 40\n",
      "loss/train_loss 2.7536351680755615 50\n",
      "loss/train_loss 3.56929874420166 60\n",
      "loss/train_loss 2.966944694519043 70\n",
      "loss/train_loss 2.6964364051818848 80\n",
      "loss/train_loss 3.0642142295837402 90\n",
      "loss/train_loss 2.96091365814209 100\n",
      "loss/train_loss 2.698448419570923 110\n",
      "loss/train_loss 2.9087462425231934 120\n",
      "loss/train_loss 3.096327066421509 130\n",
      "loss/train_loss 2.661802291870117 140\n",
      "loss/train_loss 3.1281838417053223 150\n",
      "loss/train_loss 2.584542751312256 160\n",
      "loss/train_loss 3.3675496578216553 170\n",
      "loss/train_loss 3.103978157043457 180\n",
      "loss/train_loss 3.207629680633545 190\n",
      "loss/train_loss 2.679269313812256 200\n",
      "loss/train_loss 2.7626495361328125 210\n",
      "loss/train_loss 2.8811488151550293 220\n",
      "loss/train_loss 2.3620355129241943 230\n",
      "loss/train_loss 2.3911843299865723 240\n",
      "loss/train_loss 2.940701484680176 250\n",
      "loss/train_loss 2.6687369346618652 260\n",
      "loss/train_loss 3.039407730102539 270\n",
      "loss/train_loss 2.956350326538086 280\n",
      "loss/train_loss 3.530290365219116 290\n",
      "loss/train_loss 3.1659295558929443 300\n",
      "loss/train_loss 2.888807773590088 310\n",
      "loss/train_loss 3.043426513671875 320\n",
      "loss/train_loss 3.2390296459198 330\n",
      "loss/train_loss 2.849968910217285 340\n",
      "loss/train_loss 3.3465588092803955 350\n",
      "loss/train_loss 2.8358466625213623 360\n",
      "loss/train_loss 2.977304458618164 370\n",
      "validation\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "loss/val_loss 1.3446982846260072\n",
      "Overall Acc: \t 0.876353452672861\n",
      "val_metrics/Overall Acc: \t 0.876353452672861\n",
      "Mean Acc : \t 0.46028753127832095\n",
      "val_metrics/Mean Acc : \t 0.46028753127832095\n",
      "FreqW Acc : \t 0.803022306925857\n",
      "val_metrics/FreqW Acc : \t 0.803022306925857\n",
      "Mean IoU : \t 0.3876080679362237\n",
      "val_metrics/Mean IoU : \t 0.3876080679362237\n",
      "val_metrics/cls_0 0.9440806115501955\n",
      "val_metrics/cls_1 0.6173585605066831\n",
      "val_metrics/cls_2 0.7856764731010198\n",
      "val_metrics/cls_3 0.14204640179525227\n",
      "val_metrics/cls_4 0.10567763543203071\n",
      "val_metrics/cls_5 0.27253069186768586\n",
      "val_metrics/cls_6 0.1714418005373968\n",
      "val_metrics/cls_7 0.38813971663704944\n",
      "val_metrics/cls_8 0.8055552849765686\n",
      "val_metrics/cls_9 0.32868307611306724\n",
      "val_metrics/cls_10 0.8947777930060835\n",
      "val_metrics/cls_11 0.4794060328685629\n",
      "val_metrics/cls_12 0.0074582550161759915\n",
      "val_metrics/cls_13 0.7778568718212072\n",
      "val_metrics/cls_14 0.003098665009608502\n",
      "val_metrics/cls_15 0.13189507358199518\n",
      "val_metrics/cls_16 0.01720424975419773\n",
      "val_metrics/cls_17 0.01998151841821285\n",
      "val_metrics/cls_18 0.4716845787952573\n",
      "loss/train_loss 2.9431214332580566 10\n",
      "loss/train_loss 2.5984954833984375 20\n",
      "loss/train_loss 3.421689987182617 30\n",
      "loss/train_loss 3.309673309326172 40\n",
      "loss/train_loss 2.7877309322357178 50\n",
      "loss/train_loss 3.2859625816345215 60\n",
      "loss/train_loss 2.620457649230957 70\n",
      "loss/train_loss 2.6123459339141846 80\n",
      "loss/train_loss 3.0365922451019287 90\n",
      "loss/train_loss 2.7770509719848633 100\n",
      "loss/train_loss 2.731640338897705 110\n",
      "loss/train_loss 3.3104188442230225 120\n",
      "loss/train_loss 3.0603561401367188 130\n",
      "loss/train_loss 2.64314866065979 140\n",
      "loss/train_loss 3.7473630905151367 150\n",
      "loss/train_loss 2.509547710418701 160\n",
      "loss/train_loss 3.9988908767700195 170\n",
      "loss/train_loss 2.732050895690918 180\n",
      "loss/train_loss 2.7825140953063965 190\n",
      "loss/train_loss 3.025888681411743 200\n",
      "loss/train_loss 2.654420852661133 210\n",
      "loss/train_loss 2.9531776905059814 220\n",
      "loss/train_loss 3.0966498851776123 230\n",
      "loss/train_loss 2.5640032291412354 240\n",
      "loss/train_loss 3.694061517715454 250\n",
      "loss/train_loss 2.778059244155884 260\n",
      "loss/train_loss 2.813218832015991 270\n",
      "loss/train_loss 2.7777774333953857 280\n",
      "loss/train_loss 3.7886290550231934 290\n",
      "loss/train_loss 3.1366193294525146 300\n",
      "loss/train_loss 3.09409236907959 310\n",
      "loss/train_loss 3.0485267639160156 320\n",
      "loss/train_loss 3.0316085815429688 330\n",
      "loss/train_loss 3.044720411300659 340\n",
      "loss/train_loss 3.070892333984375 350\n",
      "loss/train_loss 2.5718414783477783 360\n",
      "loss/train_loss 2.7753965854644775 370\n",
      "validation\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "loss/val_loss 1.2310838508605957\n",
      "Overall Acc: \t 0.8707040354995503\n",
      "val_metrics/Overall Acc: \t 0.8707040354995503\n",
      "Mean Acc : \t 0.5273099649232263\n",
      "val_metrics/Mean Acc : \t 0.5273099649232263\n",
      "FreqW Acc : \t 0.8010606230216312\n",
      "val_metrics/FreqW Acc : \t 0.8010606230216312\n",
      "Mean IoU : \t 0.4010008191791133\n",
      "val_metrics/Mean IoU : \t 0.4010008191791133\n",
      "val_metrics/cls_0 0.9351758964278645\n",
      "val_metrics/cls_1 0.6086709264287256\n",
      "val_metrics/cls_2 0.7638470073231333\n",
      "val_metrics/cls_3 0.10156399887837778\n",
      "val_metrics/cls_4 0.19094850980599562\n",
      "val_metrics/cls_5 0.23365032359007767\n",
      "val_metrics/cls_6 0.22202433619171263\n",
      "val_metrics/cls_7 0.330480140340109\n",
      "val_metrics/cls_8 0.8343445736043981\n",
      "val_metrics/cls_9 0.3725112210438932\n",
      "val_metrics/cls_10 0.8645675828883518\n",
      "val_metrics/cls_11 0.4677800796060023\n",
      "val_metrics/cls_12 0.02410147821528536\n",
      "val_metrics/cls_13 0.8128973346853471\n",
      "val_metrics/cls_14 0.030576243628981026\n",
      "val_metrics/cls_15 0.2627364007171276\n",
      "val_metrics/cls_16 0.03704845739702412\n",
      "val_metrics/cls_17 0.0615467709163859\n",
      "val_metrics/cls_18 0.4645442827143608\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RH8UeQwnQlgY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# with torch.no_grad():\n",
    "#     for (images_val, labels_val, _) in valloader:\n",
    "#         images_val = images_val.to(device)\n",
    "#         labels_val = labels_val.to(device)\n",
    "\n",
    "#         outputs = model(images_val)\n",
    "#         outputs = output_val_upsample(outputs)\n",
    "#         val_loss = loss_fn(input=outputs, target=labels_val)\n",
    "\n",
    "#         pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "#         gt = labels_val.data.cpu().numpy()\n",
    "\n",
    "#         running_metrics_val.update(gt, pred)\n",
    "#         val_loss_meter.update(val_loss.item())\n",
    "\n",
    "# writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n",
    "# logger.info(\"Iter %d Val Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n",
    "\n",
    "# score, class_iou = running_metrics_val.get_scores()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}