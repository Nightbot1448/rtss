{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DDRNet_HBD_B2N_DGT.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DDRNet+hardnet+bottle2nek+DetailGT\n",
    "\n",
    "Output:\n",
    "1. `seghead(FFM(DAPPPM, layer5_)` - main\n",
    "2. `seghead(layer3_ + upsample(layer3))`\n",
    "3. `seghead(layer4_ + upsample(layer4))`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.utils import data\n",
    "from datetime import datetime, timedelta\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    " \n",
    "!pip install tensorboardX\n",
    "from tensorboardX import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BatchNorm2d = nn.BatchNorm2d\n",
    "ReLU = nn.ReLU\n",
    "AvgPool2d = nn.AvgPool2d\n",
    "AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_workers = 2\n",
    "print_interval=10\n",
    "val_interval=500\n",
    "\n",
    "n_classes = 19\n",
    "\n",
    "model_arch = 'ddrnet_slim_23'\n",
    "\n",
    "bn_mom = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "float_tensor_type = torch.cuda.FloatTensor if device.type=='cuda' else torch.FloatTensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, need_relu=True, *args, **kwargs):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_chan,\n",
    "                out_chan,\n",
    "                kernel_size = ks,\n",
    "                stride = stride,\n",
    "                padding = padding,\n",
    "                bias = False)\n",
    "        # self.bn = BatchNorm2d(out_chan)\n",
    "        self.bn = BatchNorm2d(out_chan)\n",
    "        self.need_relu = need_relu\n",
    "        if need_relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.need_relu:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StepDAPPM(nn.Module):\n",
    "    def __init__(self, pool_ks, pool_stride, pool_padding, bn_inplanes, conv_out_planes, conv_ks=1):\n",
    "        super(StepDAPPM, self).__init__()\n",
    "        self.pool = None\n",
    "        if pool_ks > 1:\n",
    "            self.pool = nn.AvgPool2d(kernel_size=pool_ks, stride=pool_stride, padding=pool_padding)\n",
    "        elif pool_ks == 1:\n",
    "            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.bn = BatchNorm2d(bn_inplanes, momentum=bn_mom)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(bn_inplanes, conv_out_planes, kernel_size=conv_ks, padding=conv_ks//2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pool is not None:\n",
    "            x = self.pool(x)\n",
    "        x = self.conv(self.relu(self.bn(x)))\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DAPPM(nn.Module):\n",
    "    def __init__(self, inplanes, branch_planes, outplanes):\n",
    "        super(DAPPM, self).__init__()\n",
    "        self.scale1 = StepDAPPM( 5, 2, 2, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale2 = StepDAPPM( 9, 4, 4, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale3 = StepDAPPM(17, 8, 8, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale4 = StepDAPPM( 1, 0, 0, inplanes, branch_planes, conv_ks=1)\n",
    "        self.scale0 = StepDAPPM( 0, 0, 0, inplanes, branch_planes, conv_ks=1)\n",
    "\n",
    "        self.process1 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        self.process2 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        self.process3 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        self.process4 = StepDAPPM(0, 0, 0, branch_planes, branch_planes, conv_ks=3)\n",
    "        \n",
    "        self.compression = StepDAPPM(0, 0, 0, branch_planes * 5, outplanes, conv_ks=1)\n",
    "        self.shortcut = StepDAPPM(0, 0, 0, inplanes, outplanes, conv_ks=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = self.downsample(x)\n",
    "        width = x.shape[-1]\n",
    "        height = x.shape[-2]\n",
    "        x_list = []\n",
    "\n",
    "        x_list.append(self.scale0(x))\n",
    "        x_list.append(self.process1((F.interpolate(self.scale1(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[0])))\n",
    "        x_list.append((self.process2((F.interpolate(self.scale2(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[1]))))\n",
    "        x_list.append(self.process3((F.interpolate(self.scale3(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[2])))\n",
    "        x_list.append(self.process4((F.interpolate(self.scale4(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)+x_list[3])))\n",
    "       \n",
    "        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n",
    "        return out    \n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class segmenthead(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n",
    "        super(segmenthead, self).__init__()\n",
    "        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n",
    "        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(self.relu(self.bn1(x)))\n",
    "        out = self.conv2(self.relu(self.bn2(x)))\n",
    "\n",
    "        if self.scale_factor is not None:\n",
    "            height = x.shape[-2] * self.scale_factor\n",
    "            width = x.shape[-1] * self.scale_factor\n",
    "            out = F.interpolate(out,\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(out_chan,\n",
    "                out_chan//4,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                padding = 0,\n",
    "                bias = False)\n",
    "        self.conv2 = nn.Conv2d(out_chan//4,\n",
    "                out_chan,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                padding = 0,\n",
    "                bias = False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, fsp, fcp):\n",
    "        fcat = torch.cat([fsp, fcp], dim=1)\n",
    "        feat = self.convblk(fcat)\n",
    "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
    "        atten = self.conv1(atten)\n",
    "        atten = self.relu(atten)\n",
    "        atten = self.conv2(atten)\n",
    "        atten = self.sigmoid(atten)\n",
    "        feat_atten = torch.mul(feat, atten)\n",
    "        feat_out = feat_atten + feat\n",
    "        return feat_out\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HarDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False):\n",
    "        super(HarDBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.grmul = grmul\n",
    "        self.links = []\n",
    "        self.out_channels = 0\n",
    "        self.keepBase = keepBase\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(n_layers):\n",
    "            out_ch, in_ch, el_links = self.get_links(i+1)\n",
    "            self.links.append(el_links)\n",
    "            self.layers.append(ConvBNReLU(in_ch, out_ch, need_relu=(n_layers - 1 != i)))\n",
    "            if (i % 2 == 0) or (i == n_layers - 1):\n",
    "                self.out_channels += out_ch\n",
    "\n",
    "    def __out_ch(self, layer_id): \n",
    "        out_ch = self.growth_rate\n",
    "        for i in range(1, int(math.log2(layer_id))+1):\n",
    "            if layer_id % 2**i == 0:\n",
    "                out_ch = out_ch*self.grmul\n",
    "        return int(int(out_ch + 1) / 2) * 2\n",
    "\n",
    "    def get_links(self, layer_id):\n",
    "        in_ch = 0\n",
    "        links_ = []\n",
    "        for i in range(int(math.log2(layer_id))):\n",
    "            diff = 2**i\n",
    "            if (layer_id % diff == 0) and layer_id - diff > 0:\n",
    "                in_ch += self.__out_ch(layer_id - diff)\n",
    "                links_.append(layer_id - diff)\n",
    "        if math.log2(layer_id).is_integer():\n",
    "            in_ch += self.in_channels\n",
    "            links_.append(0)\n",
    "        return self.__out_ch(layer_id), in_ch, links_\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = [x]\n",
    "        for layer in range(len(self.layers)):\n",
    "            layer_input = []\n",
    "            for link in self.links[layer]:\n",
    "                layer_input.append(data[link])\n",
    "            in_ = layer_input[0] if len(layer_input) == 1 \\\n",
    "                else torch.cat(layer_input, dim=1)\n",
    "            data.append(self.layers[layer](in_))\n",
    "        t = len(data)\n",
    "        out = []\n",
    "        for i in range(t):\n",
    "            if (i % 2 == 1) or (self.keepBase and i == 0) or (i == t-1):\n",
    "                out.append(data[i])\n",
    "        return torch.cat(out, dim=1)\n",
    "    \n",
    "    def get_out_ch(self):\n",
    "        return self.out_channels\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.layers.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvHarDBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, growth_rate=10, grmul=1.7, n_layers=4):\n",
    "        super(ConvHarDBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        if stride != 1:\n",
    "            self.layers.append(ConvBNReLU(inplanes, planes, ks=1, stride=stride,\\\n",
    "                                          padding=0, need_relu=True))\n",
    "        hardblock_inplanes = planes if stride != 1 else inplanes\n",
    "        hardblock = HarDBlock(hardblock_inplanes, growth_rate, grmul, n_layers)\n",
    "        self.layers.append(hardblock)\n",
    "\n",
    "        self.layers.append(ConvBNReLU(hardblock.get_out_ch(), planes, ks=1, \\\n",
    "                                      stride=1, padding=0, need_relu=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x        \n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.layers.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Bottle2neck(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale = 4, stype='normal'):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            inplanes: input channel dimensionality\n",
    "            planes: output channel dimensionality\n",
    "            stride: conv stride. Replaces pooling layer.\n",
    "            downsample: None when stride = 1\n",
    "            baseWidth: basic width of conv3x3\n",
    "            scale: number of scale.\n",
    "            type: 'normal': normal set. 'stage': first block of a new stage.\n",
    "        \"\"\"\n",
    "        super(Bottle2neck, self).__init__()\n",
    "\n",
    "        width = int(math.floor(planes * (baseWidth/64.0)))\n",
    "        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width*scale)\n",
    "        \n",
    "        if scale == 1:\n",
    "          self.nums = 1\n",
    "        else:\n",
    "          self.nums = scale -1\n",
    "        if stype == 'stage':\n",
    "            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n",
    "        convs = []\n",
    "        bns = []\n",
    "        for i in range(self.nums):\n",
    "          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False))\n",
    "          bns.append(nn.BatchNorm2d(width))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width*scale, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stype = stype\n",
    "        self.scale = scale\n",
    "        self.width  = width\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "          if i==0 or self.stype=='stage':\n",
    "            sp = spx[i]\n",
    "          else:\n",
    "            sp = sp + spx[i]\n",
    "          sp = self.convs[i](sp)\n",
    "          sp = self.relu(self.bns[i](sp))\n",
    "          if i==0:\n",
    "            out = sp\n",
    "          else:\n",
    "            out = torch.cat((out, sp), 1)\n",
    "        if self.scale != 1 and self.stype=='normal':\n",
    "          out = torch.cat((out, spx[self.nums]),1)\n",
    "        elif self.scale != 1 and self.stype=='stage':\n",
    "          out = torch.cat((out, self.pool(spx[self.nums])),1)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, (BatchNorm2d, ReLU)):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DualResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=19, planes=64, spp_planes=128, head_planes=128, augment=False):\n",
    "        super(DualResNet, self).__init__()\n",
    "\n",
    "        growth_rate = [  10,16,18,24,32]\n",
    "        highres_planes = planes * 2\n",
    "        self.augment = augment\n",
    "\n",
    "\n",
    "        self.conv0 = ConvBNReLU(     3, planes, ks=3, stride=2, padding=1)\n",
    "        self.conv1 = ConvBNReLU(planes, planes, ks=3, stride=2, padding=1)\n",
    "\n",
    "        # self.conv1 =  nn.Sequential(\n",
    "        #                   nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
    "        #                   BatchNorm2d(planes, momentum=bn_mom),\n",
    "        #                   nn.ReLU(inplace=True),\n",
    "        #                   nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
    "        #                   BatchNorm2d(planes, momentum=bn_mom),\n",
    "        #                   nn.ReLU(inplace=True),\n",
    "        #               )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.layer1 = ConvHarDBlock(planes,     planes,     stride=1, growth_rate=growth_rate[0])\n",
    "        self.layer2 = ConvHarDBlock(planes,     planes * 2, stride=2, growth_rate=growth_rate[1])\n",
    "        self.layer3 = ConvHarDBlock(planes * 2, planes * 4, stride=2, growth_rate=growth_rate[2])\n",
    "        self.layer4 = ConvHarDBlock(planes * 4, planes * 8, stride=2, growth_rate=growth_rate[3])\n",
    "\n",
    "\n",
    "        self.compression3 = ConvBNReLU(planes * 4, highres_planes, ks=1, stride=1, padding=0, need_relu=False)\n",
    "        self.compression4 = ConvBNReLU(planes * 8, highres_planes, ks=1, stride=1, padding=0, need_relu=False)\n",
    "\n",
    "        # self.compression3 = nn.Sequential(\n",
    "        #                                   nn.Conv2d(planes * 4, highres_planes, kernel_size=1, bias=False),\n",
    "        #                                   BatchNorm2d(highres_planes, momentum=bn_mom),\n",
    "        #                                   )\n",
    "\n",
    "        # self.compression4 = nn.Sequential(\n",
    "        #                                   nn.Conv2d(planes * 8, highres_planes, kernel_size=1, bias=False),\n",
    "        #                                   BatchNorm2d(highres_planes, momentum=bn_mom),\n",
    "        #                                   )\n",
    "\n",
    "\n",
    "        self.down3 = ConvBNReLU(highres_planes, planes * 4, ks=3, stride=2, padding=1, need_relu=False)\n",
    "        self.down4_0 = ConvBNReLU(highres_planes, planes * 4, ks=3, stride=2, padding=1, need_relu=True)\n",
    "        self.down4_1 = ConvBNReLU(planes * 4, planes * 8, ks=3, stride=2, padding=1, need_relu=False)\n",
    "        # self.down3 = nn.Sequential(\n",
    "        #                            nn.Conv2d(highres_planes, planes * 4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "        #                            BatchNorm2d(planes * 4, momentum=bn_mom),\n",
    "        #                            )\n",
    "\n",
    "        # self.down4 = nn.Sequential(\n",
    "        #                            nn.Conv2d(highres_planes, planes * 4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "        #                            BatchNorm2d(planes * 4, momentum=bn_mom),\n",
    "        #                            nn.ReLU(inplace=True),\n",
    "        #                            nn.Conv2d(planes * 4, planes * 8, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "        #                            BatchNorm2d(planes * 8, momentum=bn_mom),\n",
    "        #                            )\n",
    "\n",
    "        self.layer3_ = ConvHarDBlock(planes * 2, highres_planes, stride=1, growth_rate=growth_rate[2])\n",
    "        \n",
    "        self.layer4_ = ConvHarDBlock(highres_planes, highres_planes, stride=1, growth_rate=growth_rate[2])\n",
    "\n",
    "        self.layer5_ = self._make_bottle2neck(highres_planes, highres_planes)\n",
    "        \n",
    "        self.layer5 =  self._make_bottle2neck(planes * 8, planes * 8, stride=2)\n",
    "        \n",
    "        self.spp = DAPPM(planes * 8 * Bottle2neck.expansion, spp_planes, planes * 4)\n",
    "\n",
    "        self.ffm = FeatureFusionModule(planes * 4 + highres_planes * 2, planes * 4)\n",
    "\n",
    "        self.seghead_out1 = segmenthead(planes * 4, head_planes, num_classes)\n",
    "        self.seghead_out2 = segmenthead(highres_planes, head_planes, 1)\n",
    "        self.seghead_out3 = segmenthead(highres_planes, head_planes, num_classes)            \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_bottle2neck(self, inplanes, planes, stride=1):\n",
    "        '''\n",
    "            outplanes = planes * 4\n",
    "        '''\n",
    "\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * Bottle2neck.expansion:\n",
    "            downsample = ConvBNReLU(inplanes, planes * Bottle2neck.expansion, ks=3, stride=stride, need_relu=False)\n",
    "        stype = 'stage' if stride != 1 else 'normal'\n",
    "        \n",
    "        return Bottle2neck(inplanes, planes, stride, downsample, stype=stype)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        width, height = x.shape[-1], x.shape[-2]\n",
    "        width_output, height_output = x.shape[-1] // 8, x.shape[-2] // 8\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        layers.append(x)\n",
    "\n",
    "        x = self.layer2(self.relu(x))\n",
    "        layers.append(x)\n",
    "  \n",
    "        x = self.layer3(self.relu(x))\n",
    "        layers.append(x)\n",
    "        x_ = self.layer3_(self.relu(layers[1]))\n",
    "\n",
    "        x = x + self.down3(self.relu(x_))\n",
    "        step_res = self.compression3(self.relu(layers[2]))\n",
    "        x_ = x_ + F.interpolate(\n",
    "                        step_res,\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "        out_2 = x_\n",
    "\n",
    "        x = self.layer4(self.relu(x))\n",
    "        layers.append(x)\n",
    "        x_ = self.layer4_(self.relu(x_))\n",
    "\n",
    "        x = x + self.down4_1(self.down4_0(self.relu(x_)))\n",
    "        x_ = x_ + F.interpolate(\n",
    "                        self.compression4(self.relu(layers[3])),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out_3 = x_\n",
    "\n",
    "        x_ = self.layer5_(self.relu(x_))\n",
    "        x = F.interpolate(\n",
    "                        self.spp(self.layer5(self.relu(x))),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out_1 = self.ffm(x, x_)\n",
    "        \n",
    "        out_1 = self.seghead_out1(out_1)\n",
    "        out_2 = self.seghead_out2(out_2)\n",
    "        out_3 = self.seghead_out3(out_3)\n",
    "\n",
    "        out_1 = F.interpolate(out_1, (height, width), mode='bilinear', align_corners=True)\n",
    "        out_2 = F.interpolate(out_2, (height, width), mode='bilinear', align_corners=True)\n",
    "        out_3 = F.interpolate(out_3, (height, width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return out_1, out_2, out_3\n",
    "        # return x\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params() \\\n",
    "                if not isinstance(child, ReLU) else ([],[])\n",
    "            wd_params += child_wd_params\n",
    "            nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = DualResNet(num_classes=19, planes=64, spp_planes=128, head_planes=128, augment=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), 'model_maxmIOU103.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Loss & Optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dice_loss_func(input, target):\n",
    "    smooth = 1.\n",
    "    n = input.size(0)\n",
    "    iflat = input.view(n, -1)\n",
    "    tflat = target.view(n, -1)\n",
    "    intersection = (iflat * tflat).sum(1)\n",
    "    loss = 1 - ((2. * intersection + smooth) /\n",
    "                (iflat.sum(1) + tflat.sum(1) + smooth))\n",
    "    return loss.mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OhemCELoss(nn.Module):\n",
    "    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n",
    "        super(OhemCELoss, self).__init__()\n",
    "        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).to(device)\n",
    "        self.n_min = n_min\n",
    "        self.ignore_lb = ignore_lb\n",
    "        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        N, C, H, W = logits.size()\n",
    "        loss = self.criteria(logits, labels).view(-1)\n",
    "        loss, _ = torch.sort(loss, descending=True)\n",
    "        if loss[self.n_min] > self.thresh:\n",
    "            loss = loss[loss>self.thresh]\n",
    "        else:\n",
    "            loss = loss[:self.n_min]\n",
    "        return torch.mean(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DetailAggregateLoss(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DetailAggregateLoss, self).__init__()\n",
    "        \n",
    "        self.laplacian_kernel = torch.tensor(\n",
    "            [-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "            dtype=torch.float32).reshape(1, 1, 3, 3).requires_grad_(False).type(float_tensor_type)        \n",
    "\n",
    "        self.fuse_kernel = torch.nn.Parameter(torch.tensor([[6./10], [3./10], [1./10]],\n",
    "            dtype=torch.float32).reshape(1, 3, 1, 1).type(float_tensor_type))\n",
    "\n",
    "    def forward(self, boundary_logits, gtmasks):\n",
    "\n",
    "        boundary_targets = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, padding=1)\n",
    "        boundary_targets = boundary_targets.clamp(min=0)\n",
    "        boundary_targets[boundary_targets > 0.1] = 1\n",
    "        boundary_targets[boundary_targets <= 0.1] = 0\n",
    "\n",
    "        boundary_targets_x2 = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, stride=2, padding=1)\n",
    "        boundary_targets_x2 = boundary_targets_x2.clamp(min=0)\n",
    "        \n",
    "        boundary_targets_x4 = F.conv2d(gtmasks.unsqueeze(1).type(float_tensor_type), self.laplacian_kernel, stride=4, padding=1)\n",
    "        boundary_targets_x4 = boundary_targets_x4.clamp(min=0)\n",
    "\n",
    "        boundary_targets_x4_up = F.interpolate(boundary_targets_x4, boundary_targets.shape[2:], mode='nearest')\n",
    "        boundary_targets_x2_up = F.interpolate(boundary_targets_x2, boundary_targets.shape[2:], mode='nearest')\n",
    "        \n",
    "        boundary_targets_x2_up[boundary_targets_x2_up > 0.1] = 1\n",
    "        boundary_targets_x2_up[boundary_targets_x2_up <= 0.1] = 0\n",
    "        \n",
    "        \n",
    "        boundary_targets_x4_up[boundary_targets_x4_up > 0.1] = 1\n",
    "        boundary_targets_x4_up[boundary_targets_x4_up <= 0.1] = 0\n",
    "       \n",
    "        boudary_targets_pyramids = torch.stack((boundary_targets, boundary_targets_x2_up, boundary_targets_x4_up), dim=1)\n",
    "        \n",
    "        boudary_targets_pyramids = boudary_targets_pyramids.squeeze(2)\n",
    "        boudary_targets_pyramid = F.conv2d(boudary_targets_pyramids, self.fuse_kernel)\n",
    "\n",
    "        boudary_targets_pyramid[boudary_targets_pyramid > 0.1] = 1\n",
    "        boudary_targets_pyramid[boudary_targets_pyramid <= 0.1] = 0\n",
    "        \n",
    "        \n",
    "        if boundary_logits.shape[-1] != boundary_targets.shape[-1]:\n",
    "            boundary_logits = F.interpolate(\n",
    "                boundary_logits, boundary_targets.shape[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        bce_loss = F.binary_cross_entropy_with_logits(boundary_logits, boudary_targets_pyramid)\n",
    "        dice_loss = dice_loss_func(torch.sigmoid(boundary_logits), boudary_targets_pyramid)\n",
    "        return bce_loss,  dice_loss\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "                nowd_params += list(module.parameters())\n",
    "        return nowd_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, model, loss, lr0, momentum, wd, warmup_steps, \n",
    "                 warmup_start_lr, max_iter, power, *args, **kwargs):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.lr0 = lr0\n",
    "        self.lr = self.lr0\n",
    "        self.max_iter = float(max_iter)\n",
    "        self.power = power\n",
    "        self.it = 0\n",
    "        wd_params, nowd_params = model.get_params()\n",
    "        loss_nowd_params = loss.get_params()\n",
    "        param_list = [\n",
    "                {'params': wd_params},\n",
    "                {'params': nowd_params, 'weight_decay': 0},\n",
    "                {'params': loss_nowd_params}]\n",
    "        self.optim = torch.optim.SGD(\n",
    "                param_list,\n",
    "                # model.parameters(),\n",
    "                lr = lr0,\n",
    "                momentum = momentum,\n",
    "                weight_decay = wd)\n",
    "        self.warmup_factor = (self.lr0/self.warmup_start_lr)**(1./self.warmup_steps)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.it <= self.warmup_steps:\n",
    "            lr = self.warmup_start_lr*(self.warmup_factor**self.it)\n",
    "        else:\n",
    "            factor = (1-(self.it-self.warmup_steps)/(self.max_iter-self.warmup_steps))**self.power\n",
    "            lr = self.lr0 * factor\n",
    "        return lr\n",
    "\n",
    "    def step(self):\n",
    "        self.lr = self.get_lr()\n",
    "        for pg in self.optim.param_groups:\n",
    "            if pg.get('lr_mul', False):\n",
    "                pg['lr'] = self.lr * 10\n",
    "            else:\n",
    "                pg['lr'] = self.lr\n",
    "        if self.optim.defaults.get('lr_mul', False):\n",
    "            self.optim.defaults['lr'] = self.lr * 10\n",
    "        else:\n",
    "            self.optim.defaults['lr'] = self.lr\n",
    "        self.it += 1\n",
    "        self.optim.step()\n",
    "        if self.it == self.warmup_steps+2:\n",
    "            logger.info('==> warmup done, start to implement poly lr strategy')\n",
    "    \n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'warmup_start_lr': self.warmup_start_lr,\n",
    "            'lr0': self.lr0,\n",
    "            'lr': self.lr,\n",
    "            'max_iter': self.max_iter,\n",
    "            'power': self.power, \n",
    "            'it': self.it,\n",
    "            'optim_state': self.optim.state_dict(),\n",
    "            'warmup_factor': self.warmup_factor\n",
    "        }\n",
    "\n",
    "    def load_state(self, state):\n",
    "        self.warmup_steps = state.get('warmup_steps')\n",
    "        self.warmup_start_lr = state.get('warmup_start_lr')\n",
    "        self.lr0 = state.get('lr0')\n",
    "        self.lr = state.get('lr')\n",
    "        self.max_iter = state.get('max_iter')\n",
    "        self.power = state.get('power')\n",
    "        self.it = state.get('it')\n",
    "        self.optim.load_state_dict(state.get('optim_state'))\n",
    "        self.warmup_factor = state.get('warmup_factor')\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optim.zero_grad()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_thres = 0.7\n",
    "n_img_per_gpu = 8\n",
    "cropsize = (512, 1024)\n",
    "n_min = n_img_per_gpu*cropsize[0]*cropsize[1]//32\n",
    "ignore_idx=255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/MichaelFan01/STDC-Seg/master/cityscapes_info.json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL.ImageEnhance as ImageEnhance\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size, *args, **kwargs):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        im = im_lb['im']\n",
    "        lb = im_lb['lb']\n",
    "        assert im.size == lb.size\n",
    "        W, H = self.size\n",
    "        w, h = im.size\n",
    "\n",
    "        if (W, H) == (w, h): return dict(im=im, lb=lb)\n",
    "        if w < W or h < H:\n",
    "            scale = float(W) / w if w < h else float(H) / h\n",
    "            w, h = int(scale * w + 1), int(scale * h + 1)\n",
    "            im = im.resize((w, h), Image.BILINEAR)\n",
    "            lb = lb.resize((w, h), Image.NEAREST)\n",
    "        sw, sh = random.random() * (w - W), random.random() * (h - H)\n",
    "        crop = int(sw), int(sh), int(sw) + W, int(sh) + H\n",
    "        return dict(\n",
    "                im = im.crop(crop),\n",
    "                lb = lb.crop(crop)\n",
    "                    )\n",
    "\n",
    "\n",
    "class HorizontalFlip(object):\n",
    "    def __init__(self, p=0.5, *args, **kwargs):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        if random.random() > self.p:\n",
    "            return im_lb\n",
    "        else:\n",
    "            im = im_lb['im']\n",
    "            lb = im_lb['lb']\n",
    "            return dict(im = im.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "                        lb = lb.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "                    )\n",
    "\n",
    "\n",
    "class RandomScale(object):\n",
    "    def __init__(self, scales=(1, ), *args, **kwargs):\n",
    "        self.scales = scales\n",
    "        # print('scales: ', scales)\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        im = im_lb['im']\n",
    "        lb = im_lb['lb']\n",
    "        W, H = im.size\n",
    "        scale = random.choice(self.scales)\n",
    "        # scale = np.random.uniform(min(self.scales), max(self.scales))\n",
    "        w, h = int(W * scale), int(H * scale)\n",
    "        return dict(im = im.resize((w, h), Image.BILINEAR),\n",
    "                    lb = lb.resize((w, h), Image.NEAREST),\n",
    "                )\n",
    "\n",
    "\n",
    "class ColorJitter(object):\n",
    "    def __init__(self, brightness=None, contrast=None, saturation=None, *args, **kwargs):\n",
    "        if not brightness is None and brightness>0:\n",
    "            self.brightness = [max(1-brightness, 0), 1+brightness]\n",
    "        if not contrast is None and contrast>0:\n",
    "            self.contrast = [max(1-contrast, 0), 1+contrast]\n",
    "        if not saturation is None and saturation>0:\n",
    "            self.saturation = [max(1-saturation, 0), 1+saturation]\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        im = im_lb['im']\n",
    "        lb = im_lb['lb']\n",
    "        r_brightness = random.uniform(self.brightness[0], self.brightness[1])\n",
    "        r_contrast = random.uniform(self.contrast[0], self.contrast[1])\n",
    "        r_saturation = random.uniform(self.saturation[0], self.saturation[1])\n",
    "        im = ImageEnhance.Brightness(im).enhance(r_brightness)\n",
    "        im = ImageEnhance.Contrast(im).enhance(r_contrast)\n",
    "        im = ImageEnhance.Color(im).enhance(r_saturation)\n",
    "        return dict(im = im,\n",
    "                    lb = lb,\n",
    "                )\n",
    "\n",
    "\n",
    "class MultiScale(object):\n",
    "    def __init__(self, scales):\n",
    "        self.scales = scales\n",
    "\n",
    "    def __call__(self, img):\n",
    "        W, H = img.size\n",
    "        sizes = [(int(W*ratio), int(H*ratio)) for ratio in self.scales]\n",
    "        imgs = []\n",
    "        [imgs.append(img.resize(size, Image.BILINEAR)) for size in sizes]\n",
    "        return imgs\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, do_list):\n",
    "        self.do_list = do_list\n",
    "\n",
    "    def __call__(self, im_lb):\n",
    "        for comp in self.do_list:\n",
    "            im_lb = comp(im_lb)\n",
    "        return im_lb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os.path as osp\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class CityScapes(Dataset):\n",
    "    def __init__(self, rootpth, cropsize=(640, 480), mode='train', \n",
    "    randomscale=(0.125, 0.25, 0.375, 0.5, 0.675, 0.75, 0.875, 1.0, 1.25, 1.5), *args, **kwargs):\n",
    "        super(CityScapes, self).__init__(*args, **kwargs)\n",
    "        assert mode in ('train', 'val', 'test', 'trainval')\n",
    "        self.mode = mode\n",
    "        print('self.mode', self.mode)\n",
    "        self.ignore_lb = 255\n",
    "\n",
    "        with open('./cityscapes_info.json', 'r') as fr:\n",
    "            labels_info = json.load(fr)\n",
    "        self.lb_map = {el['id']: el['trainId'] for el in labels_info}\n",
    "        \n",
    "\n",
    "        ## parse img directory\n",
    "        self.imgs = {}\n",
    "        imgnames = []\n",
    "        impth = osp.join(rootpth, 'leftImg8bit', mode)\n",
    "        folders = os.listdir(impth)\n",
    "        for fd in folders:\n",
    "            fdpth = osp.join(impth, fd)\n",
    "            im_names = os.listdir(fdpth)\n",
    "            names = [el.replace('_leftImg8bit.png', '') for el in im_names]\n",
    "            impths = [osp.join(fdpth, el) for el in im_names]\n",
    "            imgnames.extend(names)\n",
    "            self.imgs.update(dict(zip(names, impths)))\n",
    "\n",
    "        ## parse gt directory\n",
    "        self.labels = {}\n",
    "        gtnames = []\n",
    "        gtpth = osp.join(rootpth, 'gtFine', mode)\n",
    "        folders = os.listdir(gtpth)\n",
    "        for fd in folders:\n",
    "            fdpth = osp.join(gtpth, fd)\n",
    "            lbnames = os.listdir(fdpth)\n",
    "            lbnames = [el for el in lbnames if 'labelIds' in el]\n",
    "            names = [el.replace('_gtFine_labelIds.png', '') for el in lbnames]\n",
    "            lbpths = [osp.join(fdpth, el) for el in lbnames]\n",
    "            gtnames.extend(names)\n",
    "            self.labels.update(dict(zip(names, lbpths)))\n",
    "\n",
    "        self.imnames = imgnames\n",
    "        self.len = len(self.imnames)\n",
    "        print('self.len', self.mode, self.len)\n",
    "        assert set(imgnames) == set(gtnames)\n",
    "        assert set(self.imnames) == set(self.imgs.keys())\n",
    "        assert set(self.imnames) == set(self.labels.keys())\n",
    "\n",
    "        ## pre-processing\n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        self.trans_train = Compose([\n",
    "            ColorJitter(\n",
    "                brightness = 0.5,\n",
    "                contrast = 0.5,\n",
    "                saturation = 0.5),\n",
    "            HorizontalFlip(),\n",
    "            RandomScale(randomscale),\n",
    "            RandomCrop(cropsize)\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn  = self.imnames[idx]\n",
    "        impth = self.imgs[fn]\n",
    "        lbpth = self.labels[fn]\n",
    "        img = Image.open(impth).convert('RGB')\n",
    "        label = Image.open(lbpth)\n",
    "        if self.mode == 'train' or self.mode == 'trainval':\n",
    "            im_lb = dict(im = img, lb = label)\n",
    "            im_lb = self.trans_train(im_lb)\n",
    "            img, label = im_lb['im'], im_lb['lb']\n",
    "        img = self.to_tensor(img)\n",
    "        label = np.array(label).astype(np.int64)[np.newaxis, :]\n",
    "        label = self.convert_labels(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    def convert_labels(self, label):\n",
    "        for k, v in self.lb_map.items():\n",
    "            label[label == k] = v\n",
    "        return label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dspth = '/content/drive/MyDrive/RnD/datasets/'\n",
    "cfg_data = {\n",
    "    'dataset': 'cityscapes',\n",
    "    'train_split': 'train',\n",
    "    'val_split': 'val',\n",
    "    'img_rows': cropsize[0],\n",
    "    'img_cols': cropsize[1],\n",
    "    'path': dspth\n",
    "}\n",
    "randomscale = (0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0, 1.125, 1.25, 1.375, 1.5)\n",
    "ds = CityScapes(cfg_data['path'], cropsize=cropsize, mode='train', randomscale=randomscale)\n",
    "# sampler = torch.utils.data.distributed.DistributedSampler(ds)\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = batch_size,\n",
    "                shuffle = False,\n",
    "                # sampler = sampler,\n",
    "                num_workers = n_workers,\n",
    "                pin_memory = False,\n",
    "                drop_last = True)\n",
    "# exit(0)\n",
    "dsval = CityScapes(cfg_data['path'], mode='val', randomscale=randomscale)\n",
    "# sampler_val = torch.utils.data.distributed.DistributedSampler(dsval)\n",
    "dlval = DataLoader(dsval,\n",
    "                batch_size = 2,\n",
    "                shuffle = False,\n",
    "                # sampler = sampler_val,\n",
    "                num_workers = n_workers,\n",
    "                drop_last = False)\n",
    "\n",
    "## model\n",
    "ignore_idx = 255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Preparing for train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_iou = -100.0\n",
    "loss_all = 0\n",
    "loss_n = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_logger(logdir):\n",
    "    logger = logging.getLogger(\"DDRNet_HBD_B2N_DGT\")\n",
    "    ts = str(datetime.now()).split(\".\")[0].replace(\" \", \"_\")\n",
    "    ts = ts.replace(\":\", \"_\").replace(\"-\", \"_\")\n",
    "    file_path = os.path.join(logdir, \"run_{}.log\".format(ts))\n",
    "    hdlr = logging.FileHandler(file_path)\n",
    "    formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/PingoLH/FCHarDNet.git\n",
    "!cp -r FCHarDNet/ptsemseg ./\n",
    "!rm -rf FCHarDNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ptsemseg.metrics import runningScore, averageMeter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/RnD/runs/DDRNet\"\n",
    "model_modification = 'HDB_B2N_DGT'\n",
    "model_modification_path = os.path.join(base_path, model_modification)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logdir = os.path.join(model_modification_path, str(datetime.fromtimestamp(int(time.time()))))\n",
    "writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "print(\"RUNDIR: {}\".format(logdir))\n",
    "\n",
    "logger = get_logger(logdir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "running_metrics_val = runningScore(n_classes)\n",
    "model = DualResNet(num_classes=19, planes=64, \n",
    "                         spp_planes=128, head_planes=128, augment=False)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print( 'Parameters:',total_params )\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model.apply(weights_init)\n",
    "pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# optimizer init data\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "lr_start = 1e-2\n",
    "power = 0.9\n",
    "warmup_steps = 1000\n",
    "warmup_start_lr = 1e-5\n",
    "epoch_iteration = len(ds) // batch_size\n",
    "max_epoch = 484\n",
    "# max_epoch = 1\n",
    "max_iter = max_epoch * epoch_iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "it = 0\n",
    "local_max_epoch = start_epoch + 1 if start_epoch + 1 < max_epoch else max_epoch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criteria_ffm = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1\n",
    "criteria_layer4 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx) # out2\n",
    "boundary_loss_func = DetailAggregateLoss()                                          # out3\n",
    "criteria_val = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1 \n",
    "val_loss_meter = averageMeter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optim = Optimizer(\n",
    "            model = model.module,\n",
    "            loss = boundary_loss_func,\n",
    "            lr0 = lr_start,\n",
    "            momentum = momentum,\n",
    "            wd = weight_decay,\n",
    "            warmup_steps = warmup_steps,\n",
    "            warmup_start_lr = warmup_start_lr,\n",
    "            max_iter = max_iter,\n",
    "            power = power)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_avg = []\n",
    "loss_boundery_bce = []\n",
    "loss_boundery_dice = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Restore state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "runs = sorted(os.listdir(model_modification_path), reverse=True)\n",
    "best_path = None\n",
    "last_path = None\n",
    "for run in runs:\n",
    "    tmp_base = os.path.join(model_modification_path, run)\n",
    "    model_name = \"{}_{}\".format(model_arch, cfg_data['dataset'])\n",
    "    checkpoint = os.path.join(tmp_base, model_name+'_checkpoint.pkl')\n",
    "    best = os.path.join(tmp_base, model_name+'_best_model.pkl')\n",
    "    if not last_path and os.path.exists(checkpoint):\n",
    "        last_path = checkpoint\n",
    "    if not best_path and os.path.exists(best):\n",
    "        best_path = best\n",
    "    if last_path and best_path:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_path, last_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loaded = torch.load(last_path)\n",
    "best_iou_arrc = torch.load(best_path)\n",
    "model_state = loaded.get('model_state')\n",
    "\n",
    "optimizer_state = loaded.get('optimizer_state')\n",
    "start_epoch = loaded.get('epoch') + 1\n",
    "\n",
    "best_iou = best_iou_arrc.get('best_iou')\n",
    "local_max_epoch = start_epoch + 2\n",
    "flag = True\n",
    "loss_all = 0\n",
    "loss_n = 0\n",
    "\n",
    "model.load_state_dict(model_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optim.load_state(optimizer_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!tail -n25 '/content/drive/MyDrive/RnD/runs/DDRNet/HDB_B2N_DGT/2021-10-31 14:58:38/run_2021_10_31_14_58_38.log'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_ = list(filter(lambda x: x.endswith('.log'), os.listdir(logdir)))[0]\n",
    "file_ = os.path.join(logdir, file_)\n",
    "if not flag and osp.isfile(file_):\n",
    "    with open(file_, \"r\") as f:\n",
    "        str_ = f.readlines()[-24]\n",
    "        st__, end__ = str_.find('Epoch') + 6, str_.find(' Iter')\n",
    "        if st__ > -1 and end__ > -1 and local_max_epoch - 1 == int(str_[st__:end__]):\n",
    "            start_epoch = local_max_epoch\n",
    "            local_max_epoch += 2\n",
    "\n",
    "start_epoch, local_max_epoch, best_iou"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "st = glob_st = time.time()\n",
    "flag = False\n",
    "for epoch_id in range(start_epoch, local_max_epoch):\n",
    "    for images, labels in dl:\n",
    "        it += 1\n",
    "        start_ts = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.squeeze(labels, 1)\n",
    " \n",
    "        optim.zero_grad()\n",
    "\n",
    "        out_ffm, out_l3, out_l4 = model(images)\n",
    " \n",
    "        loss_ffm = criteria_ffm(out_ffm, labels)\n",
    "        loss_l4  = criteria_layer4(out_l4, labels)\n",
    "        boundery_bce, boundery_dice = boundary_loss_func(out_l3, labels)\n",
    "\n",
    "        boundery_bce_loss = boundery_bce\n",
    "        boundery_dice_loss = boundery_dice\n",
    "\n",
    "        loss = loss_ffm + loss_l4 + boundery_bce_loss + boundery_dice_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_avg.append(loss.item())\n",
    "\n",
    "        loss_boundery_bce.append(boundery_bce_loss.item())\n",
    "        loss_boundery_dice.append(boundery_dice_loss.item())\n",
    "\n",
    "        if (it + 1) % print_interval == 0:\n",
    "            loss_avg = sum(loss_avg) / len(loss_avg)\n",
    "            lr = optim.lr\n",
    "            ed = time.time()\n",
    "            t_intv, glob_t_intv = ed - st, ed - glob_st\n",
    "            eta = int((max_iter - it) * (glob_t_intv / it))\n",
    "            eta = str(timedelta(seconds=eta))\n",
    "\n",
    "            loss_boundery_bce_avg = sum(loss_boundery_bce) / len(loss_boundery_bce)\n",
    "            loss_boundery_dice_avg = sum(loss_boundery_dice) / len(loss_boundery_dice)\n",
    "            msg = ', '.join([\n",
    "                'epoch: {epoch}/{max_epoch}'\n",
    "                'it: {it}/{max_it}',\n",
    "                'lr: {lr:4f}',\n",
    "                'loss: {loss:.4f}',\n",
    "                'boundery_bce_loss: {boundery_bce_loss:.4f}',\n",
    "                'boundery_dice_loss: {boundery_dice_loss:.4f}',\n",
    "                'eta: {eta}',\n",
    "                'time: {time:.4f}',\n",
    "            ]).format(\n",
    "                epoch = epoch_id,\n",
    "                max_epoch = max_epoch,\n",
    "                it = it+1,\n",
    "                max_it = epoch_iteration,\n",
    "                lr = lr,\n",
    "                loss = loss_avg,\n",
    "                boundery_bce_loss = loss_boundery_bce_avg,\n",
    "                boundery_dice_loss = loss_boundery_dice_avg,\n",
    "                time = t_intv,\n",
    "                eta = eta\n",
    "            )\n",
    "            \n",
    "            logger.info(msg)\n",
    "            print(\"loss/train_loss\", loss.item(), it + 1)\n",
    "            loss_avg = []\n",
    "            loss_boundery_bce = []\n",
    "            loss_boundery_dice = []\n",
    "            st = ed\n",
    "\n",
    "        if ((it + 1) % val_interval == 0 and it + 10 < epoch_iteration) or (it + 1) % epoch_iteration == 0:\n",
    "            print('validation')\n",
    "            torch.cuda.empty_cache()\n",
    "            model.eval()\n",
    "            loss_all = 0\n",
    "            loss_n = 0\n",
    "            with torch.no_grad():\n",
    "                for i_val, (images_val, labels_val) in enumerate(dlval):\n",
    "                    if (i_val + 1) % 50 == 0:\n",
    "                        print(i_val + 1)\n",
    "\n",
    "                    images_val = images_val.to(device)\n",
    "                    labels_val = labels_val.to(device)\n",
    "                    labels_val = torch.squeeze(labels_val, 1)\n",
    "\n",
    "                    outputs = model(images_val)[0]\n",
    "                    val_loss = criteria_val(outputs, labels_val)\n",
    "\n",
    "                    pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "                    gt = labels_val.data.cpu().numpy()\n",
    "\n",
    "                    running_metrics_val.update(gt, pred)\n",
    "                    val_loss_meter.update(val_loss.item())\n",
    "\n",
    "            print(\"loss/val_loss\", val_loss_meter.avg)\n",
    "            logger.info(\"Epoch %3d Iter %d Val Loss: %.4f\" % (epoch_id, it + 1, val_loss_meter.avg))\n",
    "\n",
    "            score, class_iou = running_metrics_val.get_scores()\n",
    "            for k, v in score.items():\n",
    "                print(k, v)\n",
    "                logger.info(\"{}: {}\".format(k, v))\n",
    "                print(\"val_metrics/{}\".format(k), v)\n",
    "\n",
    "            for k, v in class_iou.items():\n",
    "                logger.info(\"{}: {}\".format(k, v))\n",
    "                print(\"val_metrics/cls_{}\".format(k), v)\n",
    "\n",
    "            val_loss_meter.reset()\n",
    "            running_metrics_val.reset()\n",
    "\n",
    "            state = {\n",
    "                    \"epoch\": epoch_id,\n",
    "                    \"iteration\": it+ 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optim.get_state(),\n",
    "            }\n",
    "            save_path = os.path.join(\n",
    "                writer.file_writer.get_logdir(),\n",
    "                \"{}_{}_checkpoint.pkl\".format(model_arch, cfg_data['dataset']),\n",
    "            )\n",
    "            torch.save(state, save_path)\n",
    "\n",
    "            if score[\"Mean IoU : \\t\"] >= best_iou:\n",
    "                best_iou = score[\"Mean IoU : \\t\"]\n",
    "                state = {\n",
    "                    \"epoch\": epoch_id,\n",
    "                    \"iteration\":it+ 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"best_iou\": best_iou,\n",
    "                }\n",
    "                save_path = os.path.join(\n",
    "                    writer.file_writer.get_logdir(),\n",
    "                    \"{}_{}_best_model.pkl\".format(model_arch, cfg_data['dataset']),\n",
    "                )\n",
    "                torch.save(state, save_path)\n",
    "            torch.cuda.empty_cache()\n",
    "    it = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for (images_val, labels_val, _) in valloader:\n",
    "#         images_val = images_val.to(device)\n",
    "#         labels_val = labels_val.to(device)\n",
    "\n",
    "#         outputs = model(images_val)\n",
    "#         outputs = output_val_upsample(outputs)\n",
    "#         val_loss = loss_fn(input=outputs, target=labels_val)\n",
    "\n",
    "#         pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "#         gt = labels_val.data.cpu().numpy()\n",
    "\n",
    "#         running_metrics_val.update(gt, pred)\n",
    "#         val_loss_meter.update(val_loss.item())\n",
    "\n",
    "# writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n",
    "# logger.info(\"Iter %d Val Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n",
    "\n",
    "# score, class_iou = running_metrics_val.get_scores()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G95g0arBR1AY"
   },
   "source": [
    "running_metrics_val = runningScore(n_classes)\n",
    "model = DualResNet(num_classes=19, planes=64, \n",
    "                         spp_planes=128, head_planes=128, augment=False)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print( 'Parameters:',total_params )\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model.apply(weights_init)\n",
    "pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P_5wFQ0UVnHt"
   },
   "source": [
    "# optimizer init data\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "lr_start = 1e-2\n",
    "power = 0.9\n",
    "warmup_steps = 1000\n",
    "warmup_start_lr = 1e-5\n",
    "epoch_iteration = len(ds) // batch_size\n",
    "max_epoch = 484\n",
    "# max_epoch = 1\n",
    "max_iter = max_epoch * epoch_iteration"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rg_faMQHOnMZ"
   },
   "source": [
    "start_epoch = 0\n",
    "it = 0\n",
    "local_max_epoch = start_epoch + 1 if start_epoch + 1 < max_epoch else max_epoch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nfsksqZNg3TM"
   },
   "source": [
    "criteria_ffm = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1\n",
    "criteria_layer4 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx) # out2\n",
    "boundary_loss_func = DetailAggregateLoss()                                          # out3\n",
    "criteria_val = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)    # out1 \n",
    "val_loss_meter = averageMeter()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SFawk_jCVuJg"
   },
   "source": [
    "optim = Optimizer(\n",
    "            model = model.module,\n",
    "            loss = boundary_loss_func,\n",
    "            lr0 = lr_start,\n",
    "            momentum = momentum,\n",
    "            wd = weight_decay,\n",
    "            warmup_steps = warmup_steps,\n",
    "            warmup_start_lr = warmup_start_lr,\n",
    "            max_iter = max_iter,\n",
    "            power = power)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gcCTdTNiWhH3"
   },
   "source": [
    "loss_avg = []\n",
    "loss_boundery_bce = []\n",
    "loss_boundery_dice = []"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFTxfMXSqw3"
   },
   "source": [
    "---\n",
    "## Restore state"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kuw_ClrzSqJM"
   },
   "source": [
    "runs = sorted(os.listdir(model_modification_path), reverse=True)\n",
    "best_path = None\n",
    "last_path = None\n",
    "for run in runs:\n",
    "    tmp_base = os.path.join(model_modification_path, run)\n",
    "    model_name = \"{}_{}\".format(model_arch, cfg_data['dataset'])\n",
    "    checkpoint = os.path.join(tmp_base, model_name+'_checkpoint.pkl')\n",
    "    best = os.path.join(tmp_base, model_name+'_best_model.pkl')\n",
    "    if not last_path and os.path.exists(checkpoint):\n",
    "        last_path = checkpoint\n",
    "    if not best_path and os.path.exists(best):\n",
    "        best_path = best\n",
    "    if last_path and best_path:\n",
    "        break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7MjZMI5bkDt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635692334405,
     "user_tz": -180,
     "elapsed": 11,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4dPEc8sFiFvY4T7tgLVyUYIc_dXuMYevP6XSD1Q=s64",
      "userId": "02584668651199014017"
     }
    },
    "outputId": "14210e14-9cde-4527-c8da-cbbfe33f5428"
   },
   "source": [
    "best_path, last_path"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/RnD/runs/DDRNet/HDB_B2N_DGT/2021-10-27 16:25:46/ddrnet_slim_23_cityscapes_best_model.pkl',\n",
       " '/content/drive/MyDrive/RnD/runs/DDRNet/HDB_B2N_DGT/2021-10-30 18:14:43/ddrnet_slim_23_cityscapes_checkpoint.pkl')"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jc6LeN9TOCq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635692336723,
     "user_tz": -180,
     "elapsed": 2322,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4dPEc8sFiFvY4T7tgLVyUYIc_dXuMYevP6XSD1Q=s64",
      "userId": "02584668651199014017"
     }
    },
    "outputId": "aa1da0fe-d97e-448e-e3b0-fc507ec387ed"
   },
   "source": [
    "loaded = torch.load(last_path)\n",
    "best_iou_arrc = torch.load(best_path)\n",
    "model_state = loaded.get('model_state')\n",
    "\n",
    "optimizer_state = loaded.get('optimizer_state')\n",
    "start_epoch = loaded.get('epoch') + 1\n",
    "\n",
    "best_iou = best_iou_arrc.get('best_iou')\n",
    "local_max_epoch = start_epoch + 2\n",
    "flag = True\n",
    "loss_all = 0\n",
    "loss_n = 0\n",
    "\n",
    "model.load_state_dict(model_state)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KbsQJbSHUZ_S"
   },
   "source": [
    "optim.load_state(optimizer_state)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XftdK_iMCpQB"
   },
   "source": [
    "!tail -n25 '/content/drive/MyDrive/RnD/runs/DDRNet/HDB_B2N_DGT/2021-10-31 14:58:38/run_2021_10_31_14_58_38.log'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcaafUp4eal5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635709296284,
     "user_tz": -180,
     "elapsed": 231,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4dPEc8sFiFvY4T7tgLVyUYIc_dXuMYevP6XSD1Q=s64",
      "userId": "02584668651199014017"
     }
    },
    "outputId": "f8dbc8c0-106b-4b9c-cfb7-f9b86d561c8f"
   },
   "source": [
    "file_ = list(filter(lambda x: x.endswith('.log'), os.listdir(logdir)))[0]\n",
    "file_ = os.path.join(logdir, file_)\n",
    "if not flag and osp.isfile(file_):\n",
    "    with open(file_, \"r\") as f:\n",
    "        str_ = f.readlines()[-24]\n",
    "        st__, end__ = str_.find('Epoch') + 6, str_.find(' Iter')\n",
    "        if st__ > -1 and end__ > -1 and local_max_epoch - 1 == int(str_[st__:end__]):\n",
    "            start_epoch = local_max_epoch\n",
    "            local_max_epoch += 2\n",
    "\n",
    "start_epoch, local_max_epoch, best_iou"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(121, 123, 0.4567439205119828)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZjxXbpI_MDX"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HuAUlVp7XTSc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635712045147,
     "user_tz": -180,
     "elapsed": 2744671,
     "user": {
      "displayName": "Maxim Dobrokhvalov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4dPEc8sFiFvY4T7tgLVyUYIc_dXuMYevP6XSD1Q=s64",
      "userId": "02584668651199014017"
     }
    },
    "outputId": "43e1924c-760b-4431-cb0e-0ae85607f05a"
   },
   "source": [
    "st = glob_st = time.time()\n",
    "flag = False\n",
    "for epoch_id in range(start_epoch, local_max_epoch):\n",
    "    for images, labels in dl:\n",
    "        it += 1\n",
    "        start_ts = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.squeeze(labels, 1)\n",
    " \n",
    "        optim.zero_grad()\n",
    "\n",
    "        out_ffm, out_l3, out_l4 = model(images)\n",
    " \n",
    "        loss_ffm = criteria_ffm(out_ffm, labels)\n",
    "        loss_l4  = criteria_layer4(out_l4, labels)\n",
    "        boundery_bce, boundery_dice = boundary_loss_func(out_l3, labels)\n",
    "\n",
    "        boundery_bce_loss = boundery_bce\n",
    "        boundery_dice_loss = boundery_dice\n",
    "\n",
    "        loss = loss_ffm + loss_l4 + boundery_bce_loss + boundery_dice_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_avg.append(loss.item())\n",
    "\n",
    "        loss_boundery_bce.append(boundery_bce_loss.item())\n",
    "        loss_boundery_dice.append(boundery_dice_loss.item())\n",
    "\n",
    "        if (it + 1) % print_interval == 0:\n",
    "            loss_avg = sum(loss_avg) / len(loss_avg)\n",
    "            lr = optim.lr\n",
    "            ed = time.time()\n",
    "            t_intv, glob_t_intv = ed - st, ed - glob_st\n",
    "            eta = int((max_iter - it) * (glob_t_intv / it))\n",
    "            eta = str(timedelta(seconds=eta))\n",
    "\n",
    "            loss_boundery_bce_avg = sum(loss_boundery_bce) / len(loss_boundery_bce)\n",
    "            loss_boundery_dice_avg = sum(loss_boundery_dice) / len(loss_boundery_dice)\n",
    "            msg = ', '.join([\n",
    "                'epoch: {epoch}/{max_epoch}'\n",
    "                'it: {it}/{max_it}',\n",
    "                'lr: {lr:4f}',\n",
    "                'loss: {loss:.4f}',\n",
    "                'boundery_bce_loss: {boundery_bce_loss:.4f}',\n",
    "                'boundery_dice_loss: {boundery_dice_loss:.4f}',\n",
    "                'eta: {eta}',\n",
    "                'time: {time:.4f}',\n",
    "            ]).format(\n",
    "                epoch = epoch_id,\n",
    "                max_epoch = max_epoch,\n",
    "                it = it+1,\n",
    "                max_it = epoch_iteration,\n",
    "                lr = lr,\n",
    "                loss = loss_avg,\n",
    "                boundery_bce_loss = loss_boundery_bce_avg,\n",
    "                boundery_dice_loss = loss_boundery_dice_avg,\n",
    "                time = t_intv,\n",
    "                eta = eta\n",
    "            )\n",
    "            \n",
    "            logger.info(msg)\n",
    "            print(\"loss/train_loss\", loss.item(), it + 1)\n",
    "            loss_avg = []\n",
    "            loss_boundery_bce = []\n",
    "            loss_boundery_dice = []\n",
    "            st = ed\n",
    "\n",
    "        if ((it + 1) % val_interval == 0 and it + 10 < epoch_iteration) or (it + 1) % epoch_iteration == 0:\n",
    "            print('validation')\n",
    "            torch.cuda.empty_cache()\n",
    "            model.eval()\n",
    "            loss_all = 0\n",
    "            loss_n = 0\n",
    "            with torch.no_grad():\n",
    "                for i_val, (images_val, labels_val) in enumerate(dlval):\n",
    "                    if (i_val + 1) % 50 == 0:\n",
    "                        print(i_val + 1)\n",
    "\n",
    "                    images_val = images_val.to(device)\n",
    "                    labels_val = labels_val.to(device)\n",
    "                    labels_val = torch.squeeze(labels_val, 1)\n",
    "\n",
    "                    outputs = model(images_val)[0]\n",
    "                    val_loss = criteria_val(outputs, labels_val)\n",
    "\n",
    "                    pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "                    gt = labels_val.data.cpu().numpy()\n",
    "\n",
    "                    running_metrics_val.update(gt, pred)\n",
    "                    val_loss_meter.update(val_loss.item())\n",
    "\n",
    "            print(\"loss/val_loss\", val_loss_meter.avg)\n",
    "            logger.info(\"Epoch %3d Iter %d Val Loss: %.4f\" % (epoch_id, it + 1, val_loss_meter.avg))\n",
    "\n",
    "            score, class_iou = running_metrics_val.get_scores()\n",
    "            for k, v in score.items():\n",
    "                print(k, v)\n",
    "                logger.info(\"{}: {}\".format(k, v))\n",
    "                print(\"val_metrics/{}\".format(k), v)\n",
    "\n",
    "            for k, v in class_iou.items():\n",
    "                logger.info(\"{}: {}\".format(k, v))\n",
    "                print(\"val_metrics/cls_{}\".format(k), v)\n",
    "\n",
    "            val_loss_meter.reset()\n",
    "            running_metrics_val.reset()\n",
    "\n",
    "            state = {\n",
    "                    \"epoch\": epoch_id,\n",
    "                    \"iteration\": it+ 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optim.get_state(),\n",
    "            }\n",
    "            save_path = os.path.join(\n",
    "                writer.file_writer.get_logdir(),\n",
    "                \"{}_{}_checkpoint.pkl\".format(model_arch, cfg_data['dataset']),\n",
    "            )\n",
    "            torch.save(state, save_path)\n",
    "\n",
    "            if score[\"Mean IoU : \\t\"] >= best_iou:\n",
    "                best_iou = score[\"Mean IoU : \\t\"]\n",
    "                state = {\n",
    "                    \"epoch\": epoch_id,\n",
    "                    \"iteration\":it+ 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"best_iou\": best_iou,\n",
    "                }\n",
    "                save_path = os.path.join(\n",
    "                    writer.file_writer.get_logdir(),\n",
    "                    \"{}_{}_best_model.pkl\".format(model_arch, cfg_data['dataset']),\n",
    "                )\n",
    "                torch.save(state, save_path)\n",
    "            torch.cuda.empty_cache()\n",
    "    it = 0"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss/train_loss 3.1090667247772217 10\n",
      "loss/train_loss 2.6163554191589355 20\n",
      "loss/train_loss 3.1735916137695312 30\n",
      "loss/train_loss 3.1155989170074463 40\n",
      "loss/train_loss 2.2499639987945557 50\n",
      "loss/train_loss 3.314539670944214 60\n",
      "loss/train_loss 3.019232988357544 70\n",
      "loss/train_loss 2.4114272594451904 80\n",
      "loss/train_loss 2.649233818054199 90\n",
      "loss/train_loss 2.5990281105041504 100\n",
      "loss/train_loss 2.3465218544006348 110\n",
      "loss/train_loss 3.182102680206299 120\n",
      "loss/train_loss 2.647751569747925 130\n",
      "loss/train_loss 2.5675182342529297 140\n",
      "loss/train_loss 3.475510597229004 150\n",
      "loss/train_loss 2.7880547046661377 160\n",
      "loss/train_loss 3.3184492588043213 170\n",
      "loss/train_loss 2.8550868034362793 180\n",
      "loss/train_loss 3.253541946411133 190\n",
      "loss/train_loss 2.690558910369873 200\n",
      "loss/train_loss 2.7692368030548096 210\n",
      "loss/train_loss 2.820671796798706 220\n",
      "loss/train_loss 2.9968626499176025 230\n",
      "loss/train_loss 2.5720973014831543 240\n",
      "loss/train_loss 2.852435827255249 250\n",
      "loss/train_loss 2.8489763736724854 260\n",
      "loss/train_loss 2.578950881958008 270\n",
      "loss/train_loss 3.0173587799072266 280\n",
      "loss/train_loss 2.683004856109619 290\n",
      "loss/train_loss 2.83668851852417 300\n",
      "loss/train_loss 3.160796642303467 310\n",
      "loss/train_loss 2.7181448936462402 320\n",
      "loss/train_loss 2.6680479049682617 330\n",
      "loss/train_loss 2.964998960494995 340\n",
      "loss/train_loss 3.1117331981658936 350\n",
      "loss/train_loss 3.043376922607422 360\n",
      "loss/train_loss 2.3436410427093506 370\n",
      "validation\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "loss/val_loss 1.4919252870082855\n",
      "Overall Acc: \t 0.788980964592089\n",
      "val_metrics/Overall Acc: \t 0.788980964592089\n",
      "Mean Acc : \t 0.5178153223673814\n",
      "val_metrics/Mean Acc : \t 0.5178153223673814\n",
      "FreqW Acc : \t 0.6779300929709253\n",
      "val_metrics/FreqW Acc : \t 0.6779300929709253\n",
      "Mean IoU : \t 0.3676406887525924\n",
      "val_metrics/Mean IoU : \t 0.3676406887525924\n",
      "val_metrics/cls_0 0.8229907772231738\n",
      "val_metrics/cls_1 0.5573004870103739\n",
      "val_metrics/cls_2 0.6382271172708771\n",
      "val_metrics/cls_3 0.13521814335208174\n",
      "val_metrics/cls_4 0.18804528578396393\n",
      "val_metrics/cls_5 0.27086112430022974\n",
      "val_metrics/cls_6 0.22371796162228538\n",
      "val_metrics/cls_7 0.4763474565719635\n",
      "val_metrics/cls_8 0.6980250343762064\n",
      "val_metrics/cls_9 0.156931564124923\n",
      "val_metrics/cls_10 0.26747084760205037\n",
      "val_metrics/cls_11 0.5630861670303491\n",
      "val_metrics/cls_12 0.3063641833132785\n",
      "val_metrics/cls_13 0.6747977688027892\n",
      "val_metrics/cls_14 0.00043255692499766723\n",
      "val_metrics/cls_15 0.2273422552961045\n",
      "val_metrics/cls_16 0.2734367397315548\n",
      "val_metrics/cls_17 0.0014234904642548007\n",
      "val_metrics/cls_18 0.5031541254977981\n",
      "loss/train_loss 2.886301279067993 10\n",
      "loss/train_loss 2.6571950912475586 20\n",
      "loss/train_loss 2.6682169437408447 30\n",
      "loss/train_loss 2.9045333862304688 40\n",
      "loss/train_loss 2.858959197998047 50\n",
      "loss/train_loss 3.171006679534912 60\n",
      "loss/train_loss 2.938650369644165 70\n",
      "loss/train_loss 2.8227503299713135 80\n",
      "loss/train_loss 2.887829303741455 90\n",
      "loss/train_loss 2.7708778381347656 100\n",
      "loss/train_loss 2.6526808738708496 110\n",
      "loss/train_loss 2.8809654712677 120\n",
      "loss/train_loss 2.7144157886505127 130\n",
      "loss/train_loss 2.3946146965026855 140\n",
      "loss/train_loss 3.559183359146118 150\n",
      "loss/train_loss 2.795658826828003 160\n",
      "loss/train_loss 3.401266574859619 170\n",
      "loss/train_loss 2.801955223083496 180\n",
      "loss/train_loss 2.7155981063842773 190\n",
      "loss/train_loss 2.977011203765869 200\n",
      "loss/train_loss 2.798858642578125 210\n",
      "loss/train_loss 2.7935590744018555 220\n",
      "loss/train_loss 3.09993052482605 230\n",
      "loss/train_loss 2.611987590789795 240\n",
      "loss/train_loss 3.5956289768218994 250\n",
      "loss/train_loss 2.5990078449249268 260\n",
      "loss/train_loss 2.8193259239196777 270\n",
      "loss/train_loss 2.78273606300354 280\n",
      "loss/train_loss 3.106135845184326 290\n",
      "loss/train_loss 3.0484459400177 300\n",
      "loss/train_loss 2.880997657775879 310\n",
      "loss/train_loss 3.042120933532715 320\n",
      "loss/train_loss 2.683526039123535 330\n",
      "loss/train_loss 2.3539371490478516 340\n",
      "loss/train_loss 3.009891986846924 350\n",
      "loss/train_loss 2.9684314727783203 360\n",
      "loss/train_loss 2.626817226409912 370\n",
      "validation\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "loss/val_loss 1.2589403467178344\n",
      "Overall Acc: \t 0.8443581479413333\n",
      "val_metrics/Overall Acc: \t 0.8443581479413333\n",
      "Mean Acc : \t 0.5891309698836238\n",
      "val_metrics/Mean Acc : \t 0.5891309698836238\n",
      "FreqW Acc : \t 0.7487313461441778\n",
      "val_metrics/FreqW Acc : \t 0.7487313461441778\n",
      "Mean IoU : \t 0.44498393205418385\n",
      "val_metrics/Mean IoU : \t 0.44498393205418385\n",
      "val_metrics/cls_0 0.8563676413302149\n",
      "val_metrics/cls_1 0.6005333702672551\n",
      "val_metrics/cls_2 0.6674480625410568\n",
      "val_metrics/cls_3 0.17783222042169572\n",
      "val_metrics/cls_4 0.20560965440086318\n",
      "val_metrics/cls_5 0.36132788365194235\n",
      "val_metrics/cls_6 0.30166608162556424\n",
      "val_metrics/cls_7 0.5207233475394499\n",
      "val_metrics/cls_8 0.8291891882422974\n",
      "val_metrics/cls_9 0.27151186616710027\n",
      "val_metrics/cls_10 0.7163901067327866\n",
      "val_metrics/cls_11 0.5951783535228337\n",
      "val_metrics/cls_12 0.29564281857678604\n",
      "val_metrics/cls_13 0.7872331542545538\n",
      "val_metrics/cls_14 0.05638757472138248\n",
      "val_metrics/cls_15 0.3358790584215004\n",
      "val_metrics/cls_16 0.2546706884980913\n",
      "val_metrics/cls_17 0.09683585504159783\n",
      "val_metrics/cls_18 0.5242677830725202\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RH8UeQwnQlgY"
   },
   "source": [
    "# with torch.no_grad():\n",
    "#     for (images_val, labels_val, _) in valloader:\n",
    "#         images_val = images_val.to(device)\n",
    "#         labels_val = labels_val.to(device)\n",
    "\n",
    "#         outputs = model(images_val)\n",
    "#         outputs = output_val_upsample(outputs)\n",
    "#         val_loss = loss_fn(input=outputs, target=labels_val)\n",
    "\n",
    "#         pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "#         gt = labels_val.data.cpu().numpy()\n",
    "\n",
    "#         running_metrics_val.update(gt, pred)\n",
    "#         val_loss_meter.update(val_loss.item())\n",
    "\n",
    "# writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n",
    "# logger.info(\"Iter %d Val Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n",
    "\n",
    "# score, class_iou = running_metrics_val.get_scores()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}